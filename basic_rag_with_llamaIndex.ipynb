{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e75806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a662e9c",
   "metadata": {},
   "source": [
    "sentence-transformers/all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7fc9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:3b\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a456a2ea",
   "metadata": {},
   "source": [
    "# 2️⃣ Breaking it down\n",
    "``` LlamaIndex Workflows (an event-driven architecture). In that system, functions don't call each other directly; they communicate by throwing \"Events\" back and forth.```\n",
    "\n",
    "### **A. `Event`**\n",
    "\n",
    "* `Event` is a **base class in LlamaIndex workflows**.\n",
    "* Workflows in LlamaIndex are **step-based pipelines**, and **events** are the objects that pass between steps.\n",
    "* Examples of events:\n",
    "\n",
    "  * `StartEvent` → triggers the workflow\n",
    "  * `StopEvent` → indicates a step has finished\n",
    "* You can define **custom events** to carry specific data between steps.\n",
    "\n",
    "---\n",
    "\n",
    "### **B. `NodeWithScore`**\n",
    "\n",
    "* Each `NodeWithScore` is a **document chunk + similarity score**.\n",
    "* When you retrieve documents from your vector database, you get:\n",
    "\n",
    "  * The **chunk of text** (Node)\n",
    "  * Its **relevance score** (Score)\n",
    "\n",
    "So `NodeWithScore` represents **a retrieved document and how relevant it is to the query**.\n",
    "\n",
    "---\n",
    "\n",
    "### **C. `RetrieverEvent`**\n",
    "\n",
    "* This is a **custom event** that will hold the **results of the retrieval step** in your RAG workflow.\n",
    "* By defining:\n",
    "\n",
    "```python\n",
    "nodes: list[NodeWithScore]\n",
    "```\n",
    "\n",
    "You are saying:\n",
    "\n",
    "> “This event will carry a list of retrieved nodes (documents) with their similarity scores.”\n",
    "\n",
    "# 4️⃣ How it fits into a RAG workflow\n",
    "\n",
    "```\n",
    "StartEvent(query)\n",
    "       │\n",
    "       ▼\n",
    "Retrieve Step\n",
    "       │\n",
    "       ▼\n",
    "RetrieverEvent(nodes=[NodeWithScore, ...])\n",
    "       │\n",
    "       ▼\n",
    "Synthesize Step (uses nodes to generate answer)\n",
    "```\n",
    "\n",
    "* `RetrieverEvent` is **just a container**\n",
    "* Carries **retrieved text chunks + scores** from retrieval → synthesis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b760fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "class RetrieverEvent(Event):\n",
    "    \"\"\"Result of running retrieval\"\"\"\n",
    "\n",
    "    nodes: list[NodeWithScore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.response_synthesizers import CompactAndRefine\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step\n",
    ")\n",
    "\n",
    "class RAGWorkflow(Workflow):\n",
    "    @step\n",
    "    async def ingest(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n",
    "        dirname = ev.get(\"dirname\")\n",
    "        if not dirname:\n",
    "            return None\n",
    "        \n",
    "        documents = SimpleDirectoryReader(dirname).load_data()\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents=documents\n",
    "        )\n",
    "        return StopEvent(result=index)\n",
    "    \n",
    "    @step\n",
    "    async def retrieve(self, ctx: Context, ev: StartEvent) -> RetrieverEvent | None:\n",
    "        \"\"\" Retrieve relevant documents from the index based on the query. \"\"\"\n",
    "        query = ev.get(\"query\")\n",
    "        index = ev.get(\"index\")\n",
    "\n",
    "        if not query:\n",
    "            return None\n",
    "        \n",
    "        print(f\"Retrieving documents for query: {query}\")\n",
    "\n",
    "        await ctx.store.set(\"query\", query)\n",
    "\n",
    "        if index is None:\n",
    "            print(\"Index is empty, load some documents before querying!\")\n",
    "            return None\n",
    "\n",
    "        retriever = index.as_retriever( similarity_top_k=2 )\n",
    "        nodes = await retriever.aretrieve(query)\n",
    "        print(f\"Retrieved {len(nodes)} documents.\")\n",
    "        print((\"Document Retrieved:\"))\n",
    "        for node in nodes:\n",
    "            print(\"-----\"*10)\n",
    "            print(node.get_text())\n",
    "        return RetrieverEvent(nodes=nodes)\n",
    "\n",
    "\n",
    "    @step\n",
    "    async def synthesize(self, ctx: Context, ev: RetrieverEvent) -> StopEvent:\n",
    "        \"\"\"Return a streaming response using reranked nodes.\"\"\"\n",
    "        summarizer = CompactAndRefine(streaming=True, verbose=True)\n",
    "        query = await ctx.store.get(\"query\", default=None)\n",
    "\n",
    "        response = await summarizer.asynthesize(query, nodes=ev.nodes)\n",
    "        return StopEvent(result=response)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ffd1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = RAGWorkflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81ba3bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest the documents\n",
    "index = await w.run(dirname=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e713aad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: How was DeepSeekR1 trained?\n",
      "Retrieved 2 documents.\n",
      "Document contents:\n",
      "--------------------------------------------------\n",
      "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n",
      "Reinforcement Learning\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\n",
      "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\n",
      "vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\n",
      "Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\n",
      "reasoning behaviors. However, it encounters challenges such as poor readability, and language\n",
      "mixing. To address these issues and further enhance reasoning performance, we introduce\n",
      "DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\n",
      "R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\n",
      "research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\n",
      "(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\n",
      "AIME 2024\n",
      "(Pass@1)\n",
      "Codeforces\n",
      "(Percentile)\n",
      "GPQA Diamond\n",
      "(Pass@1)\n",
      "MATH-500\n",
      "(Pass@1)\n",
      "MMLU\n",
      "(Pass@1)\n",
      "SWE-bench Verified\n",
      "(Resolved)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100Accuracy / Percentile (%)\n",
      "79.8\n",
      "96.3\n",
      "71.5\n",
      "97.3\n",
      "90.8\n",
      "49.2\n",
      "79.2\n",
      "96.6\n",
      "75.7\n",
      "96.4\n",
      "91.8\n",
      "48.9\n",
      "72.6\n",
      "90.6\n",
      "62.1\n",
      "94.3\n",
      "87.4\n",
      "36.8\n",
      "63.6\n",
      "93.4\n",
      "60.0\n",
      "90.0\n",
      "85.2\n",
      "41.6\n",
      "39.2\n",
      "58.7 59.1\n",
      "90.2\n",
      "88.5\n",
      "42.0\n",
      "DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\n",
      "Figure 1 |Benchmark performance of DeepSeek-R1.\n",
      "arXiv:2501.12948v1  [cs.CL]  22 Jan 2025\n",
      "--------------------------------------------------\n",
      "This im-\n",
      "provement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-\n",
      "icant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1\n",
      "excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis\n",
      "capabilities. This highlights the potential of reasoning models in AI-driven search and data\n",
      "analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\n",
      "demonstrating its capability in handling fact-based queries. A similar trend is observed where\n",
      "OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than\n",
      "DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\n",
      "answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an\n",
      "accuracy of over 70%.\n",
      "DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\n",
      "model’s ability to follow format instructions. These improvements can be linked to the inclusion\n",
      "of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\n",
      "training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,\n",
      "indicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering. Its\n",
      "significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale\n",
      "RL, which not only boosts reasoning capabilities but also improves performance across diverse\n",
      "domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\n",
      "average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\n",
      "13\n",
      "\n",
      "Final response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 15:50:53,879 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-R1 was trained using a multi-stage approach, which includes supervised fine-tuning (SFT) followed by large-scale reinforcement learning (RL). The model was initially trained with cold-start data before undergoing RL training. This combination of SFT and RL appears to have significantly enhanced the reasoning capabilities of DeepSeek-R1."
     ]
    }
   ],
   "source": [
    "# Run a query\n",
    "result = await w.run(query=\"How was DeepSeekR1 trained?\", index=index)\n",
    "print(\"\\nFinal response:\")\n",
    "async for chunk in result.async_response_gen():\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfdf5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepSeek-R1 was trained using a multi-stage approach, which includes supervised fine-tuning (SFT) followed by large-scale reinforcement learning (RL). The model was initially trained with cold-start data before undergoing RL training. This combination of SFT and RL appears to have significantly enhanced the reasoning capabilities of DeepSeek-R1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdffdcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407511e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9930c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d343c71e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
