{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a50358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import nest_asyncio\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad7f23b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://developers.llamaindex.ai/python/framework/understanding/', 'title': 'Building an LLM application', 'text': 'Using LLMs: hit the ground running by getting started working with LLMs. We’ll show you how to use any of our dozens of supported LLMs, whether via remote API calls or running locally on your machine.\\n\\nBuilding agents: agents are LLM-powered knowledge workers that can interact with the world via a set of tools. Those tools can retrieve information (such as RAG, see below) or take action. This tutorial includes:\\n\\nBuilding a single agent: We show you how to build a simple agent that can interact with the world via a set of tools.\\n\\nUsing existing tools: LlamaIndex provides a registry of pre-built agent tools at LlamaHub that you can incorporate into your agents.\\n\\nMaintaining state: agents can maintain state, which is important for building more complex applications.\\n\\nStreaming output and events: providing visibility and feedback to the user is important, and streaming allows you to do that.\\n\\nHuman in the loop: getting human feedback to your agent can be critical.\\n\\nMulti-agent systems with AgentWorkflow: combining multiple agents to collaborate is a powerful technique for building more complex systems; this section shows you how to do so.\\n\\nWorkflows: Workflows are a lower-level, event-driven abstraction for building agentic applications. They’re the base layer you should be using to build any advanced agentic application. You can use the pre-built abstractions you learned above, or build agents completely from scratch. This tutorial covers:\\n\\nBuilding a simple workflow: a simple workflow that shows you how to use the Workflow class to build a basic agentic application.\\n\\nLooping and branching: these core control flow patterns are the building blocks of more complex workflows.\\n\\nConcurrent execution: you can run steps in parallel to split up work efficiently.\\n\\nStreaming events: your agents can emit user-facing events just like the agents you built above.\\n\\nStateful workflows: workflows can maintain state, which is important for building more complex applications.\\n\\nObservability: workflows can be traced and debugged using various integrations like Arize Pheonix, OpenTelemetry, and more.\\n\\nAdding RAG to your agents: Retrieval-Augmented Generation (RAG) is a key technique for getting your data to an LLM, and a component of more sophisticated agentic systems. We’ll show you how to enhance your agents with a full-featured RAG pipeline that can answer questions about your data. This includes:\\n\\nLoading & Ingestion: Getting your data from wherever it lives, whether that’s unstructured text, PDFs, databases, or APIs to other applications. LlamaIndex has hundreds of connectors to every data source over at LlamaHub.\\n\\nIndexing and Embedding: Once you’ve got your data there are an infinite number of ways to structure access to that data to ensure your applications is always working with the most relevant data. LlamaIndex has a huge number of these strategies built-in and can help you select the best ones.\\n\\nStoring: You will probably find it more efficient to store your data in indexed form, or pre-processed summaries provided by an LLM, often in a specialized database known as a Vector Store (see below). You can also store your indexes, metadata and more.\\n\\nQuerying: Every indexing strategy has a corresponding querying strategy and there are lots of ways to improve the relevance, speed and accuracy of what you retrieve and what the LLM does with it before returning it to you, including turning it into structured responses such as an API.\\n\\nPutting it all together: whether you are building question & answering, chatbots, an API, or an autonomous agent, we show you how to get your application into production.\\n\\nTracing and debugging: also called observability, it’s especially important with LLM applications to be able to look into the inner workings of what’s going on to help you debug problems and spot places to improve.\\n\\nEvaluating: every strategy has pros and cons and a key part of building, shipping and evolving your application is evaluating whether your change has improved your application in terms of accuracy, performance, clarity, cost and more. Reliably evaluating your changes is a crucial part of LLM application development.'}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import newspaper\n",
    "from newspaper import Config\n",
    "\n",
    "# 1. Create a configuration object\n",
    "config = Config()\n",
    "# 2. Tell it to allow \"binary\" urls (bypassing the bad check)\n",
    "config.allow_binary_content = True \n",
    "\n",
    "urls = [\n",
    "    \"https://developers.llamaindex.ai/python/framework/understanding/\",\n",
    "    \"https://developers.llamaindex.ai/python/framework/understanding/using_llms/\",\n",
    "    \"https://developers.llamaindex.ai/python/framework/understanding/rag/indexing/\",\n",
    "    \"https://developers.llamaindex.ai/python/framework/understanding/rag/querying/\",\n",
    "]   \n",
    "\n",
    "\n",
    "pages_content = []\n",
    "\n",
    "# Retrieve the Content\n",
    "for url in urls:\n",
    "    try:\n",
    "        article = newspaper.Article(url, config=config)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        if len(article.text) > 0:\n",
    "            pages_content.append(\n",
    "                {\"url\": url, \"title\": article.title, \"text\": article.text}\n",
    "            )\n",
    "    except:\n",
    "        print(f\"Failed to retrieve content from {url}\")\n",
    "        continue\n",
    "\n",
    "print(pages_content[0])\n",
    "print(len(pages_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "777c006a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://developers.llamaindex.ai/python/framework/understanding/',\n",
       "  'title': 'Building an LLM application',\n",
       "  'text': 'Using LLMs: hit the ground running by getting started working with LLMs. We’ll show you how to use any of our dozens of supported LLMs, whether via remote API calls or running locally on your machine.\\n\\nBuilding agents: agents are LLM-powered knowledge workers that can interact with the world via a set of tools. Those tools can retrieve information (such as RAG, see below) or take action. This tutorial includes:\\n\\nBuilding a single agent: We show you how to build a simple agent that can interact with the world via a set of tools.\\n\\nUsing existing tools: LlamaIndex provides a registry of pre-built agent tools at LlamaHub that you can incorporate into your agents.\\n\\nMaintaining state: agents can maintain state, which is important for building more complex applications.\\n\\nStreaming output and events: providing visibility and feedback to the user is important, and streaming allows you to do that.\\n\\nHuman in the loop: getting human feedback to your agent can be critical.\\n\\nMulti-agent systems with AgentWorkflow: combining multiple agents to collaborate is a powerful technique for building more complex systems; this section shows you how to do so.\\n\\nWorkflows: Workflows are a lower-level, event-driven abstraction for building agentic applications. They’re the base layer you should be using to build any advanced agentic application. You can use the pre-built abstractions you learned above, or build agents completely from scratch. This tutorial covers:\\n\\nBuilding a simple workflow: a simple workflow that shows you how to use the Workflow class to build a basic agentic application.\\n\\nLooping and branching: these core control flow patterns are the building blocks of more complex workflows.\\n\\nConcurrent execution: you can run steps in parallel to split up work efficiently.\\n\\nStreaming events: your agents can emit user-facing events just like the agents you built above.\\n\\nStateful workflows: workflows can maintain state, which is important for building more complex applications.\\n\\nObservability: workflows can be traced and debugged using various integrations like Arize Pheonix, OpenTelemetry, and more.\\n\\nAdding RAG to your agents: Retrieval-Augmented Generation (RAG) is a key technique for getting your data to an LLM, and a component of more sophisticated agentic systems. We’ll show you how to enhance your agents with a full-featured RAG pipeline that can answer questions about your data. This includes:\\n\\nLoading & Ingestion: Getting your data from wherever it lives, whether that’s unstructured text, PDFs, databases, or APIs to other applications. LlamaIndex has hundreds of connectors to every data source over at LlamaHub.\\n\\nIndexing and Embedding: Once you’ve got your data there are an infinite number of ways to structure access to that data to ensure your applications is always working with the most relevant data. LlamaIndex has a huge number of these strategies built-in and can help you select the best ones.\\n\\nStoring: You will probably find it more efficient to store your data in indexed form, or pre-processed summaries provided by an LLM, often in a specialized database known as a Vector Store (see below). You can also store your indexes, metadata and more.\\n\\nQuerying: Every indexing strategy has a corresponding querying strategy and there are lots of ways to improve the relevance, speed and accuracy of what you retrieve and what the LLM does with it before returning it to you, including turning it into structured responses such as an API.\\n\\nPutting it all together: whether you are building question & answering, chatbots, an API, or an autonomous agent, we show you how to get your application into production.\\n\\nTracing and debugging: also called observability, it’s especially important with LLM applications to be able to look into the inner workings of what’s going on to help you debug problems and spot places to improve.\\n\\nEvaluating: every strategy has pros and cons and a key part of building, shipping and evolving your application is evaluating whether your change has improved your application in terms of accuracy, performance, clarity, cost and more. Reliably evaluating your changes is a crucial part of LLM application development.'},\n",
       " {'url': 'https://developers.llamaindex.ai/python/framework/understanding/using_llms/',\n",
       "  'title': 'Using LLMs',\n",
       "  'text': 'One of the first steps when building an LLM-based application is which LLM to use; they have different strengths and price points and you may wish to use more than one.\\n\\nLlamaIndex provides a single interface to a large number of different LLMs. Using an LLM can be as simple as installing the appropriate integration:\\n\\nAnd then calling it in a one-liner:\\n\\nNote that this requires an API key called OPENAI_API_KEY in your environment; see the starter tutorial for more details.\\n\\ncomplete is also available as an async method, acomplete.\\n\\nYou can also get a streaming response by calling stream_complete, which returns a generator that yields tokens as they are produced:\\n\\nstream_complete is also available as an async method, astream_complete.\\n\\nThe LLM class also implements a chat method, which allows you to have more sophisticated interactions:\\n\\nstream_chat and astream_chat are also available.\\n\\nMany LLM integrations provide more than one model. You can specify a model by passing the model parameter to the LLM constructor:\\n\\nSome LLMs support multi-modal chat messages. This means that you can pass in a mix of text and other modalities (images, audio, video, etc.) and the LLM will handle it.\\n\\nCurrently, LlamaIndex supports text, images, and audio inside ChatMessages using content blocks.\\n\\nSome LLMs (OpenAI, Anthropic, Gemini, Ollama, etc.) support tool calling directly over API calls — this means tools and functions can be called without specific prompts and parsing mechanisms.\\n\\nFor more details on even more advanced tool calling, check out the in-depth guide using OpenAI. The same approaches work for any LLM that supports tools/functions (e.g. Anthropic, Gemini, Ollama, etc.).\\n\\nYou can learn more about tools and agents in the tools guide.\\n\\nWe support integrations with OpenAI, Anthropic, Mistral, DeepSeek, Hugging Face, and dozens more. Check out our module guide to LLMs for a full list, including how to run a local model.\\n\\nLlamaIndex doesn’t just support hosted LLM APIs; you can also run a local model such as Meta’s Llama 3 locally. For example, if you have Ollama installed and running:'},\n",
       " {'url': 'https://developers.llamaindex.ai/python/framework/understanding/rag/indexing/',\n",
       "  'title': 'Indexing',\n",
       "  'text': 'With your data loaded, you now have a list of Document objects (or a list of Nodes). It’s time to build an Index over these objects so you can start querying them.\\n\\nIn LlamaIndex terms, an Index is a data structure composed of Document objects, designed to enable querying by an LLM. Your Index is designed to be complementary to your querying strategy.\\n\\nLlamaIndex offers several different index types. We’ll cover the two most common here.\\n\\nA VectorStoreIndex is by far the most frequent type of Index you’ll encounter. The Vector Store Index takes your Documents and splits them up into Nodes. It then creates vector embeddings of the text of every node, ready to be queried by an LLM.\\n\\nVector embeddings are central to how LLM applications function.\\n\\nA vector embedding, often just called an embedding, is a numerical representation of the semantics, or meaning of your text. Two pieces of text with similar meanings will have mathematically similar embeddings, even if the actual text is quite different.\\n\\nThis mathematical relationship enables semantic search, where a user provides query terms and LlamaIndex can locate text that is related to the meaning of the query terms rather than simple keyword matching. This is a big part of how Retrieval-Augmented Generation works, and how LLMs function in general.\\n\\nThere are many types of embeddings, and they vary in efficiency, effectiveness and computational cost. By default LlamaIndex uses text-embedding-ada-002, which is the default embedding used by OpenAI. If you are using different LLMs you will often want to use different embeddings.\\n\\nVector Store Index turns all of your text into embeddings using an API from your LLM; this is what is meant when we say it “embeds your text”. If you have a lot of text, generating embeddings can take a long time since it involves many round-trip API calls.\\n\\nWhen you want to search your embeddings, your query is itself turned into a vector embedding, and then a mathematical operation is carried out by VectorStoreIndex to rank all the embeddings by how semantically similar they are to your query.\\n\\nOnce the ranking is complete, VectorStoreIndex returns the most-similar embeddings as their corresponding chunks of text. The number of embeddings it returns is known as k, so the parameter controlling how many embeddings to return is known as top_k. This whole type of search is often referred to as “top-k semantic retrieval” for this reason.\\n\\nTop-k retrieval is the simplest form of querying a vector index; you will learn about more complex and subtler strategies when you read the querying section.\\n\\nTo use the Vector Store Index, pass it the list of Documents you created during the loading stage:\\n\\nYou can also choose to build an index over a list of Node objects directly:\\n\\nWith your text indexed, it is now technically ready for querying! However, embedding all your text can be time-consuming and, if you are using a hosted LLM, it can also be expensive. To save time and money you will want to store your embeddings first.\\n\\nA Summary Index is a simpler form of Index best suited to queries where, as the name suggests, you are trying to generate a summary of the text in your Documents. It simply stores all of the Documents and returns all of them to your query engine.'},\n",
       " {'url': 'https://developers.llamaindex.ai/python/framework/understanding/rag/querying/',\n",
       "  'title': 'Querying',\n",
       "  'text': 'Now you’ve loaded your data, built an index, and stored that index for later, you’re ready to get to the most significant part of an LLM application: querying.\\n\\nAt its simplest, querying is just a prompt call to an LLM: it can be a question and get an answer, or a request for summarization, or a much more complex instruction.\\n\\nMore complex querying could involve repeated/chained prompt + LLM calls, or even a reasoning loop across multiple components.\\n\\nThe basis of all querying is the QueryEngine. The simplest way to get a QueryEngine is to get your index to create one for you, like this:\\n\\nHowever, there is more to querying than initially meets the eye. Querying consists of three distinct stages:\\n\\nRetrieval is when you find and return the most relevant documents for your query from your Index. As previously discussed in indexing, the most common type of retrieval is “top-k” semantic retrieval, but there are many other retrieval strategies.\\n\\nPostprocessing is when the Nodes retrieved are optionally reranked, transformed, or filtered, for instance by requiring that they have specific metadata such as keywords attached.\\n\\nResponse synthesis is when your query, your most-relevant data and your prompt are combined and sent to your LLM to return a response.\\n\\nLlamaIndex features a low-level composition API that gives you granular control over your querying.\\n\\nIn this example, we customize our retriever to use a different number for top_k and add a post-processing step that requires that the retrieved nodes reach a minimum similarity score to be included. This would give you a lot of data when you have relevant results but potentially no data if you have nothing relevant.\\n\\nYou can also add your own retrieval, response synthesis, and overall query logic, by implementing the corresponding interfaces.\\n\\nFor a full list of implemented components and the supported configurations, check out our reference docs.\\n\\nLet’s go into more detail about customizing each step:\\n\\nThere are a huge variety of retrievers that you can learn about in our module guide on retrievers.\\n\\nWe support advanced Node filtering and augmentation that can further improve the relevancy of the retrieved Node objects. This can help reduce the time/number of LLM calls/cost or improve response quality.\\n\\nFor example:\\n\\nKeywordNodePostprocessor: filters nodes by required_keywords and exclude_keywords.\\n\\nSimilarityPostprocessor: filters nodes by setting a threshold on the similarity score (thus only supported by embedding-based retrievers)\\n\\nPrevNextNodePostprocessor: augments retrieved Node objects with additional relevant context based on Node relationships.\\n\\nThe full list of node postprocessors is documented in the Node Postprocessor Reference.\\n\\nTo configure the desired node postprocessors:\\n\\nAfter a retriever fetches relevant nodes, a BaseSynthesizer synthesizes the final response by combining the information.\\n\\nYou can configure it via\\n\\nRight now, we support the following options:\\n\\ndefault: “create and refine” an answer by sequentially going through each retrieved Node; This makes a separate LLM call per Node. Good for more detailed answers.\\n\\ncompact: “compact” the prompt during each LLM call by stuffing as many Node text chunks that can fit within the maximum prompt size. If there are too many chunks to stuff in one prompt, “create and refine” an answer by going through multiple prompts.\\n\\ntree_summarize: Given a set of Node objects and the query, recursively construct a tree and return the root node as the response. Good for summarization purposes.\\n\\nno_text: Only runs the retriever to fetch the nodes that would have been sent to the LLM, without actually sending them. Then can be inspected by checking response.source_nodes. The response object is covered in more detail in Section 5.\\n\\naccumulate: Given a set of Node objects and the query, apply the query to each Node text chunk while accumulating the responses into an array. Returns a concatenated string of all responses. Good for when you need to run the same query separately against each text chunk.\\n\\nYou may want to ensure your output is structured. See our Query Engines + Pydantic Outputs to see how to extract a Pydantic object from a query engine class.\\n\\nAlso make sure to check out our entire Structured Outputs guide.\\n\\nIf you want to design complex query flows, you can compose your own query workflow across many different modules, from prompts/LLMs/output parsers to retrievers to response synthesizers to your own custom components.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e20312",
   "metadata": {},
   "source": [
    "Convert that into the Document so Llama-index understand it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0fd4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Document\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "documents = [\n",
    "    Document(text=row[\"text\"], metadata={\"title\": row[\"title\"], \"url\": row[\"url\"]})\n",
    "    for row in pages_content\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7dfe64",
   "metadata": {},
   "source": [
    "## Now we will use the **crawl4ai** for scrapt the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71cbd4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n",
    "\n",
    "# urls_to_crawl = [\n",
    "#     \"https://docs.llamaindex.ai/en/stable/understanding/\",\n",
    "# ]\n",
    "\n",
    "# def crawl_sync():\n",
    "#     async def crawl_with_crawl4ai():\n",
    "#         config = CrawlerRunConfig(\n",
    "#             cache_mode=CacheMode.BYPASS,\n",
    "#             page_timeout=80000,\n",
    "#             word_count_threshold=50\n",
    "#         )\n",
    "\n",
    "#         data_res = {\"data\": []}\n",
    "\n",
    "#         async with AsyncWebCrawler() as crawler:\n",
    "#             results = await crawler.arun(\n",
    "#                 urls=urls_to_crawl,\n",
    "#                 run_config=config\n",
    "#             ) \n",
    "\n",
    "#             for result in results:\n",
    "#                 if result.success:\n",
    "#                     title = result.metadata.get(\"title\" , \"\")\n",
    "\n",
    "#                     if not title and result.markdown:\n",
    "#                         lines = result.markdown.raw_markdown.split(\"\\n\")\n",
    "#                         for line in lines:\n",
    "#                             if line.startswith(\"#\"):\n",
    "#                                 title = line.lstrip(\"#\").strip()\n",
    "#                                 break\n",
    "\n",
    "#                     data_res[\"data\"].append({\n",
    "#                         \"text\": result.markdown.raw.markdown if result.markdown else \"\",\n",
    "#                         \"url\": result.url,\n",
    "#                         \"meta\": {\n",
    "#                             \"meta\": {\n",
    "#                                 \"title\": title\n",
    "#                             }\n",
    "#                         }\n",
    "#                     })\n",
    "#         return data_res\n",
    "\n",
    "    \n",
    "#     # Handle async execution\n",
    "#     nest_asyncio.apply()\n",
    "#     loop = asyncio.get_event_loop()\n",
    "#     data = loop.run_until_complete(crawl_with_crawl4ai())\n",
    "#     return data\n",
    "\n",
    "# data_res = crawl_sync()\n",
    "\n",
    "# # Print results (same format as before)\n",
    "# print(\"URL:\", data_res[\"data\"][0][\"meta\"][\"url\"])\n",
    "# print(\"Title:\", data_res[\"data\"][0][\"meta\"][\"meta\"][\"title\"])\n",
    "# print(\"Content:\", data_res[\"data\"][0][\"text\"][0:500], \"...\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef80d1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-5' coro=<Connection.run() done, defined at e:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\playwright\\_impl\\_connection.py:305> exception=NotImplementedError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"e:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\playwright\\_impl\\_connection.py\", line 312, in run\n",
      "    await self._transport.connect()\n",
      "  File \"e:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 133, in connect\n",
      "    raise exc\n",
      "  File \"e:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 120, in connect\n",
      "    self._proc = await asyncio.create_subprocess_exec(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\subprocess.py\", line 224, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1756, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 528, in _make_subprocess_transport\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loop.run_until_complete(crawl_with_crawl4ai())\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Run the crawler\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m data_res = \u001b[43mcrawl_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data_res[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mcrawl_sync\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# --- FIX 3: Re-use the Existing Loop ---\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Instead of creating a new loop, we ask Jupyter for the current one\u001b[39;00m\n\u001b[32m     59\u001b[39m loop = asyncio.get_event_loop()\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrawl_with_crawl4ai\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mcrawl_sync.<locals>.crawl_with_crawl4ai\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     21\u001b[39m config = CrawlerRunConfig(\n\u001b[32m     22\u001b[39m     cache_mode=CacheMode.BYPASS,\n\u001b[32m     23\u001b[39m     page_timeout=\u001b[32m80000\u001b[39m,\n\u001b[32m     24\u001b[39m     word_count_threshold=\u001b[32m50\u001b[39m\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m data_res = {\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: []}\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m AsyncWebCrawler() \u001b[38;5;28;01mas\u001b[39;00m crawler:\n\u001b[32m     30\u001b[39m     results = \u001b[38;5;28;01mawait\u001b[39;00m crawler.arun_many(\n\u001b[32m     31\u001b[39m         urls_to_crawl, \n\u001b[32m     32\u001b[39m         config=config\n\u001b[32m     33\u001b[39m     )\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\crawl4ai\\async_webcrawler.py:194\u001b[39m, in \u001b[36mAsyncWebCrawler.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.start()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\crawl4ai\\async_webcrawler.py:177\u001b[39m, in \u001b[36mAsyncWebCrawler.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    171\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[33;03m    Start the crawler explicitly without using context manager.\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[33;03m    This is equivalent to using 'async with' but gives more control over the lifecycle.\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[33;03m        AsyncWebCrawler: The initialized crawler instance\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.crawler_strategy.\u001b[34m__aenter__\u001b[39m()\n\u001b[32m    178\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCrawl4AI \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcrawl4ai_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, tag=\u001b[33m\"\u001b[39m\u001b[33mINIT\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m.ready = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\crawl4ai\\async_crawler_strategy.py:119\u001b[39m, in \u001b[36mAsyncPlaywrightCrawlerStrategy.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.start()\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\crawl4ai\\async_crawler_strategy.py:129\u001b[39m, in \u001b[36mAsyncPlaywrightCrawlerStrategy.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    126\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[33;03m    Start the browser and initialize the browser manager.\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.browser_manager.start()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execute_hook(\n\u001b[32m    131\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mon_browser_created\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    132\u001b[39m         \u001b[38;5;28mself\u001b[39m.browser_manager.browser,\n\u001b[32m    133\u001b[39m         context=\u001b[38;5;28mself\u001b[39m.browser_manager.default_context,\n\u001b[32m    134\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\crawl4ai\\browser_manager.py:659\u001b[39m, in \u001b[36mBrowserManager.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplaywright\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masync_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m async_playwright\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# Initialize playwright\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m \u001b[38;5;28mself\u001b[39m.playwright = \u001b[38;5;28;01mawait\u001b[39;00m async_playwright().start()\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.cdp_url \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_managed_browser:\n\u001b[32m    662\u001b[39m     \u001b[38;5;28mself\u001b[39m.config.use_managed_browser = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:51\u001b[39m, in \u001b[36mPlaywrightContextManager.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AsyncPlaywright:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__aenter__\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:46\u001b[39m, in \u001b[36mPlaywrightContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m playwright_future.done():\n\u001b[32m     45\u001b[39m     playwright_future.cancel()\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m playwright = AsyncPlaywright(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     47\u001b[39m playwright.stop = \u001b[38;5;28mself\u001b[39m.\u001b[34m__aexit__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m playwright\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\playwright\\_impl\\_transport.py:120\u001b[39m, in \u001b[36mPipeTransport.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    117\u001b[39m         startupinfo.wShowWindow = subprocess.SW_HIDE\n\u001b[32m    119\u001b[39m     executable_path, entrypoint_path = compute_driver_executable()\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28mself\u001b[39m._proc = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_subprocess_exec(\n\u001b[32m    121\u001b[39m         executable_path,\n\u001b[32m    122\u001b[39m         entrypoint_path,\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrun-driver\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    124\u001b[39m         stdin=asyncio.subprocess.PIPE,\n\u001b[32m    125\u001b[39m         stdout=asyncio.subprocess.PIPE,\n\u001b[32m    126\u001b[39m         stderr=_get_stderr_fileno(),\n\u001b[32m    127\u001b[39m         limit=\u001b[32m32768\u001b[39m,\n\u001b[32m    128\u001b[39m         env=env,\n\u001b[32m    129\u001b[39m         startupinfo=startupinfo,\n\u001b[32m    130\u001b[39m     )\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_error_future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\subprocess.py:224\u001b[39m, in \u001b[36mcreate_subprocess_exec\u001b[39m\u001b[34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[39m\n\u001b[32m    221\u001b[39m loop = events.get_running_loop()\n\u001b[32m    222\u001b[39m protocol_factory = \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit=limit,\n\u001b[32m    223\u001b[39m                                                     loop=loop)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m transport, protocol = \u001b[38;5;28;01mawait\u001b[39;00m loop.subprocess_exec(\n\u001b[32m    225\u001b[39m     protocol_factory,\n\u001b[32m    226\u001b[39m     program, *args,\n\u001b[32m    227\u001b[39m     stdin=stdin, stdout=stdout,\n\u001b[32m    228\u001b[39m     stderr=stderr, **kwds)\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:1756\u001b[39m, in \u001b[36mBaseEventLoop.subprocess_exec\u001b[39m\u001b[34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[39m\n\u001b[32m   1754\u001b[39m     debug_log = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mexecute program \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogram\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m   1755\u001b[39m     \u001b[38;5;28mself\u001b[39m._log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[32m-> \u001b[39m\u001b[32m1756\u001b[39m transport = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_subprocess_transport(\n\u001b[32m   1757\u001b[39m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[32m   1758\u001b[39m     bufsize, **kwargs)\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1760\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m, debug_log, transport)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:528\u001b[39m, in \u001b[36mBaseEventLoop._make_subprocess_transport\u001b[39m\u001b[34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_subprocess_transport\u001b[39m(\u001b[38;5;28mself\u001b[39m, protocol, args, shell,\n\u001b[32m    525\u001b[39m                                      stdin, stdout, stderr, bufsize,\n\u001b[32m    526\u001b[39m                                      extra=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    527\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create subprocess transport.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import sys\n",
    "import nest_asyncio\n",
    "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n",
    "\n",
    "# --- FIX 1: Windows Subprocess Support ---\n",
    "# This must run before any async code\n",
    "if sys.platform == 'win32':\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "\n",
    "# --- FIX 2: Apply Nest Asyncio Globally ---\n",
    "# This allows us to run \"sync\" wrappers inside a Notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "urls_to_crawl = [\n",
    "    \"https://docs.llamaindex.ai/en/stable/understanding/\",\n",
    "]\n",
    "\n",
    "def crawl_sync():\n",
    "    async def crawl_with_crawl4ai():\n",
    "        config = CrawlerRunConfig(\n",
    "            cache_mode=CacheMode.BYPASS,\n",
    "            page_timeout=80000,\n",
    "            word_count_threshold=50\n",
    "        )\n",
    "\n",
    "        data_res = {\"data\": []}\n",
    "\n",
    "        async with AsyncWebCrawler() as crawler:\n",
    "            results = await crawler.arun_many(\n",
    "                urls_to_crawl, \n",
    "                config=config\n",
    "            )\n",
    "\n",
    "            for result in results:\n",
    "                if result.success:\n",
    "                    # Logic to find title if missing\n",
    "                    title = result.metadata.get(\"title\", \"\")\n",
    "                    if not title and result.markdown:\n",
    "                        lines = result.markdown.raw_markdown.split('\\n')\n",
    "                        for line in lines:\n",
    "                            if line.startswith('#'):\n",
    "                                title = line.strip('#').strip()\n",
    "                                break\n",
    "                    \n",
    "                    data_res[\"data\"].append({\n",
    "                        \"text\": result.markdown.raw_markdown if result.markdown else \"\",\n",
    "                        \"meta\": {\n",
    "                            \"url\": result.url,\n",
    "                            \"meta\": {\n",
    "                                \"title\": title\n",
    "                            }\n",
    "                        }\n",
    "                    })\n",
    "        return data_res\n",
    "\n",
    "    # --- FIX 3: Re-use the Existing Loop ---\n",
    "    # Instead of creating a new loop, we ask Jupyter for the current one\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(crawl_with_crawl4ai())\n",
    "\n",
    "# Run the crawler\n",
    "data_res = crawl_sync()\n",
    "\n",
    "# Print results\n",
    "if data_res[\"data\"]:\n",
    "    print(\"URL:\", data_res[\"data\"][0][\"meta\"][\"url\"])\n",
    "    print(\"Title:\", data_res[\"data\"][0][\"meta\"][\"meta\"][\"title\"])\n",
    "    print(\"Content Sample:\", data_res[\"data\"][0][\"text\"][0:200], \"...\")\n",
    "else:\n",
    "    print(\"No data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e21802",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m         \u001b[38;5;28mprint\u001b[39m(result.markdown)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Run the async main function, adapting for Colab environment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Create an instance of AsyncWebCrawler\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m AsyncWebCrawler() \u001b[38;5;28;01mas\u001b[39;00m crawler:\n\u001b[32m      7\u001b[39m         \u001b[38;5;66;03m# Run the crawler on a URL\u001b[39;00m\n\u001b[32m      8\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m crawler.arun(url=\u001b[33m\"\u001b[39m\u001b[33mhttps://crawl4ai.com\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m         \u001b[38;5;66;03m# Print the extracted content\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\crawl4ai\\async_webcrawler.py:194\u001b[39m, in \u001b[36mAsyncWebCrawler.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.start()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\crawl4ai\\async_webcrawler.py:177\u001b[39m, in \u001b[36mAsyncWebCrawler.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    171\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[33;03m    Start the crawler explicitly without using context manager.\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[33;03m    This is equivalent to using 'async with' but gives more control over the lifecycle.\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[33;03m        AsyncWebCrawler: The initialized crawler instance\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.crawler_strategy.\u001b[34m__aenter__\u001b[39m()\n\u001b[32m    178\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCrawl4AI \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcrawl4ai_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, tag=\u001b[33m\"\u001b[39m\u001b[33mINIT\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m.ready = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\crawl4ai\\async_crawler_strategy.py:119\u001b[39m, in \u001b[36mAsyncPlaywrightCrawlerStrategy.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.start()\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\crawl4ai\\async_crawler_strategy.py:129\u001b[39m, in \u001b[36mAsyncPlaywrightCrawlerStrategy.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    126\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[33;03m    Start the browser and initialize the browser manager.\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.browser_manager.start()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execute_hook(\n\u001b[32m    131\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mon_browser_created\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    132\u001b[39m         \u001b[38;5;28mself\u001b[39m.browser_manager.browser,\n\u001b[32m    133\u001b[39m         context=\u001b[38;5;28mself\u001b[39m.browser_manager.default_context,\n\u001b[32m    134\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\crawl4ai\\browser_manager.py:659\u001b[39m, in \u001b[36mBrowserManager.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplaywright\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masync_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m async_playwright\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# Initialize playwright\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m \u001b[38;5;28mself\u001b[39m.playwright = \u001b[38;5;28;01mawait\u001b[39;00m async_playwright().start()\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.cdp_url \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_managed_browser:\n\u001b[32m    662\u001b[39m     \u001b[38;5;28mself\u001b[39m.config.use_managed_browser = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:51\u001b[39m, in \u001b[36mPlaywrightContextManager.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AsyncPlaywright:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__aenter__\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:46\u001b[39m, in \u001b[36mPlaywrightContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m playwright_future.done():\n\u001b[32m     45\u001b[39m     playwright_future.cancel()\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m playwright = AsyncPlaywright(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     47\u001b[39m playwright.stop = \u001b[38;5;28mself\u001b[39m.\u001b[34m__aexit__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m playwright\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\playwright\\_impl\\_transport.py:120\u001b[39m, in \u001b[36mPipeTransport.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    117\u001b[39m         startupinfo.wShowWindow = subprocess.SW_HIDE\n\u001b[32m    119\u001b[39m     executable_path, entrypoint_path = compute_driver_executable()\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28mself\u001b[39m._proc = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_subprocess_exec(\n\u001b[32m    121\u001b[39m         executable_path,\n\u001b[32m    122\u001b[39m         entrypoint_path,\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrun-driver\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    124\u001b[39m         stdin=asyncio.subprocess.PIPE,\n\u001b[32m    125\u001b[39m         stdout=asyncio.subprocess.PIPE,\n\u001b[32m    126\u001b[39m         stderr=_get_stderr_fileno(),\n\u001b[32m    127\u001b[39m         limit=\u001b[32m32768\u001b[39m,\n\u001b[32m    128\u001b[39m         env=env,\n\u001b[32m    129\u001b[39m         startupinfo=startupinfo,\n\u001b[32m    130\u001b[39m     )\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_error_future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\subprocess.py:224\u001b[39m, in \u001b[36mcreate_subprocess_exec\u001b[39m\u001b[34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[39m\n\u001b[32m    221\u001b[39m loop = events.get_running_loop()\n\u001b[32m    222\u001b[39m protocol_factory = \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit=limit,\n\u001b[32m    223\u001b[39m                                                     loop=loop)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m transport, protocol = \u001b[38;5;28;01mawait\u001b[39;00m loop.subprocess_exec(\n\u001b[32m    225\u001b[39m     protocol_factory,\n\u001b[32m    226\u001b[39m     program, *args,\n\u001b[32m    227\u001b[39m     stdin=stdin, stdout=stdout,\n\u001b[32m    228\u001b[39m     stderr=stderr, **kwds)\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:1756\u001b[39m, in \u001b[36mBaseEventLoop.subprocess_exec\u001b[39m\u001b[34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[39m\n\u001b[32m   1754\u001b[39m     debug_log = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mexecute program \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogram\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m   1755\u001b[39m     \u001b[38;5;28mself\u001b[39m._log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[32m-> \u001b[39m\u001b[32m1756\u001b[39m transport = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_subprocess_transport(\n\u001b[32m   1757\u001b[39m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[32m   1758\u001b[39m     bufsize, **kwargs)\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1760\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m, debug_log, transport)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:528\u001b[39m, in \u001b[36mBaseEventLoop._make_subprocess_transport\u001b[39m\u001b[34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_subprocess_transport\u001b[39m(\u001b[38;5;28mself\u001b[39m, protocol, args, shell,\n\u001b[32m    525\u001b[39m                                      stdin, stdout, stderr, bufsize,\n\u001b[32m    526\u001b[39m                                      extra=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    527\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create subprocess transport.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "\n",
    "async def main():\n",
    "    # Create an instance of AsyncWebCrawler\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        # Run the crawler on a URL\n",
    "        result = await crawler.arun(url=\"https://crawl4ai.com\")\n",
    "\n",
    "        # Print the extracted content\n",
    "        print(result.markdown)\n",
    "\n",
    "# Run the async main function, adapting for Colab environment\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
