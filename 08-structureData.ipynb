{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aebc963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import nest_asyncio\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "GOOGLE_SEARCH_KEY = os.getenv('GOOGLE_SEARCH_KEY')\n",
    "GOOGLE_SEARCH_ENGINE = os.getenv('GOOGLE_SEARCH_ENGINE')\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a806748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class Books(BaseModel):\n",
    "    \"\"\"Summary\"\"\"\n",
    "    title: str = Field(..., description=\"Title of the book\")\n",
    "    author: str = Field(..., description=\"Author of the book\")\n",
    "    yearPublished: int = Field(..., description=\"Year the book was published\", alias=\"yearPublished\")\n",
    "    summary: str = Field(..., description=\"Brief summary of the book\")\n",
    "\n",
    "\n",
    "class BookResponseFormatJson(BaseModel):\n",
    "    \"\"\"Response Format\"\"\"\n",
    "    Top10BoookingSelling: List[Books] = Field(..., description=\"List of top 10 best-selling books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\"Generate a list of the top 10 best-selling books of all time.\")\n",
    "response_obj = llm.structured_predict(\n",
    "  BookResponseFormatJson,\n",
    "  prompt=prompt_template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6f954e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Books(title='Don Quixote', author='Miguel de Cervantes', yearPublished=1605, summary='A Spanish noble reads so many chivalric romances that he decides to revive chivalry himself and sets out on a series of adventures as a knight-errant.'),\n",
       " Books(title='A Tale of Two Cities', author='Charles Dickens', yearPublished=1859, summary='Set during the French Revolution, it tells the story of the French doctor Manette, his daughter Lucie, and her two suitors, the French aristocrat Charles Darnay and the English lawyer Sydney Carton.'),\n",
       " Books(title='The Lord of the Rings', author='J.R.R. Tolkien', yearPublished=1954, summary='A hobbit named Frodo Baggins inherits a magical ring that he discovers is the One Ring, an evil artifact created by the Dark Lord Sauron. He embarks on a quest to destroy the Ring in the fires of Mount Doom.'),\n",
       " Books(title='The Little Prince', author='Antoine de Saint-Exupéry', yearPublished=1943, summary='A pilot stranded in the desert meets a young prince who has fallen to Earth from a tiny asteroid. The prince shares his travels and observations about adults.'),\n",
       " Books(title=\"Harry Potter and the Sorcerer's Stone\", author='J.K. Rowling', yearPublished=1997, summary='An orphaned boy, Harry Potter, discovers on his eleventh birthday that he is a wizard and is invited to study at Hogwarts School of Witchcraft and Wizardry.'),\n",
       " Books(title='And Then There Were None', author='Agatha Christie', yearPublished=1939, summary='Ten strangers are lured to an isolated island and, one by one, they are killed off according to the lines of a nursery rhyme.'),\n",
       " Books(title='The Hobbit', author='J.R.R. Tolkien', yearPublished=1937, summary='The hobbit Bilbo Baggins is swept into an epic quest by the wizard Gandalf and a company of thirteen dwarves to reclaim their treasure from the dragon Smaug.'),\n",
       " Books(title=\"Alice's Adventures in Wonderland\", author='Lewis Carroll', yearPublished=1865, summary='A young girl named Alice falls through a rabbit hole into a fantastical world populated by peculiar, anthropomorphic creatures.'),\n",
       " Books(title='The Lion, the Witch and the Wardrobe', author='C.S. Lewis', yearPublished=1950, summary='Four siblings discover a magical wardrobe that leads them to the land of Narnia, a world of talking animals and mythical creatures ruled by the White Witch.'),\n",
       " Books(title='Pinocchio', author='Carlo Collodi', yearPublished=1883, summary='A wooden puppet named Pinocchio dreams of becoming a real boy and embarks on a series of adventures where his nose grows whenever he tells a lie.')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_obj.Top10BoookingSelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab66f9e",
   "metadata": {},
   "source": [
    "## Strucutred output from PDF + OpenAI + pdf2images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4342db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "file_path = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"rag_research_paper.zip\",repo_type=\"dataset\",local_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0336517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_instruction_prompt =\"\"\"\n",
    "# You are an expert in extracting structured data from research paper PDFs.\n",
    "\n",
    "# Task Description:\n",
    "# Your task is to process an entire research paper provided as a PDF document and extract comprehensive, structured information from it. This includes all text, headlines, and detailed descriptions of visual elements. The final output must be a single, well-structured JSON object.\n",
    "\n",
    "# Must Follow Guidelines:\n",
    "# 1.  Process the Entire PDF: Treat the input as a complete document, not as individual pages.\n",
    "# 2.  Accurate Data Extraction: Extract all text and information with high precision. Do not summarize, paraphrase, or omit any details.\n",
    "# 3.  Logical Structure: Organize the extracted content into logical sections based on the paper's structure (e.g., Abstract, Introduction, Methods, Results, Conclusion, Appendices).\n",
    "# 4.  Complete Information: Ensure no information is fragmented. Merge text that spans columns or pages into coherent paragraphs and sentences.\n",
    "\n",
    "# Content Requirements:\n",
    "\n",
    "# 1.  Source Identification:\n",
    "#     *   Accurately extract the arXiv ID (e.g., `arXiv:2405.07437v2`). Verify its correctness. If no arXiv ID is present, use `null`.\n",
    "\n",
    "# 2.  Headlines and Sections:\n",
    "#     *   Extract all headlines and subheadlines to define the structure (e.g., \"1. Introduction,\" \"2.1. System Architecture\").\n",
    "#     *   If a section of content has no visible headline, generate a descriptive title for it based on its content.\n",
    "#     *   Each distinct section of the paper should become a separate object in the final JSON output.\n",
    "\n",
    "# 3.  Text Content:\n",
    "#     *   For each section, extract the complete and verbatim text. Preserve all technical details, equations, and specific terminology.\n",
    "\n",
    "# 4.  Visual Elements (Figures, Tables, Graphs, Architectures):\n",
    "#     *   Within the content of each section, when you encounter a visual element, provide a detailed analysis.\n",
    "#     *   **Title/Caption:** Extract the exact title and caption.\n",
    "#     *   **Detailed Description:** Describe the visual element's purpose and what it depicts.\n",
    "#     *   **Key Information:** Detail the main trends, data points, comparisons, and conclusions shown. For architectures, describe the components, layers, and data flow.\n",
    "#     *   **Contextual Insights:** Include any related insights or explanations from the surrounding text that refer to the visual element.\n",
    "\n",
    "# You are a data extraction engine. CRITICAL: Do not include more than two consecutive newlines. If you encounter empty space in the PDF, ignore it. Do not hallucinate content. Output only the requested JSON fields. Be concise.\n",
    "\n",
    "# Required Output Format (JSON):\n",
    "\n",
    "# Your output must be a single JSON object that strictly adheres to the following structure:\n",
    "\n",
    "# ```json\n",
    "# {\n",
    "#   \"source_name\": \"Extract complete arXiv ID including prefix (e.g., arXiv:2405.07437v2). If none, use null.\",\n",
    "#   \"source_id\": \"Extract complete arXiv ID including prefix (e.g., arXiv:2405.07437v2). If none, use null.\",\n",
    "#   \"research_paper_data\": [\n",
    "#     {\n",
    "#       \"content_title\": \"The title of the first section (e.g., 'Abstract').\",\n",
    "#       \"content\": \"The complete, verbatim text of the Abstract. Include descriptions of any visual elements if present.\"\n",
    "#     },\n",
    "#     {\n",
    "#       \"content_title\": \"The title of the second section (e.g., '1. Introduction').\",\n",
    "#       \"content\": \"The complete, verbatim text of the Introduction. This section should include detailed descriptions of any figures or tables found within it, as per the 'Visual Elements' guidelines.\"\n",
    "#     },\n",
    "#     .\n",
    "#     .\n",
    "#     .\n",
    "#   ]\n",
    "# }\n",
    "# ```\n",
    "# Key Guidelines:\n",
    "# - Extract exact content without summarization\n",
    "# - Ensure accuracy in complex technical details\n",
    "# - Maintain logical content organization\n",
    "# - Include complete visual element analysis\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96412962",
   "metadata": {},
   "source": [
    "I reduce the system instruction, because this will be helpful in the model like `GPT-5`\n",
    "Since, I'm using the Germini, I decided to minimize this, otherwise I was getting invalid json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35da6b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction_prompt = \"\"\"\n",
    "You are a data extraction bot. \n",
    "Output ONLY valid JSON. \n",
    "CRITICAL: Do not use tabs (\\\\t) or multiple newlines (\\\\n). \n",
    "Keep the JSON compact. If you find headers or footers in the PDF, ignore them. \n",
    "Do not repeat the paper title in the output.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c91145bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "# The response format- JSON schema\n",
    "class ResearchPaperData(BaseModel):\n",
    "  content_title: Optional[str] = Field(..., description=\"Extract or generate headlines and subheadlines (e.g., Abstract, Introduction, Methods, etc). Include section titles and subsection headings.\")\n",
    "  content: Optional[str] = Field(..., description=\"For each section: - Complete text content - Visual element descriptions - Figure/graph details: * Title/caption * Description * Key trends/comparisons * Architecture details * Related insights, Don't Summarize, Extract all the Content in the section\")\n",
    "\n",
    "class ResearchPaperResponseFormatJSON(BaseModel):\n",
    "  source_name: str = Field(..., description=\"Extract Research paper Title.\")\n",
    "  source_id: str = Field(..., description=\"Extract complete arXiv ID including prefix (e.g., arXiv:2405.07437v2). Verify ID accuracy multiple times. if there is no Arxiv ID return None\")\n",
    "  research_paper_data: List[ResearchPaperData] = Field(..., description=\"List of Extracted research paper data complete data without summarizing,\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c5f9846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import base64\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Parameters\n",
    "PDF_FOLDER = \"./data/rag_research_paper\"     \n",
    "PAGES_PER_CHUNK = 2                  \n",
    "SYSTEM_INSTRUCTION = system_instruction_prompt  \n",
    "RESPONSE_FORMAT = ResearchPaperResponseFormatJSON\n",
    "\n",
    "def split_pdf_by_pages(pdf_path, pages_per_chunk=PAGES_PER_CHUNK):\n",
    "    \"\"\"Split a single PDF into smaller page-range chunks.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    total_pages = len(reader.pages)\n",
    "    chunks = []\n",
    "\n",
    "    for start in range(0, total_pages, pages_per_chunk):\n",
    "        end = min(start + pages_per_chunk, total_pages)\n",
    "        writer = PdfWriter()\n",
    "        for page_idx in range(start, end):\n",
    "            writer.add_page(reader.pages[page_idx])\n",
    "        chunk_filename = f\"{os.path.splitext(os.path.basename(pdf_path))[0]}_\" \\\n",
    "                         f\"pages_{start+1}_{end}.pdf\"\n",
    "        chunk_path = os.path.join(PDF_FOLDER+\"/chunks_data/\", chunk_filename)\n",
    "        with open(chunk_path, \"wb\") as f_out:\n",
    "            writer.write(f_out)\n",
    "        chunks.append({\"path\": chunk_path, \"pages\": f\"{start+1}-{end}\"})\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a373b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_chunk(chunk, system_instructions, response_format):\n",
    "\n",
    "    pdf_path = chunk['path']\n",
    "    pages = chunk['pages']\n",
    "    print(f\"Processing chunk {pages} (Uploading {pdf_path})...\")\n",
    "\n",
    "    file_ref = genai.upload_file(pdf_path, mime_type=\"application/pdf\")\n",
    "\n",
    "    # 2. Wait for processing (Essential step)\n",
    "    # Gemini needs a moment to process the file before it can be queried\n",
    "    while file_ref.state.name == \"PROCESSING\":\n",
    "        time.sleep(1)\n",
    "        file_ref = genai.get_file(file_ref.name)\n",
    "\n",
    "    if file_ref.state.name == \"FAILED\":\n",
    "        print(f\"Uploading Failed for {pdf_path}\")\n",
    "\n",
    "    try:\n",
    "\n",
    "        config = genai.GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=response_format,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        model = genai.GenerativeModel(\n",
    "            model_name=\"gemini-2.5-flash-preview-09-2025\",\n",
    "            system_instruction=system_instructions\n",
    "        )\n",
    "\n",
    "        response = model.generate_content(\n",
    "            [file_ref, f\"Extract structured content from pages {chunk['pages']}.\"],\n",
    "            generation_config=config\n",
    "        )\n",
    "\n",
    "        # Check if it was cut off again\n",
    "        if response.candidates[0].finish_reason.name == \"MAX_TOKENS\":\n",
    "            print(f\"Warning: Chunk {pages} was truncated! Try reducing PAGES_PER_CHUNK further.\")\n",
    "\n",
    "        # --- DEBUG START ---\n",
    "        # print(f\"--- RAW OUTPUT FOR PAGES {pages} ---\")\n",
    "        # print(\"Last 200 characters of response.text:\", response.text[-600:], \"\\n\") # See the last 200 characters\n",
    "        # print(\"First 200 characters of response.text:\", response.text[:600], \"\\n\") # See the first 200 characters\n",
    "        \n",
    "        # Check why the model stopped\n",
    "        finish_reason = response.candidates[0].finish_reason.name\n",
    "        print(f\"Finish Reason: {finish_reason}\")\n",
    "\n",
    "        # The actual use of model_validate_json is to act as a bridge between a raw text string (which is what APIs send) and a Python object (which is what your code needs).\n",
    "        # Think of it as a \"parser + safety inspector\" combined.\n",
    "        # THE PROBLEM IT SOLVES - When you call any LLM (OpenAI, Gemini, Anthropic) directly, the model does not send back a Python object. It sends back a String\n",
    "        parse_obj = response_format.model_validate_json(response.text)\n",
    "        return parse_obj.research_paper_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting from chunk {pages}: {e}\")\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        # 6. Cleanup: Delete the file from Google's server\n",
    "        # This prevents cluttering your Google Cloud storage\n",
    "        try:\n",
    "            file_ref.delete()\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5a3aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dict(obj):\n",
    "    \"\"\"Convert Pydantic model or custom object to dictionary.\"\"\"\n",
    "    if hasattr(obj, 'model_dump'):\n",
    "        # For Pydantic v2\n",
    "        return obj.model_dump()\n",
    "    elif hasattr(obj, 'dict'):\n",
    "        # For Pydantic v1\n",
    "        return obj.dict()\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        # For regular objects\n",
    "        return obj.__dict__\n",
    "    else:\n",
    "        # If it's already a basic type\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "418bd14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2405.07437v2.pdf...\n",
      "** Processing chunk: 1-2\n",
      "Processing chunk 1-2 (Uploading ./data/rag_research_paper/chunks_data/2405.07437v2_pages_1_2.pdf)...\n",
      "Finish Reason: STOP\n",
      "** Data extracted.\n",
      "[ResearchPaperData(content_title='Abstract', content='Retrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of RAG systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the current RAG benchmarks, encompassing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of RAG benchmarks.'), ResearchPaperData(content_title='1 Introduction', content='Retrieval-Augmented Generation (RAG) [34] efficiently enhances the performance of generative language models through integrating information retrieval techniques. It addresses a critical challenge faced by standalone generative language models: the tendency to produce responses that, while plausible, may not be grounded in facts. By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [23] or factually incorrect outputs, thereby improving the content\\'s reliability and richness. [73] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a development in the pursuit of more intelligent and versatile language models [73,64]. Numerous studies of RAG systems have emerged from various perspectives since the advent of Large Language Models (LLMs) [55,45,59,42,41,69,16]. The RAG system comprises two primary components: Retrieval and Generation. The retrieval component aims to extract relevant information from various external knowledge sources. It involves two main phases, indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval [16,12,28]. The searching component utilizes these indexes to fetch relevant documents on the user\\'s query, often incorporating the optional rerankers [4,39,6,52] to refine the ranking of the retrieved documents. The generation component utilizes the retrieved content and question query to formulate coherent and contextually relevant responses with the prompting and inferencing phases. As the \"Emerging\" ability [59] of LLMs and the breakthrough in aligning human commands [42], LLMs are the best performance choices model for the generation stage. Prompting methods like Chain of Thought (CoT) [60], Tree of Thgouht [65], Rephrase and Respond (RaR) [8] guide better generation results. In the inferencing step, LLMs interpret the prompted input to generate accurate and in-depth responses that align with the query\\'s intent and integrate the extracted information [35,9] without further finetuning, such as fully finetuning [16,1,67,68] or LoRA [21]. Appendix A details the complete RAG structure. Figure 1 illustrates the structure of the RAG systems as mentioned. Fig. 1: The structure of the RAG system with retrieval and generation components and corresponding four phrases: indexing, search, prompting and inferencing. The pairs of \"Evaluable Outputs\" (EOs) and “Ground Truths” (GTs) are highlighted in read frame and green frame, with brown dashed arrows. The importance of evaluating RAG is increasing in parallel with the advancement of RAG-specific methodologies. On the one hand, RAG is a complex system intricately tied to specific requirements and language models, resulting in various evaluation methods, indicators, and tools, particularly given the black-box LLM generation. Evaluating RAG systems involves specific components and the complexity of the overall system assessment. On the other hand, the complexity of RAG systems is further compounded')] \n",
      "\n",
      "** Processing chunk: 3-4\n",
      "Processing chunk 3-4 (Uploading ./data/rag_research_paper/chunks_data/2405.07437v2_pages_3_4.pdf)...\n",
      "Finish Reason: STOP\n",
      "** Data extracted.\n",
      "[ResearchPaperData(content_title='Introduction/Contributions', content='by the external dynamic database and the various downstream tasks, such as content creation or open domain question answering [16,70]. These challenges necessitate the development of comprehensive evaluation metrics that can effectively capture the interplay between retrieval accuracy and generative quality [2,7]. To clarify the elements further, we try to address the current gaps in the area, which differs from the prior RAG surveys [74,16,24] that predominantly collected specific RAG methods or data. We have compiled 12 distinct evaluation frameworks, encompassing a range of aspects of the RAG system. Following the procedure of making benchmarks, we analyze through targets, datasets and metrics mentioned in these benchmarks and summarize them into A Unified Evaluation Process of RAG (Auepora) as three corresponding phases. For this paper, we contribute in the following aspects: 1. Challenge of Evaluation: This is the first work that summarizes and classifies the challenges in evaluating RAG systems through the structure of RAG systems, including three parts retrieval, generation, and the whole system. 2. Analysis Framework: In light of the challenges posed by RAG systems, we introduce an analytical framework, referred to as A Unified Evaluation Process of RAG (Auepora), which aims to elucidate the unique complexities inherent to RAG systems and guide for readers to comprehend the effectiveness of RAG benchmarks across various dimensions 3. RAG Benchmark Analysis: With the help of Auepora, we comprehensively analyze existing RAG benchmarks, highlighting their strengths and limitations and proposing recommendations for future developments in RAG system evaluation.'), ResearchPaperData(content_title='2 Challenges in Evaluating RAG Systems', content='Evaluating hybrid RAG systems entails evaluating retrieval, generation and the RAG system as a whole. These evaluations are multifaceted, requiring careful consideration and analysis. Each of them encompasses specific difficulties that complicate the development of a comprehensive evaluation framework and benchmarks for RAG systems.'), ResearchPaperData(content_title='Retrieval', content='The retrieval component is critical for fetching relevant information that informs the generation process. One primary challenge is the dynamic and vast nature of potential knowledge bases, ranging from structured databases to the entire web. This vastness requires evaluation metrics that can effectively measure the precision, recall, and relevance of retrieved documents in the context of a given query [52,32]. Moreover, the temporal aspect of information, where the relevance and accuracy of data can change over time, adds another layer of complexity to the evaluation process [6]. Additionally, the diversity of information sources and the possibility of retrieving misleading or low-quality information pose significant challenges in assessing the effectiveness of filtering and selecting the most pertinent information [39]. The traditional evaluation indicators for retrieval, such as Recall and Precision, cannot fully capture the nuances of RAG retrieval systems, necessitating the development of more nuanced and task-specific evaluation metrics [49].'), ResearchPaperData(content_title='Generation', content='The generation component, powered by LLMs, produces coherent and contextually appropriate responses based on the retrieved content. The challenge here lies in evaluating the faithfulness and accuracy of the generated content to the input data. This involves not only assessing the factual correctness of responses but also their relevance to the original query and the coherence of the generated text [75,49]. The subjective nature of certain tasks, such as creative content generation or open-ended question answering, further complicates the evaluation, as it introduces variability in what constitutes a “correct” or “high-quality\" response [48].'), ResearchPaperData(content_title='RAG System as a Whole', content=\"Evaluating the whole RAG system introduces additional complexities. The interplay between the retrieval and generation components means that the entire system's performance cannot be fully understood by evaluating each component in isolation [49,14]. The system needs to be assessed on its ability to leverage retrieved information effectively to improve response quality, which involves measuring the added value of the retrieval component to the generative process. Furthermore, practical considerations such as response latency and the ability to handle ambiguous or complex queries are also crucial for evaluating the system's overall effectiveness and usability [39,6].\"), ResearchPaperData(content_title='Conclusion (Challenges in Evaluating RAG Systems)', content='Evaluating the target shift from traditional absolute numeric metrics to multi-source and multi-target generation evaluation, along with the intricate interplay between retrieval and generation components, poses significant challenges. [5,50] Searches in a dynamic database may lead to misleading results or contradict the facts. Diverse and comprehensive datasets that accurately reflect real-world scenarios are crucial. Challenges also arise in the realm of metrics, encompassing generative evaluation criteria for distinct downstream tasks, human preferences, and practical considerations within the RAG system. Most prior benchmarks predominantly tackle one or several aspects of the RAG assessment but lack a comprehensive, holistic analysis.'), ResearchPaperData(content_title='3 A Unified Evaluation Process of RAG (Auepora)', content='To facilitate a deeper understanding of RAG benchmarks, we introduce A Unified Evaluation Process of RAG (Auepora), which focuses on three key questions of benchmarks: What to Evaluate? How to Evaluate? How to Measure? which correlated to Target, Dataset, and Metric respectively. We aim to provide a clear and accessible way for readers to comprehend the complexities and nuances of RAG benchmarking. The Target module is intended to determine the evaluation direction. The Dataset module facilitates the comparison of various data constructions in RAG benchmarks. The final module, Metrics, introduces the metrics that correspond to specific targets and datasets used during evaluation. Overall, it is designed to provide a systematic methodology for assessing the effectiveness of RAG systems across various aspects by covering all possible pairs at the beginning between the \"Evaluable Outputs” (EOs) and \"Ground Truths\" (GTs). In the following section, we will explain thoroughly Aueporaand utilize it for introducing and comparing the RAG benchmarks.')] \n",
      "\n",
      "** Processing chunk: 5-6\n",
      "Processing chunk 5-6 (Uploading ./data/rag_research_paper/chunks_data/2405.07437v2_pages_5_6.pdf)...\n",
      "Finish Reason: STOP\n",
      "** Data extracted.\n",
      "[ResearchPaperData(content_title='3.1 Evaluation Target (What to Evaluate?)', content='The combination of EOs and GTs in the RAG system can generate all possible targets, which is the fundamental concept of the Auepora (as shown in Figure 1). Once identified, these targets can be defined based on a specific pair of EOs or EO with GT, as illustrated in Figure 2, and used to analyze all aspects of current RAG benchmarks.Figure 2: The Target modular of the Auepora. The diagram illustrates the flow from Query to Retrieval (Result, Relevant Docs) and Generation (Response, Output), interacting with Ground Truth (Docs Candidates, Sample Response, Label). The resulting evaluation targets are Relevance, Accuracy (for Retrieval), Relevance, Faithfulness, Correctness (for Generation), and Additional Requirements (Latency, Noise Robustness, Negative Rejection, Diversity, etc.).'), ResearchPaperData(content_title='Retrieval', content='The EOs are the relevant documents for evaluating the retrieval component depending on the query. Then we can construct two pairwise relationships for the retrieval component, which are Relevant Documents \\t Query, Relevant Documents \\t Documents Candidates.'), ResearchPaperData(content_title='Relevance', content='(Relevant Documents \\t Query) evaluates how well the retrieved documents match the information needed expressed in the query. It measures the precision and specificity of the retrieval process.'), ResearchPaperData(content_title='Accuracy', content=\"(Relevant Documents \\t Documents Candidates) assesses how accurate the retrieved documents are in comparison to a set of candidate documents. It is a measure of the system's ability to identify and score relevant documents higher than less relevant or irrelevant ones.\"), ResearchPaperData(content_title='Generation', content='The similar pairwise relations for the generation components are listed below. The EOs are the generated text and phrased structured content. Then we need to compare these EOs with the provided GTs and labels.'), ResearchPaperData(content_title='Relevance', content='(Response \\t Query) measures how well the generated response aligns with the intent and content of the initial query. It ensures that the response is related to the query topic and meets the query’s specific requirements.'), ResearchPaperData(content_title='Faithfulness', content='(Response \\t Relevant Documents) evaluates if the generated response accurately reflects the information contained within the relevant documents and measures the consistency between generated content and the source documents.'), ResearchPaperData(content_title='Correctness', content='(Response \\t Sample Response) Similar to the accuracy in the retrieval component, this measures the accuracy of the generated response against a sample response, which serves as a ground truth. It checks if the response is correct in terms of factual information and appropriate in the context of the query.'), ResearchPaperData(content_title='Table 1: The evaluating targets and corresponding metrics across various frameworks for evaluating RAG systems.', content='The targets of Retrieval and Generation components are introduced. Table 1 lists the relative work on improving and evaluating RAG and its benchmarks cut off in June.Table 1: The evaluating targets and corresponding metrics across various frameworks for evaluating RAG systems. The presentation distinguishes between the core areas of Retrieval and Generation considered in the evaluation. The different aspects of the evaluation are set as different colours in the table: Relevance, Accuracy of Retrieval and Faithfulness, Correctness and Relevance of Generation. The consideration of the Additional Requirements beyond the retrieval and generation component is also collected. Noted that quite a few of the works employed multiple methods or evaluated multiple aspects simultaneously.||Category|Framework|Time|Raw Targets|Retrieval|Generation||Tool|TruEra RAG Triad [54]|2023.10|Context Relevance, Answer Relevance, Groundedness|LLM as a Judge|LLM as a Judge||Tool|LangChain Bench. [32]|2023.11|Accuracy, Faithfulness, Execution Time, Embed. CosDistance|Accuracy|LLM as a Judge||Tool|Databricks Eval [33]|2023.12|Correctness, Readability, Comprehensiveness|-|LLM as a Judge||Benchmark|RAGAS [14]|2023.09|Context Relevance, Answer Relevance, Faithfulness|LLM as a Judge|LLM Gen + CosSim, LLM as a Judge||Benchmark|RECALL [38]|2023.11|Response Quality, Robustness|-|BLEU, ROUGE-L||Benchmark|ARES [49]|2023.11|Context Relevance, Answer Faithfulness, Answer Relevance|LLM + Classifier|LLM + Classifier, LLM + Classifier||Benchmark|RGB [6]|2023.12|Information Integration, Noise Robustness, Negative Rejection, Counterfactual Robustness|-|Accuracy||Benchmark|MultiHop-RAG [52]|2024.01|Retrieval Quality, Response Correctness|MAP, MRR, Hit@K|LLM as a Judge||Benchmark|CRUD-RAG [39]|2024.02|CREATE, READ, UPDATE, DELETE|-|ROUGE, BLEU, RAGQuestEval||Benchmark|MedRAG [61]|2024.02|Accuracy|-|Accuracy||Benchmark|FeB4RAG [57]|2024.02|Consistency, Correctness, Clarity, Coverage|-|Human Evaluation, Human Evaluation||Benchmark|CDQA [62]|2024.03|Accuracy|-|F1||Benchmark|DomainRAG [58]|2024.06|Correctness, Faithfulness, Noise Robustness, Structural Output|-|F1, Exact-Match, Rouge-L, LLM as a Judge||Benchmark|ReEval [66]|2024.06|Hallucination|-|F1, Exacct-Match, LLM as a Judge, Human Evaluation||Research|FiD-Light [20]|2023.07|Latency|-|-||Research|Diversity Reranker [4]|2023.08|Diversity|Cosine Distance|-||')] \n",
      "\n",
      "** Processing chunk: 7-8\n",
      "Processing chunk 7-8 (Uploading ./data/rag_research_paper/chunks_data/2405.07437v2_pages_7_8.pdf)...\n",
      "Finish Reason: STOP\n",
      "** Data extracted.\n",
      "[ResearchPaperData(content_title='Evaluation of Retrieval-Augmented Generation: A Survey', content='2024. Table 1 portrays this information, where each evaluation criterion is represented by a different colour. For example, FeB4RAG [57], the fourth from the last, has posited four standards based on [17] that comprise Consistency, Correctness, Clarity, and Coverage. Correctness is equivalent to accuracy in retrieval, and Consistency is tantamount to faithfulness in the generation component. While accuracy in retrieval gauges the correctness of the retrieved information, we posit that Coverage pertains to the coverage rate and is more associated with diversity. Therefore, we consider Coverage to be linked with diversity and an additional requirement in our proposed evaluation framework, which will be introduced subsequently. The remaining standard, Clarity, is also classified as an additional requirement in our proposed framework. The other tools and benchmarks are processed similarly. Tools and benchmarks offer varying degrees of flexibility in evaluating datasets for RAG systems. Tools, which specify only evaluation targets, provide a versatile framework capable of constructing complete RAG applications and evaluation pipelines, as seen in works like [54,32,33]. Benchmarks, on the other hand, focus on different aspects of RAG evaluation with specific emphasis on either retrieval outputs or generation targets. For instance, RAGAS [14] and ARES [49] assess the relevance of retrieval documents, while RGB and MultiHop-RAG [6,52] prioritize accuracy, necessitating comparison with GTs. The [66] focuses on the Hallucination, which is a combination of faithfulness and correctness. All benchmarks consider generation targets due to their critical role in RAG systems, though their focus areas vary.'), ResearchPaperData(content_title='Additional Requirement', content=\"In addition to evaluating the two primary components outlined, a portion of the works also addressed some additional requirements of RAG (Black and Italics targets in Table 2). The requirements are as follows: Latency [20,32] measures how quickly the system can find information and respond, crucial for user experience. Diversity [4,32] checks if the system retrieves a variety of relevant documents and generates diverse responses. Noise Robustness [6] assesses how well the system handles irrelevant information without affecting response quality. Negative Rejection [6] gauges the system's ability to refrain from providing a response when the available information is insufficient. Counterfactual Robustness [6] evaluates the system's capacity to identify and disregard incorrect information, even when alerted about potential misinformation. More: For more human preferences considerations, there can be more additional requirements, such as readability [57,33], toxicity, perplexity [33], etc. For the exception, CRUD-RAG [39] introduces a comprehensive benchmark addressing the broader spectrum of RAG applications beyond question-answering, categorized into Create, Read, Update, and Delete scenarios. This benchmark evaluates RAG systems across diverse tasks, including text continuation, question answering, hallucination modification, and multi-document summarization. It offers insights for optimizing RAG technology across different scenarios. DomainRAG [58] identifies six complex abilities for RAG systems: conversational, structural information, faithfulness, denoising, time-sensitive problem solving, and multi-doc understanding. ReEval [66] specifically targets hallucination evaluation by employing a cost-effective LLM-based framework that utilizes prompt chaining to create dynamic test cases.\"), ResearchPaperData(content_title='Table 2: The evaluation datasets used for each benchmark.', content='The evaluation datasets used for each benchmark. The dataset without citation was constructed by the benchmark itself. Benchmark: RAGAS [14], Dataset: WikiEval. Benchmark: RECALL [38], Dataset: EventKG [19], UJ [22]. Benchmark: ARES [49], Dataset: NQ [29], Hotpot [63], FEVER [53], WoW [11], MultiRC [10], ReCoRD [71]. Benchmark: RGB [6], Dataset: Generated (Source: News). Benchmark: MultiHop-RAG [52], Dataset: Generated (Source: News). Benchmark: CRUD-RAG [39], Dataset: Generated (Source: News). Benchmark: MedRAG [61], Dataset: UHGEval [36]. Benchmark: FeB4RAG [57], Dataset: MIRAGE. Benchmark: CDQA [62], Dataset: FeB4RAG, BEIR [26]. Benchmark: DomainRAG [58], Dataset: Generation (Source: News), Labeller. Benchmark: ReEval [66], Dataset: Generation (Source: College Admission Information), RealTimeQA [27], NQ [15,29]).'), ResearchPaperData(content_title='3.2 Evaluation Dataset (How to evaluate?)', content=\"In Table 2, distinct benchmarks employ varying strategies for dataset construction, ranging from leveraging existing resources to generating entirely new data tailored for specific evaluation aspects. Several benchmarks draw upon the part of KILT (Knowledge Intensive Language Tasks) benchmark [44] (Natural Questions [29], HotpotQA [63], and FEVER [53]) and other established datasets such as SuperGLUE [56] (MultiRC [10], and ReCoRD [71]) [49]. However, the drawback of using such datasets can't solve the challenges in dynamic real-world scenarios. A similar situation can be observed in WikiEval, from Wikipedia pages post 2022, constructed by RAGAs [14]. The advent of powerful LLMs has revolutionized the process of dataset construction. With the ability to design queries and ground truths for specific evaluation targets using these frameworks, authors can now create datasets in the desired format with ease. Benchmarks like RGB, MultiHop-RAG, CRUD-RAG, and CDQA [6,52,39,62] have taken this approach further by building their own datasets using online news articles to test RAG systems' ability to handle real-world information beyond the training data of LM frameworks. Most recently, DomainRAG [58] combines various types of QA datasets with single-doc, multi-doc, single-round, and multi-round. These datasets are generated from the yearly changed information from the college website for admission and enrollment, which forces the LLMs to use the provided and updated information.\")] \n",
      "\n",
      "** Processing chunk: 9-10\n",
      "Processing chunk 9-10 (Uploading ./data/rag_research_paper/chunks_data/2405.07437v2_pages_9_10.pdf)...\n",
      "Finish Reason: STOP\n",
      "** Data extracted.\n",
      "[ResearchPaperData(content_title='Summary of Dataset Selection (Continuation)', content='In summary, the creation and selection of datasets are crucial for evaluating RAG systems. Datasets tailored for specific metrics or tasks improve evaluation accuracy and guide the development of adaptable RAG systems for real-world information needs.'), ResearchPaperData(content_title='3.3 Evaluation Metric (How to quantify?)', content='Navigating the intricate terrain of evaluating RAG systems necessitates a nuanced understanding of the metrics that can precisely quantify the evaluation targets. However, creating evaluative criteria that align with human preferences and address practical considerations is challenging. Each component within the RAG systems requires a tailored evaluative approach that reflects its distinct functionalities and objectives.'), ResearchPaperData(content_title='Retrieval Metrics', content=\"Various targets can be evaluated with various metrics that correspond to the given datasets. This section will introduce several commonly used metrics for retrieval and generation targets. The metrics for additional requirements can also be found in these commonly used metrics. The more specifically designed metrics can be explored in the original paper via Table 1 as a reference.For the retrieval evaluation, the focus is on metrics that can accurately capture the relevance, accuracy, diversity, and robustness of the information retrieved in response to queries. These metrics must not only reflect the system's precision in fetching pertinent information but also its resilience in navigating the dynamic, vast, and sometimes misleading landscape of available data. The deployment of metrics like Misleading Rate, Mistake Reappearance Rate, and Error Detection Rate within the [38] benchmark underscores a heightened awareness of RAG systems' inherent intricacies. The integration of MAP@K, MRR@K, and Tokenization with F1 into benchmarks like [52,62] mirrors a deepening comprehension of traditional retrieval's multifaceted evaluation. While the [17] also emphasizes that this ranking-based evaluation methodology is not unsuitable for the RAG system, and should have more RAG-specific retrieval evaluation metrics. These metrics not only capture the precision and recall of retrieval systems but also account for the diversity and relevance of retrieved documents, aligning with the complex and dynamic nature of information needs in RAG systems. The introduction of LLMs as evaluative judges, as seen in [14], further underscores the adaptability and versatility of retrieval evaluation, offering a comprehensive and context-aware approach to assessing retrieval quality.\"), ResearchPaperData(content_title='Non-Rank Based Metrics', content='often assess binary outcomes—whether an item is relevant or not without considering the position of the item in a ranked list. Notice, that the following formula is just one format of these metrics, the definition of each metric may vary by the different evaluating tasks.Accuracy is the proportion of true results (both true positives and true negatives) among the total number of cases examined.Precision is the fraction of relevant instances among the retrieved instances, Precision = TP / (TP + FP) where TP represents true positives and FP represents false positives.Recall at k (Recall@k) is the fraction of relevant instances that have been retrieved over the total amount of relevant cases, considering only the top k results. Recall@k = |RD ∩ Topkd| / |RD| where RD is the relevant documents, and Topkd is the top-k retrieved documents.'), ResearchPaperData(content_title='Rank-Based Metrics', content='evaluate the order in which relevant items are presented, with higher importance placed on the positioning of relevant items at the ranking list.Mean Reciprocal Rank (MRR) is the average of the reciprocal ranks of the first correct answer for a set of queries. MRR = (1/|Q|) * Σ_{i=1}^{|Q|} (1/rank_i) where |Q| is the number of queries and rank_i is the rank position of the first relevant document for the i-th query.Mean Average Precision (MAP) is the mean of the average precision scores for each query. MAP = (1/|Q|) * Σ_{q=1}^{|Q|} (Σ_{k=1}^{n} (P(k) × rel(k))) / |relevant documents_q| where P(k) is the precision at cutoff k in the list, rel(k) is an indicator function equaling 1 if the item at rank k is a relevant document, 0 otherwise, and n is the number of retrieved documents.'), ResearchPaperData(content_title='Generation Metrics', content=\"In the realm of generation, evaluation transcends the mere accuracy of generated responses, venturing into the quality of text in terms of coherence, relevance, fluency, and alignment with human judgment. This necessitates metrics that can assess the nuanced aspects of language production, including factual correctness, readability, and user satisfaction with the generated content. The traditional metrics like BLEU, ROUGE, and F1 Score continue to play a crucial role, emphasizing the significance of precision and recall in determining response quality. Yet, the advent of metrics such as Misleading Rate, Mistake Reappearance Rate, and Error Detection Rate highlights an evolving understanding of RAG systems' distinct challenges [38].The evaluation done by humans is still a very significant standard to compare the performance of generation models with one another or with the ground truth. The approach of employing LLMs as evaluative judges [75] is a versatile and automatic method for quality assessment, catering to instances where traditional ground truths may be elusive [14]. This methodology benefits from employing prediction-powered inference (PPI) and context relevance scoring, offering a nuanced lens through which LLM output can be assessed. [49] The strategic use of detailed prompt templates ensures a guided assessment aligned with human preferences, effectively standardizing evaluations across various content dimensions [1]. This shift towards leveraging LLMs\")] \n",
      "\n",
      "** Processing chunk: 11-12\n",
      "Processing chunk 11-12 (Uploading ./data/rag_research_paper/chunks_data/2405.07437v2_pages_11_12.pdf)...\n",
      "Finish Reason: STOP\n",
      "** Data extracted.\n",
      "[ResearchPaperData(content_title='ROUGE', content='as arbiters mark a significant progression towards automated and context-responsive evaluation frameworks, enriching the evaluation landscape with minimal reliance on reference comparisons. ROUGE Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [37] is a set of metrics designed to evaluate the quality of summaries by comparing them to human-generated reference summaries. ROUGE can be indicative of the content overlap between the generated text and the reference text. The variants of ROUGES measure the overlap of n-grams (ROUGE-N, ROUGGE-W), word subsequences (ROUGE-L, ROUGGE-S), and word pairs between the system-generated summary and the reference summaries.'), ResearchPaperData(content_title='BLEU', content='Bilingual Evaluation Understudy (BLEU) [43] is a metric for evaluating the quality of machine-translated text against one or more reference translations. BLEU calculates the precision of n-grams in the generated text compared to the reference text and then applies a brevity penalty to discourage overly short translations. BLEU has limitations, such as not accounting for the fluency or grammaticality of the generated text.'), ResearchPaperData(content_title='BertScore', content='BertScore [72] leverages the contextual embedding from pre-trained transformers like BERT to evaluate the semantic similarity between generated text and reference text. BertScore computes token-level similarity using contextual embedding and produces precision, recall, and F1 scores. Unlike n-gram-based metrics, BertScore captures the meaning of words in context, making it more robust to paraphrasing and more sensitive to semantic equivalence.'), ResearchPaperData(content_title='LLM as a Judge', content='Using \"LLM as a Judge\" for evaluating generated text is a more recent approach. [75] In this method, LLMs are used to score the generated text based on criteria such as coherence, relevance, and fluency. The LLM can be optionally finetuned on human judgments to predict the quality of unseen text or used to generate evaluations in a zero-shot or few-shot setting. This approach leverages the LLM\\'s understanding of language and context to provide a more nuanced text quality assessment. For instance, [1] illustrates how providing LLM judges with detailed scoring guidelines, such as a scale from 1 to 5, can standardize the evaluation process. This methodology encompasses critical aspects of content assessment, including coherence, relevance, fluency, coverage, diversity, and detail - both in the context of answer evaluation and query formulation.'), ResearchPaperData(content_title='Additional Requirements', content='These additional requirements, such as latency, diversity, noise robustness, negative rejection, and counterfactual robustness, are used to ensure the practical applicability of RAG systems in real-world scenarios aligned with human preference. This section delves into the metrics used for evaluating these additional requirements, highlighting their significance in the comprehensive assessment of RAG systems.'), ResearchPaperData(content_title='Latency', content='measures the time taken by the RAG system to finish the response of one query. It is a critical factor for user experience, especially in interactive applications such as chatbots or search engines [20]. Single Query Latency: The mean time is taken to process a single query, including both retrieval and generating phases.'), ResearchPaperData(content_title='Diversity', content='evaluates the variety and breadth of information retrieved and generated by the RAG system. It ensures that the system can provide a wide range of perspectives and avoid redundancy in responses [4]. Cosine Similarity / Cosine Distance: The cosine similarity/distance calculates embeddings of retrieved documents or generated responses. [30] Lower cosine similarity scores indicate higher diversity, suggesting that the system can retrieve or generate a broader spectrum of information.'), ResearchPaperData(content_title='Noise Robustness', content=\"measures the RAG system's ability to handle irrelevant or misleading information without compromising the quality of the response [38]. The metrics Misleading Rate and Mistake Reappearance Rate are described in [38], providing detailed descriptions tailored to the specific dataset and experimental setup. [58]\"), ResearchPaperData(content_title='Negative Rejection', content=\"evaluates the system's capability to withhold responses when the available information is insufficient or too ambiguous to provide an accurate answer [6]. Rejection Rate: The rate at which the system refrains from generating a response.\"), ResearchPaperData(content_title='Counterfactual Robustness', content=\"Counterfactual robustness assesses the system's ability to identify and disregard incorrect or counterfactual information within the retrieved documents [39]. Error Detection Rate: The ratio of counterfactual statements detected in retrieved information.\"), ResearchPaperData(content_title='4 Discussion', content=\"For RAG systems, traditional Question Answering (QA) datasets and metrics remain a common format for interaction. [14,49,38,6,61,62,58,66] While these provide a basic verification of RAG's capabilities, it becomes challenging to distinguish the impact of retrieval components when faced with strong Language Models (LLMs) capable of excelling in QA benchmarks. To comprehensively evaluate the performance of entire RAG systems, there is a need for diverse and RAG-specific benchmarks. Several papers offer guidance on improving QA format benchmarks, including variations in question types: from simple Wikipedia filling questions to multi-hop [52], multi-document questions [66] and single-round to multi-round dialogue [39,58]. For answers, aspects such as structural output [58], content moderation [6,54], and hallucination [66] can be considered when evaluating relevance, faithfulness, and correctness. In addition to these, RAG systems require additional requirements such as robustness to noisy documents, language expression, latency, and result diversity. [32,33,38,6,39,57,58,20,4] Furthermore, research is needed on performance changes involving intermediate outputs and retrieved documents, as well as the relationship and analysis between retrieval metrics and final generation outputs. Regarding datasets, creating a universal dataset was challenging due to the target-specific nature of different RAG benchmarks. Tailored datasets [14,38,49,39,57] are necessary for a thorough evaluation, but this approach increases the effort and resources required. Moreover, the diversity of datasets, from news articles to structured databases [66], reflects the adaptability required of RAG systems but also poses a barrier to streamlined evaluation. Recently, with the cutting-edge performance of LLMs, complex data processing and automatic QA pair generation can be automated to achieve daily or finer-grained time resolution, preventing LLMs from cheating and evaluating the robustness of RAG systems in rapidly changing data. [6,52,39,62,58,66]\")] \n",
      "\n",
      "** Processing chunk: 13-14\n",
      "Processing chunk 13-14 (Uploading ./data/rag_research_paper/chunks_data/2405.07437v2_pages_13_14.pdf)...\n",
      "Finish Reason: STOP\n",
      "** Data extracted.\n",
      "[ResearchPaperData(content_title='Evaluation Metrics and Challenges', content='When it comes to metrics, the use of LLMs as automatic evaluative judges signifies a burgeoning trend, promising versatility and depth in generative outputs with reasoning on a large scale compared to human evaluation. However, using \"LLMs as a Judge\" [75] for responses presents challenges in aligning with human judgment, establishing effective grading scales, and applying consistent evaluation across varied use cases. Determining correctness, clarity, and richness can differ between automated and human assessments. Moreover, the effectiveness of example-based scoring can vary, and there\\'s no universally applicable grading scale and prompting text, complicating the standardization of “LLM as a Judge\". [33] In addition to the challenges mentioned above, it is important to consider the resource-intensive nature [76] of using Large Language Models (LLMs) for data generation and validation. RAG benchmarks must balance the need for thorough evaluation with the practical constraints of limited computational resources. As such, it is desirable to develop evaluation methodologies that can effectively assess RAG systems using smaller amounts of data while maintaining the validity and reliability of the results.'), ResearchPaperData(content_title='5 Conclusion', content='This survey systematically explores the complexities of evaluating RAG systems, highlighting the challenges in assessing their performance. Through the proposed A Unified Evaluation Process of RAG, we outline a structured approach to analyzing RAG evaluations, focusing on targets, datasets and measures. Our analysis emphasizes the need for targeted benchmarks that reflect the dynamic interplay between retrieval accuracy and generative quality and practical considerations for real-world applications. By identifying gaps in current methodologies and suggesting future research directions, we aim to contribute to more effective, and user-aligned benchmarks of RAG systems.'), ResearchPaperData(content_title='References', content=\"1. Balaguer, A., Benara, V., Cunha, R.L.d.F., Filho, R.d.M.E., Hendry, T., Holstein, D., Marsman, J., Mecklenburg, N., Malvar, S., Nunes, L.O., Padilha, R., Sharp, M., Silva, B., Sharma, S., Aski, V., Chandra, R.: RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. Tech. rep. (Jan 2024), http://arxiv.org/abs/2401.08406, arXiv:2401.08406 [cs] type: article 2. Barnett, S., Kurniawan, S., Thudumu, S., Brannelly, Z., Abdelrazek, M.: Seven failure points when engineering a retrieval augmented generation system (Jan 2024). https://doi.org/10.48550/ARXIV.2401.05856 3. Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Podstawski, M., Gianinazzi, L., Gajda, J., Lehmann, T., Niewiadomski, H., Nyczyk, P., Hoefler, T.: Graph of thoughts: Solving elaborate problems with large language models. Proceedings of the AAAI Conference on Artificial Intelligence 2024 (AAAI'24) (Aug 2023). https://doi.org/10.48550/ARXIV.2308.09687 4. Blagojevic, V.: Enhancing RAG Pipelines in Haystack: Introducing DiversityRanker and LostInTheMiddleRanker (Aug 2023), https://towardsdatascience.com/enhancing-rag-pipelines-in-haystack-45f14e2bc9f5 5. Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., et al.: A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology 15(3), 1–45 (2024) 6. Chen, J., Lin, H., Han, X., Sun, L.: Benchmarking large language models in retrieval-augmented generation (Sep 2023). https://doi.org/10.48550/ARXIV.2309.01431 7. Cuconasu, F., Trappolini, G., Siciliano, F., Filice, S., Campagnano, C., Maarek, Y., Tonellotto, N., Silvestri, F.: The power of noise: Redefining retrieval for rag systems (Jan 2024). https://doi.org/10.48550/ARXIV.2401.14887 8. Deng, Y., Zhang, W., Chen, Z., Gu, Q.: Rephrase and respond: Let large language models ask better questions for themselves (Nov 2023). https://doi.org/10.48550/ARXIV.2311.04205 9. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In: Burstein, J., Doran, C., Solorio, T. (eds.) Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 4171-4186. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019). https://doi.org/10.18653/v1/N19-1423, https://aclanthology.org/N19-1423 10. De Young, J., Jain, S., Rajani, N.F., Lehman, E., Xiong, C., Socher, R., Wallace, B.C.: Eraser: A benchmark to evaluate rationalized nlp models 11. Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., Weston, J.: Wizard of Wikipedia: Knowledge-powered conversational agents. In: Proceedings of the International Conference on Learning Representations (ICLR) (2019) 12. Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazaré, P.E., Lomeli, M., Hosseini, L., Jégou, H.: The faiss library (2024) 13. DuckDuckGo: DuckDuckGo Privacy, simplified. (2024), https://duckduckgo.com//home 14. Es, S., James, J., Espinosa-Anke, L., Schockaert, S.: Ragas: Automated evaluation of retrieval augmented generation (Sep 2023). https://doi.org/10.48550/ARXIV.2309.15217\")] \n",
      "\n",
      "** Processing chunk: 15-16\n",
      "Processing chunk 15-16 (Uploading ./data/rag_research_paper/chunks_data/2405.07437v2_pages_15_16.pdf)...\n",
      "Finish Reason: STOP\n",
      "** Data extracted.\n",
      "[ResearchPaperData(content_title='15.', content='Fisch, A., Talmor, A., Jia, R., Seo, M., Choi, E., Chen, D.: MRQA 2019 shared task: Evaluating generalization in reading comprehension. In: Fisch, A., Talmor, A., Jia, R., Seo, M., Choi, E., Chen, D. (eds.) Proceedings of the 2nd Workshop on Machine Reading for Question Answering. pp. 1-13. Association for Computational Linguistics, Hong Kong, China (Nov 2019). https://doi.org/10.18653/v1/D19-5801, https://aclanthology.org/D19-5801'), ResearchPaperData(content_title='16.', content='Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Guo, Q., Wang, M., Wang, H.: Retrieval-Augmented Generation for Large Language Models: A Survey. Tech. rep. (Jan 2024), http://arxiv.org/abs/2312.10997, arXiv:2312.10997 [cs] type: article'), ResearchPaperData(content_title='17.', content='Gienapp, L., Scells, H., Deckers, N., Bevendorff, J., Wang, S., Kiesel, J., Syed, S., Fröbe, M., Zuccon, G., Stein, B., Hagen, M., Potthast, M.: Evaluating Generative Ad Hoc Information Retrieval. Tech. rep. (Nov 2023), http://arxiv.org/abs/2311.04694, arXiv:2311.04694 [cs] type: article'), ResearchPaperData(content_title='18.', content='Google: Programmable Search Engine | Google for Developers (2024), https://developers.google.com/custom-search'), ResearchPaperData(content_title='19.', content='Gottschalk, S., Demidova, E.: Eventkg: A multilingual event-centric temporal knowledge graph (Apr 2018). https://doi.org/10.48550/ARXIV.1804.04526'), ResearchPaperData(content_title='20.', content=\"Hofstätter, S., Chen, J., Raman, K., Zamani, H.: FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation. In: Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 1437-1447. SIGIR '23, Association for Computing Machinery, New York, NY, USA (Jul 2023). https://doi.org/10.1145/3539618.3591687, https://doi.org/10.1145/3539618.3591687\"), ResearchPaperData(content_title='21.', content='Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: LORA: Low-Rank Adaptation of Large Language Models. Tech. rep. (Oct 2021). https://doi.org/10.48550/arXiv.2106.09685, http://arxiv.org/abs/2106.09685, arXiv:2106.09685 [cs] type: article'), ResearchPaperData(content_title='22.', content='Huang, J., Shao, H., Chang, K.C.C., Xiong, J., Hwu, W.m.: Understanding jargon: Combining extraction and generation for definition modeling. In: Proceedings of EMNLP (2022)'), ResearchPaperData(content_title='23.', content='Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., Liu, T.: A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions (Nov 2023). https://doi.org/10.48550/ARXIV.2311.05232'), ResearchPaperData(content_title='24.', content='Huang, Y., Huang, J.: A survey on retrieval-augmented text generation for large language models (Apr 2024). https://doi.org/10.48550/ARXIV.2404.10981'), ResearchPaperData(content_title='25.', content='Johnson, J., Douze, M., Jégou, H.: Billion-scale similarity search with GPUs. IEEE Transactions on Big Data 7(3), 535-547 (2019)'), ResearchPaperData(content_title='26.', content='Kamalloo, E., Thakur, N., Lassance, C., Ma, X., Yang, J.H., Lin, J.: Resources for brewing beir: Reproducible reference models and an official leaderboard (2023)'), ResearchPaperData(content_title='27.', content=\"Kasai, J., Sakaguchi, K., Takahashi, Y., Bras, R.L., Asai, A., Yu, X., Radev, D., Smith, N.A., Choi, Y., Inui, K.: Realtime qa: What's the answer right now? (Jul 2022). https://doi.org/10.48550/ARXIV.2207.13332, https://arxiv.org/abs/2207.13332\"), ResearchPaperData(content_title='28.', content='Khattab, O., Zaharia, M.: Colbert: Efficient and effective passage search via contextualized late interaction over bert (Apr 2020). https://doi.org/10.48550/ARXIV.2004.12832'), ResearchPaperData(content_title='29.', content='Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.W., Dai, A.M., Uszkoreit, J., Le, Q., Petrov, S.: Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics 7, 453-466 (2019). https://doi.org/10.1162/tacl_a_00276, https://doi.org/10.1162/tacl_a_00276'), ResearchPaperData(content_title='30.', content='Lahitani, A.R., Permanasari, A.E., Setiawan, N.A.: Cosine similarity to determine similarity measure: Study case in online essay assessment. In: 2016 4th International Conference on Cyber and IT Service Management. pp. 1-6 (2016). https://doi.org/10.1109/CITSM.2016.7577578'), ResearchPaperData(content_title='31.', content='Lanchantin, J., Toshniwal, S., Weston, J., Szlam, A., Sukhbaatar, S.: Learning to reason and memorize with self-notes (May 2023). https://doi.org/10.48550/ARXIV.2305.00833'), ResearchPaperData(content_title='32.', content='LangChain: Evaluating rag architectures on benchmark tasks (Nov 2023), https://langchain-ai.github.io/langchain-benchmarks/notebooks/retrieval/langchain_docs_qa.html'), ResearchPaperData(content_title='33.', content='Leng, Q., Uhlenhuth, K., Polyzotis, A.: Best Practices for LLM Evaluation of RAG Applications (Dec 2023), https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG'), ResearchPaperData(content_title='34.', content=\"Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.t., Rocktäschel, T., Riedel, S., Kiela, D.: Retrieval-augmented generation for knowledge-intensive NLP tasks. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. pp. 9459–9474. NIPS'20, Curran Associates Inc., Red Hook, NY, USA (Dec 2020)\"), ResearchPaperData(content_title='35.', content='Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.t., Rocktäschel, T., Riedel, S., Kiela, D.: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Tech. rep. (Apr 2021), http://arxiv.org/abs/2005.11401, arXiv:2005.11401 [cs] type: article'), ResearchPaperData(content_title='36.', content='Liang, X., Song, S., Niu, S., Li, Z., Xiong, F., Tang, B., Wy, Z., He, D., Cheng, P., Wang, Z., Deng, H.: Uhgeval: Benchmarking the hallucination of chinese large language models via unconstrained generation. arXiv preprint arXiv:2311.15296 (2023)'), ResearchPaperData(content_title='37.', content='Lin, C.Y.: ROUGE: A package for automatic evaluation of summaries. In: Text Summarization Branches Out. pp. 74-81. Association for Computational Linguistics, Barcelona, Spain (Jul 2004), https://aclanthology.org/W04-1013'), ResearchPaperData(content_title='38.', content='Liu, Y., Huang, L., Li, S., Chen, S., Zhou, H., Meng, F., Zhou, J., Sun, X.: Recall: A benchmark for llms robustness against external counterfactual knowledge (Nov 2023). https://doi.org/10.48550/ARXIV.2311.08147'), ResearchPaperData(content_title='39.', content='Lyu, Y., Li, Z., Niu, S., Xiong, F., Tang, B., Wang, W., Wu, H., Liu, H., Xu, T., Chen, E., Luo, Y., Cheng, P., Deng, H., Wang, Z., Lu, Z.: Crud-rag: A comprehensive chinese benchmark for retrieval-augmented generation of large language models (Jan 2024). https://doi.org/10.48550/ARXIV.2401.17043'), ResearchPaperData(content_title='40.', content='Microsoft: Web Search API | Microsoft Bing, https://www.microsoft.com/en-us/bing/apis/bing-web-search-api'), ResearchPaperData(content_title='41.', content='OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.L., Brockman, G., Brooks, T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson, C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H.W., Cummings, D., Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S.P., Forte, J., Fulford, I., Gao, L., Georges, E., Gibson, C., Goel, V., Gogineni, T., ...')] \n",
      "\n",
      "** Processing chunk: 17-18\n",
      "Processing chunk 17-18 (Uploading ./data/rag_research_paper/chunks_data/2405.07437v2_pages_17_18.pdf)...\n",
      "Finish Reason: STOP\n",
      "** Data extracted.\n",
      "[ResearchPaperData(content_title='References', content=\"Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S.S., Guo, Y., Hallacy, C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Kamali, A., Kanitscheider, I., Keskar, N.S., Khan, T., Kilpatrick, L., Kim, J.W., Kim, C., Kim, Y., Kirchner, J.H., Kiros, J., Knight, M., Kokotajlo, D., Kondraciuk, A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D., Li, C.M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju, A., Malfacini, K., Manning, S., Markov, T., Markovski, Y., Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S.M., McLeavey, C., McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V., Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O., Mély, D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang, L., O'Keefe, C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo, G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A., Peres, F.d.A.B., Petrov, M., Pinto, H.P.d.O., Michael, Pokorny, Pokrass, M., Pong, V.H., Powell, T., Power, A., Power, B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D., Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N., Such, F.P., Summers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M.B., Tillet, P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J.F.C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J.J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann, C., Welihinda, A., Welinder, P., Weng, J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., Zoph, B.: GPT-4 Technical Report (Mar 2023). https://doi.org/10.48550/ARXIV.2303.08774\\n42. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., Lowe, R.: Training language models to follow instructions with human feedback. Tech. rep. (Mar 2022). https://doi. org/10.48550/arXiv.2203.02155, http://arxiv.org/abs/2203.02155, arXiv:2203.02155 [cs] type: article\\n43. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of machine translation. In: Isabelle, P., Charniak, E., Lin, D. (eds.) Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311–318. Association for Computational Linguistics, Philadelphia, Pennsylvania, USA (Jul 2002). https://doi. org/10.3115/1073083.1073135, https://aclanthology.org/P02-1040\\n44. Petroni, F., Piktus, A., Fan, A., Lewis, P., Yazdani, M., De Cao, N., Thorne, J., Jernite, Y., Karpukhin, V., Maillard, J., Plachouras, V., Rocktäschel, T., Riedel, S.: KILT: a benchmark for knowledge intensive language tasks. In: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2523-2544. Association for Computational Linguistics, Online (Jun 2021). https://doi.org/10.18653/v1/2021.naacl-main.200, https://aclanthology.org/2021.naacl-main.200\\n45. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019)\\n46. Ramos, J., et al.: Using tf-idf to determine word relevance in document queries. In: Proceedings of the first instructional conference on machine learning. vol. 242, pp. 29-48. Citeseer (2003)\\n47. Robertson, S., Zaragoza, H., et al.: The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval 3(4), 333–389 (2009)\\n48. Rosset, C., Chung, H.L., Qin, G., Chau, E.C., Feng, Z., Awadallah, A., Neville, J., Rao, N.: Researchy questions: A dataset of multi-perspective, decompositional questions for llm web agents (Feb 2024). https://doi.org/10.48550/ARXIV.2402.17896\\n49. Saad-Falcon, J., Khattab, O., Potts, C., Zaharia, M.: Ares: An automated evaluation framework for retrieval-augmented generation systems (Nov 2023). https://doi.org/10. 48550/ARXIV.2311.09476\\n50. Sai, A.B., Mohankumar, A.K., Khapra, M.M.: A survey of evaluation metrics used for nlg systems. ACM Computing Surveys (CSUR) 55(2), 1–39 (2022)\\n51. Shahabi, C., Kolahdouzan, M.R., Sharifzadeh, M.: A road network embedding technique for k-nearest neighbor search in moving object databases. In: Proceedings of the 10th ACM international symposium on advances in geographic information systems. pp. 94–100 (2002)\\n52. Tang, Y., Yang, Y.: Multihop-rag: Benchmarking retrieval-augmented generation for multi- hop queries (Jan 2024). https://doi.org/10.48550/ARXIV.2401.15391\\n53. Thorne, J., Vlachos, A., Christodoulopoulos, C., Mittal, A.: FEVER: a large-scale dataset for fact extraction and VERification. In: NAACL-HLT (2018)\\n54. TruLens: TruLens (2023), https://www.trulens.org/trulens_eval/ getting_started/quickstarts/quickstart/\\n55. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need (Jun 2017). https://doi.org/10.48550/ ARXIV.1706.03762\\n56. Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: SuperGLUE: A stickier benchmark for general-purpose language understanding systems. arXiv preprint 1905.00537 (2019)\\n57. Wang, S., Khramtsova, E., Zhuang, S., Zuccon, G.: Feb4rag: Evaluating federated search in the context of retrieval augmented generation (Feb 2024). https://doi.org/10. 48550/ARXIV.2402.11891\\n58. Wang, S., Liu, J., Song, S., Cheng, J., Fu, Y., Guo, P., Fang, K., Zhu, Y., Dou, Z.: Domainrag: A chinese benchmark for evaluating domain-specific retrieval-augmented generation (Jun 2024). https://doi.org/10.48550/ARXIV.2406.05654\\n59. Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E.H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., Fedus, W.: Emergent abilities of large language models (Jun 2022). https://doi.org/10. 48550/ARXIV.2206.07682\\n60. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., Zhou, D.: Chain-of-thought prompting elicits reasoning in large language models (Jan 2022). https: //doi.org/10.48550/ARXIV.2201.11903\\n61. Xiong, G., Jin, Q., Lu, Z., Zhang, A.: Benchmarking retrieval-augmented generation for medicine (Feb 2024). https://doi.org/10.48550/ARXIV.2402.13178\\n62. Xu, Z., Li, Y., Ding, R., Wang, X., Chen, B., Jiang, Y., Zheng, H.T., Lu, W., Xie, P., Huang, F.: Let llms take on the latest challenges! a chinese dynamic question answering benchmark (Feb 2024). https://doi.org/10.48550/ARXIV.2402.19248\\n63. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W.W., Salakhutdinov, R., Manning, C.D.: HotpotQA: A dataset for diverse, explainable multi-hop question answering. In: Conference on Empirical Methods in Natural Language Processing (EMNLP) (2018)\\n64. Yao, J.Y., Ning, K.P., Liu, Z.H., Ning, M.N., Yuan, L.: Llm lies: Hallucinations are not bugs, but features as adversarial examples. arXiv preprint arXiv:2310.01469 (2023)\\n65. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.L., Cao, Y., Narasimhan, K.: Tree of Thoughts: Deliberate problem solving with large language models (2023)\")] \n",
      "\n",
      "** Processing chunk: 19-20\n",
      "Processing chunk 19-20 (Uploading ./data/rag_research_paper/chunks_data/2405.07437v2_pages_19_20.pdf)...\n",
      "Finish Reason: STOP\n",
      "** Data extracted.\n",
      "[ResearchPaperData(content_title='References (66-77)', content='66. Yu, X., Cheng, H., Liu, X., Roth, D., Gao, J.: ReEval: Automatic hallucination evaluation for retrieval-augmented large language models via transferable adversarial attacks. In: Duh, K., Gomez, H., Bethard, S. (eds.) Findings of the Association for Computational Linguistics: NAACL 2024. pp. 1333-1351. Association for Computational Linguistics, Mexico City, Mexico (Jun 2024), https://aclanthology.org/2024.findings-naacl.85\\n67. Zhang, K., Liu, Q., Qian, H., Xiang, B., Cui, Q., Zhou, J., Chen, E.: Eatn: An efficient adaptive transfer network for aspect-level sentiment analysis. IEEE Transactions on Knowledge and Data Engineering 35(1), 377–389 (2021)\\n68. Zhang, K., Zhang, H., Liu, Q., Zhao, H., Zhu, H., Chen, E.: Interactive attention transfer network for cross-domain sentiment classification. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 33, pp. 5773–5780 (2019)\\n69. Zhang, K., Zhang, K., Zhang, M., Zhao, H., Liu, Q., Wu, W., Chen, E.: Incorporating dynamic semantics into pre-trained language model for aspect-based sentiment analysis. arXiv preprint arXiv:2203.16369 (2022)\\n70. Zhang, Q., Chen, S., Xu, D., Cao, Q., Chen, X., Cohn, T., Fang, M.: A Survey for Efficient Open Domain Question Answering. In: Rogers, A., Boyd-Graber, J., Okazaki, N. (eds.) Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 14447–14465. Association for Computational Linguistics, Toronto, Canada (Jul 2023). https://doi.org/10.18653/v1/2023.acl-long.808, https://aclanthology.org/2023.acl-long.808\\n71. Zhang, S., Liu, X., Liu, J., Gao, J., Duh, K., Van Durme, B.: Record: Bridging the gap between human and machine commonsense reading comprehension (Oct 2018). https://doi.org/10.48550/ARXIV.1810.12885\\n72. Zhang, T., Kishore, V., Wu, F., Weinberger, K.Q., Artzi, Y.: BERTScore: Evaluating Text Generation with BERT. In: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net (2020), https://openreview.net/forum?id=SkeHuCVFDr\\n73. Zhang, Y., Khalifa, M., Logeswaran, L., Lee, M., Lee, H., Wang, L.: Merging Generated and Retrieved Knowledge for Open-Domain QA. In: Bouamor, H., Pino, J., Bali, K. (eds.) Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp. 4710-4728. Association for Computational Linguistics, Singapore (Dec 2023). https://doi.org/10.18653/v1/2023.emnlp-main.286, https://aclanthology.org/2023.emnlp-main.286\\n74. Zhao, P., Zhang, H., Yu, Q., Wang, Z., Geng, Y., Fu, F., Yang, L., Zhang, W., Cui, B.: Retrieval-augmented generation for ai-generated content: A survey (Feb 2024). https://doi.org/10.48550/ARXIV.2402.19473\\n75. Zheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E.P., Zhang, H., Gonzalez, J.E., Stoica, I.: Judging llm-as-a-judge with mt-bench and chatbot arena (Jun 2023). https://doi.org/10.48550/ARXIV.2306.05685\\n76. Zhou, Y., Lin, X., Zhang, X., Wang, M., Jiang, G., Lu, H., Wu, Y., Zhang, K., Yang, Z., Wang, K., Sui, Y., Jia, F., Tang, Z., Zhao, Y., Zhang, H., Yang, T., Chen, W., Mao, Y., Li, Y., Bao, D., Li, Y., Liao, H., Liu, T., Liu, J., Guo, J., Zhao, X., WEI, Y., Qian, H., Liu, Q., Wang, X., Kin, W., Chan, Li, C., Li, Y., Yang, S., Yan, J., Mou, C., Han, S., Jin, W., Zhang, G., Zeng, X.: On the opportunities of green computing: A survey (Nov 2023)\\n77. Zhu, F., Lei, W., Wang, C., Zheng, J., Poria, S., Chua, T.S.: Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering. Tech. rep. (May 2021), http://arxiv.org/abs/2101.00774, arXiv:2101.00774 [cs] type: article'), ResearchPaperData(content_title='A Structure of RAG System', content=None), ResearchPaperData(content_title='A.1 Retrieval Component', content='The retrieval component of RAG systems in Figure 1 can be categorized into three types: sparse retrieval, dense retrieval [77], and web search engine. The standard for evaluation is the output of relevant documents with numerical scores or rankings. Before the introduction of neural networks, sparse retrievals are widely used for retrieving relative text content. Methods like TF-IDF [46] and BM25 [47] rely on keyword matching and word frequency but may miss semantically relevant documents without keyword overlap. By leveraging deep learning models such as BERT [9], dense retrieval can capture the semantic meaning of texts, which allows them to find relevant documents even when keyword overlap is minimal. This is crucial for complex queries that require a contextual understanding to retrieve accurate information. With advanced fusion structure for queries and documents [28] and the more efficient implementation of K-Nearest Neighbors (KNN) [51], Approximate Nearest Neighbor (ANN) [12,25] search techniques, dense retrieval methods have become practical for large-scale use. Web search engine employs the complex online search engine to provide relevant documents, such as Google Search [18], Bing Search [40], DuckDuckGo [13]. RAG systems can traverse the web\\'s extensive information, potentially returning a more diverse and semantically relevant set of documents via the API of the search provider. The black box of the search engine and the expense of large-scale search are not affordable sometimes. It is observed that dense retrieval techniques, particularly those leveraging embeddings, stand out as the preferred choice within the RAG ecosystem. These methods are frequently employed in tandem with sparse retrieval strategies, creating a hybrid approach that balances precision and breadth in information retrieval. Moreover, the adoption of sophisticated web search engines for benchmark assessment underscores their growing significance in enhancing the robustness and comprehensiveness of evaluations. Indexing The indexing component processes and indexes document collections, such as HuggingFace datasets or Wikipedia pages. Chunking before indexing can improve retrieval by limiting similarity scores to individual chunks, as semantic embedding is less accurate for long articles, and desired content is often brief [32]. Index creation is designed for fast and efficient search. For example, the inverted index for sparse retrieval and the ANN index for dense retrieval. Sparse Retrieval involves calculating IDF for each term and storing values in a database for quick look-up and scoring when queried. Dense Retrieval encodes documents into dense vectors using a pre-trained language model like BERT. These vectors are then indexed using an Approximate Nearest Neighbor (ANN) search technique, like graph-based Hierarchical Navigable Small World (HNSW) or Inverted File Index (IVF) [12]. This process allows for the efficient retrieval of \"closed\" items by given predefined distance metrics.')] \n",
      "\n",
      "** Processing chunk: 21-21\n",
      "Processing chunk 21-21 (Uploading ./data/rag_research_paper/chunks_data/2405.07437v2_pages_21_21.pdf)...\n",
      "Finish Reason: STOP\n",
      "** Data extracted.\n",
      "[ResearchPaperData(content_title='Search', content='This step is responsible for retrieving relevant documents based on a given query. Queries are submitted using the respective API to retrieve relevant documents for web search engine retrieval. For local resources, the query component is responsible for formatting the query in the format required by different sparse or dense retrieval methods. Then, the query is submitted to the retrieval system, which returns a set of relevant documents along with their scores. In both local and web-based scenarios, an optional reranker can be employed to refine the ranking of retrieved documents further. The reranker usually comprises a more complex and larger model that considers additional features of the documents and the given query. These additional features often include the semantic relationship between the query and the document content, document importance or popularity, and other custom measures specific to the information need at hand.'), ResearchPaperData(content_title='A.2 Generation Component', content='The evaluable output for the generation component is the response of LLMs and the structured or formatted output from the phrased response.'), ResearchPaperData(content_title='Prompting', content=\"The generation process critically hinges on prompting, where a query, retrieval outcomes, and instructions converge into a single input for the language model. Research showcases various strategic prompting tactics such as the Chain of Thought (CoT) [60], Tree of Thought (ToT) [3], and Self-Note [31], each significantly shaping the model's output. These methods, especially the step-by-step approach, are pivotal in augmenting LLMs for intricate tasks. Prompting innovations have introduced methods like Rephrase and Respond (RaR) [8], enhancing LLMs by refining queries within prompts for better comprehension and response. This technique has proven to boost performance across diverse tasks. The latest RAG benchmarks [61,62] in the specific domains start to evaluate the robustness of various prompting engineering skills, including CoT, RaR, etc.\"), ResearchPaperData(content_title='Inference', content='The final input string prepared in the prompting step is then passed on to the LLMs as input, which generates the output. The inference stage is where the LLM operates on the input derived from the retrieval and the prompting stages in the pipeline to generate the final output. This is usually the answer to the initial query and is used for downstream tasks. Depending on the specifics of the task or expected output structure, a post-processing step may be implemented here to format the generated output suitably or extract specific information from the response. For example, the classification problems (multi-choice questions) or if the task requires the extraction of specific information from the generated text, this step could involve additional named entity recognition or parsing operations.')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Main workflow\n",
    "all_results = {}\n",
    "pdf_paths = glob.glob(os.path.join(PDF_FOLDER, \"*.pdf\"))\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    pdf_name = os.path.basename(pdf_path)\n",
    "    print(f\"Processing {pdf_name}...\")\n",
    "\n",
    "    chunks = split_pdf_by_pages(pdf_path)\n",
    "    results = []\n",
    "    for chunk in chunks:\n",
    "        # if count == 6:\n",
    "        print(f\"** Processing chunk: {chunk['pages']}\")\n",
    "        data = process_pdf_chunk(chunk, SYSTEM_INSTRUCTION, RESPONSE_FORMAT)\n",
    "        print(\"** Data extracted.\")\n",
    "        print(data, \"\\n\")\n",
    "        # Convert each ResearchPaperData object to a dictionary\n",
    "        for item in data:\n",
    "            results.append(convert_to_dict(item))\n",
    "        # else:\n",
    "        #     pass\n",
    "        # count += 1\n",
    "\n",
    "    all_results[pdf_name] = results\n",
    "\n",
    "    # Remove the break if you want to process all PDFs**\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6d2938a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2405.07437v2.pdf': [{'content_title': 'Abstract',\n",
       "   'content': 'Retrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of RAG systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the current RAG benchmarks, encompassing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of RAG benchmarks.'},\n",
       "  {'content_title': '1 Introduction',\n",
       "   'content': 'Retrieval-Augmented Generation (RAG) [34] efficiently enhances the performance of generative language models through integrating information retrieval techniques. It addresses a critical challenge faced by standalone generative language models: the tendency to produce responses that, while plausible, may not be grounded in facts. By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [23] or factually incorrect outputs, thereby improving the content\\'s reliability and richness. [73] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a development in the pursuit of more intelligent and versatile language models [73,64]. Numerous studies of RAG systems have emerged from various perspectives since the advent of Large Language Models (LLMs) [55,45,59,42,41,69,16]. The RAG system comprises two primary components: Retrieval and Generation. The retrieval component aims to extract relevant information from various external knowledge sources. It involves two main phases, indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval [16,12,28]. The searching component utilizes these indexes to fetch relevant documents on the user\\'s query, often incorporating the optional rerankers [4,39,6,52] to refine the ranking of the retrieved documents. The generation component utilizes the retrieved content and question query to formulate coherent and contextually relevant responses with the prompting and inferencing phases. As the \"Emerging\" ability [59] of LLMs and the breakthrough in aligning human commands [42], LLMs are the best performance choices model for the generation stage. Prompting methods like Chain of Thought (CoT) [60], Tree of Thgouht [65], Rephrase and Respond (RaR) [8] guide better generation results. In the inferencing step, LLMs interpret the prompted input to generate accurate and in-depth responses that align with the query\\'s intent and integrate the extracted information [35,9] without further finetuning, such as fully finetuning [16,1,67,68] or LoRA [21]. Appendix A details the complete RAG structure. Figure 1 illustrates the structure of the RAG systems as mentioned. Fig. 1: The structure of the RAG system with retrieval and generation components and corresponding four phrases: indexing, search, prompting and inferencing. The pairs of \"Evaluable Outputs\" (EOs) and “Ground Truths” (GTs) are highlighted in read frame and green frame, with brown dashed arrows. The importance of evaluating RAG is increasing in parallel with the advancement of RAG-specific methodologies. On the one hand, RAG is a complex system intricately tied to specific requirements and language models, resulting in various evaluation methods, indicators, and tools, particularly given the black-box LLM generation. Evaluating RAG systems involves specific components and the complexity of the overall system assessment. On the other hand, the complexity of RAG systems is further compounded'},\n",
       "  {'content_title': 'Introduction/Contributions',\n",
       "   'content': 'by the external dynamic database and the various downstream tasks, such as content creation or open domain question answering [16,70]. These challenges necessitate the development of comprehensive evaluation metrics that can effectively capture the interplay between retrieval accuracy and generative quality [2,7]. To clarify the elements further, we try to address the current gaps in the area, which differs from the prior RAG surveys [74,16,24] that predominantly collected specific RAG methods or data. We have compiled 12 distinct evaluation frameworks, encompassing a range of aspects of the RAG system. Following the procedure of making benchmarks, we analyze through targets, datasets and metrics mentioned in these benchmarks and summarize them into A Unified Evaluation Process of RAG (Auepora) as three corresponding phases. For this paper, we contribute in the following aspects: 1. Challenge of Evaluation: This is the first work that summarizes and classifies the challenges in evaluating RAG systems through the structure of RAG systems, including three parts retrieval, generation, and the whole system. 2. Analysis Framework: In light of the challenges posed by RAG systems, we introduce an analytical framework, referred to as A Unified Evaluation Process of RAG (Auepora), which aims to elucidate the unique complexities inherent to RAG systems and guide for readers to comprehend the effectiveness of RAG benchmarks across various dimensions 3. RAG Benchmark Analysis: With the help of Auepora, we comprehensively analyze existing RAG benchmarks, highlighting their strengths and limitations and proposing recommendations for future developments in RAG system evaluation.'},\n",
       "  {'content_title': '2 Challenges in Evaluating RAG Systems',\n",
       "   'content': 'Evaluating hybrid RAG systems entails evaluating retrieval, generation and the RAG system as a whole. These evaluations are multifaceted, requiring careful consideration and analysis. Each of them encompasses specific difficulties that complicate the development of a comprehensive evaluation framework and benchmarks for RAG systems.'},\n",
       "  {'content_title': 'Retrieval',\n",
       "   'content': 'The retrieval component is critical for fetching relevant information that informs the generation process. One primary challenge is the dynamic and vast nature of potential knowledge bases, ranging from structured databases to the entire web. This vastness requires evaluation metrics that can effectively measure the precision, recall, and relevance of retrieved documents in the context of a given query [52,32]. Moreover, the temporal aspect of information, where the relevance and accuracy of data can change over time, adds another layer of complexity to the evaluation process [6]. Additionally, the diversity of information sources and the possibility of retrieving misleading or low-quality information pose significant challenges in assessing the effectiveness of filtering and selecting the most pertinent information [39]. The traditional evaluation indicators for retrieval, such as Recall and Precision, cannot fully capture the nuances of RAG retrieval systems, necessitating the development of more nuanced and task-specific evaluation metrics [49].'},\n",
       "  {'content_title': 'Generation',\n",
       "   'content': 'The generation component, powered by LLMs, produces coherent and contextually appropriate responses based on the retrieved content. The challenge here lies in evaluating the faithfulness and accuracy of the generated content to the input data. This involves not only assessing the factual correctness of responses but also their relevance to the original query and the coherence of the generated text [75,49]. The subjective nature of certain tasks, such as creative content generation or open-ended question answering, further complicates the evaluation, as it introduces variability in what constitutes a “correct” or “high-quality\" response [48].'},\n",
       "  {'content_title': 'RAG System as a Whole',\n",
       "   'content': \"Evaluating the whole RAG system introduces additional complexities. The interplay between the retrieval and generation components means that the entire system's performance cannot be fully understood by evaluating each component in isolation [49,14]. The system needs to be assessed on its ability to leverage retrieved information effectively to improve response quality, which involves measuring the added value of the retrieval component to the generative process. Furthermore, practical considerations such as response latency and the ability to handle ambiguous or complex queries are also crucial for evaluating the system's overall effectiveness and usability [39,6].\"},\n",
       "  {'content_title': 'Conclusion (Challenges in Evaluating RAG Systems)',\n",
       "   'content': 'Evaluating the target shift from traditional absolute numeric metrics to multi-source and multi-target generation evaluation, along with the intricate interplay between retrieval and generation components, poses significant challenges. [5,50] Searches in a dynamic database may lead to misleading results or contradict the facts. Diverse and comprehensive datasets that accurately reflect real-world scenarios are crucial. Challenges also arise in the realm of metrics, encompassing generative evaluation criteria for distinct downstream tasks, human preferences, and practical considerations within the RAG system. Most prior benchmarks predominantly tackle one or several aspects of the RAG assessment but lack a comprehensive, holistic analysis.'},\n",
       "  {'content_title': '3 A Unified Evaluation Process of RAG (Auepora)',\n",
       "   'content': 'To facilitate a deeper understanding of RAG benchmarks, we introduce A Unified Evaluation Process of RAG (Auepora), which focuses on three key questions of benchmarks: What to Evaluate? How to Evaluate? How to Measure? which correlated to Target, Dataset, and Metric respectively. We aim to provide a clear and accessible way for readers to comprehend the complexities and nuances of RAG benchmarking. The Target module is intended to determine the evaluation direction. The Dataset module facilitates the comparison of various data constructions in RAG benchmarks. The final module, Metrics, introduces the metrics that correspond to specific targets and datasets used during evaluation. Overall, it is designed to provide a systematic methodology for assessing the effectiveness of RAG systems across various aspects by covering all possible pairs at the beginning between the \"Evaluable Outputs” (EOs) and \"Ground Truths\" (GTs). In the following section, we will explain thoroughly Aueporaand utilize it for introducing and comparing the RAG benchmarks.'},\n",
       "  {'content_title': '3.1 Evaluation Target (What to Evaluate?)',\n",
       "   'content': 'The combination of EOs and GTs in the RAG system can generate all possible targets, which is the fundamental concept of the Auepora (as shown in Figure 1). Once identified, these targets can be defined based on a specific pair of EOs or EO with GT, as illustrated in Figure 2, and used to analyze all aspects of current RAG benchmarks.Figure 2: The Target modular of the Auepora. The diagram illustrates the flow from Query to Retrieval (Result, Relevant Docs) and Generation (Response, Output), interacting with Ground Truth (Docs Candidates, Sample Response, Label). The resulting evaluation targets are Relevance, Accuracy (for Retrieval), Relevance, Faithfulness, Correctness (for Generation), and Additional Requirements (Latency, Noise Robustness, Negative Rejection, Diversity, etc.).'},\n",
       "  {'content_title': 'Retrieval',\n",
       "   'content': 'The EOs are the relevant documents for evaluating the retrieval component depending on the query. Then we can construct two pairwise relationships for the retrieval component, which are Relevant Documents \\t Query, Relevant Documents \\t Documents Candidates.'},\n",
       "  {'content_title': 'Relevance',\n",
       "   'content': '(Relevant Documents \\t Query) evaluates how well the retrieved documents match the information needed expressed in the query. It measures the precision and specificity of the retrieval process.'},\n",
       "  {'content_title': 'Accuracy',\n",
       "   'content': \"(Relevant Documents \\t Documents Candidates) assesses how accurate the retrieved documents are in comparison to a set of candidate documents. It is a measure of the system's ability to identify and score relevant documents higher than less relevant or irrelevant ones.\"},\n",
       "  {'content_title': 'Generation',\n",
       "   'content': 'The similar pairwise relations for the generation components are listed below. The EOs are the generated text and phrased structured content. Then we need to compare these EOs with the provided GTs and labels.'},\n",
       "  {'content_title': 'Relevance',\n",
       "   'content': '(Response \\t Query) measures how well the generated response aligns with the intent and content of the initial query. It ensures that the response is related to the query topic and meets the query’s specific requirements.'},\n",
       "  {'content_title': 'Faithfulness',\n",
       "   'content': '(Response \\t Relevant Documents) evaluates if the generated response accurately reflects the information contained within the relevant documents and measures the consistency between generated content and the source documents.'},\n",
       "  {'content_title': 'Correctness',\n",
       "   'content': '(Response \\t Sample Response) Similar to the accuracy in the retrieval component, this measures the accuracy of the generated response against a sample response, which serves as a ground truth. It checks if the response is correct in terms of factual information and appropriate in the context of the query.'},\n",
       "  {'content_title': 'Table 1: The evaluating targets and corresponding metrics across various frameworks for evaluating RAG systems.',\n",
       "   'content': 'The targets of Retrieval and Generation components are introduced. Table 1 lists the relative work on improving and evaluating RAG and its benchmarks cut off in June.Table 1: The evaluating targets and corresponding metrics across various frameworks for evaluating RAG systems. The presentation distinguishes between the core areas of Retrieval and Generation considered in the evaluation. The different aspects of the evaluation are set as different colours in the table: Relevance, Accuracy of Retrieval and Faithfulness, Correctness and Relevance of Generation. The consideration of the Additional Requirements beyond the retrieval and generation component is also collected. Noted that quite a few of the works employed multiple methods or evaluated multiple aspects simultaneously.||Category|Framework|Time|Raw Targets|Retrieval|Generation||Tool|TruEra RAG Triad [54]|2023.10|Context Relevance, Answer Relevance, Groundedness|LLM as a Judge|LLM as a Judge||Tool|LangChain Bench. [32]|2023.11|Accuracy, Faithfulness, Execution Time, Embed. CosDistance|Accuracy|LLM as a Judge||Tool|Databricks Eval [33]|2023.12|Correctness, Readability, Comprehensiveness|-|LLM as a Judge||Benchmark|RAGAS [14]|2023.09|Context Relevance, Answer Relevance, Faithfulness|LLM as a Judge|LLM Gen + CosSim, LLM as a Judge||Benchmark|RECALL [38]|2023.11|Response Quality, Robustness|-|BLEU, ROUGE-L||Benchmark|ARES [49]|2023.11|Context Relevance, Answer Faithfulness, Answer Relevance|LLM + Classifier|LLM + Classifier, LLM + Classifier||Benchmark|RGB [6]|2023.12|Information Integration, Noise Robustness, Negative Rejection, Counterfactual Robustness|-|Accuracy||Benchmark|MultiHop-RAG [52]|2024.01|Retrieval Quality, Response Correctness|MAP, MRR, Hit@K|LLM as a Judge||Benchmark|CRUD-RAG [39]|2024.02|CREATE, READ, UPDATE, DELETE|-|ROUGE, BLEU, RAGQuestEval||Benchmark|MedRAG [61]|2024.02|Accuracy|-|Accuracy||Benchmark|FeB4RAG [57]|2024.02|Consistency, Correctness, Clarity, Coverage|-|Human Evaluation, Human Evaluation||Benchmark|CDQA [62]|2024.03|Accuracy|-|F1||Benchmark|DomainRAG [58]|2024.06|Correctness, Faithfulness, Noise Robustness, Structural Output|-|F1, Exact-Match, Rouge-L, LLM as a Judge||Benchmark|ReEval [66]|2024.06|Hallucination|-|F1, Exacct-Match, LLM as a Judge, Human Evaluation||Research|FiD-Light [20]|2023.07|Latency|-|-||Research|Diversity Reranker [4]|2023.08|Diversity|Cosine Distance|-||'},\n",
       "  {'content_title': 'Evaluation of Retrieval-Augmented Generation: A Survey',\n",
       "   'content': '2024. Table 1 portrays this information, where each evaluation criterion is represented by a different colour. For example, FeB4RAG [57], the fourth from the last, has posited four standards based on [17] that comprise Consistency, Correctness, Clarity, and Coverage. Correctness is equivalent to accuracy in retrieval, and Consistency is tantamount to faithfulness in the generation component. While accuracy in retrieval gauges the correctness of the retrieved information, we posit that Coverage pertains to the coverage rate and is more associated with diversity. Therefore, we consider Coverage to be linked with diversity and an additional requirement in our proposed evaluation framework, which will be introduced subsequently. The remaining standard, Clarity, is also classified as an additional requirement in our proposed framework. The other tools and benchmarks are processed similarly. Tools and benchmarks offer varying degrees of flexibility in evaluating datasets for RAG systems. Tools, which specify only evaluation targets, provide a versatile framework capable of constructing complete RAG applications and evaluation pipelines, as seen in works like [54,32,33]. Benchmarks, on the other hand, focus on different aspects of RAG evaluation with specific emphasis on either retrieval outputs or generation targets. For instance, RAGAS [14] and ARES [49] assess the relevance of retrieval documents, while RGB and MultiHop-RAG [6,52] prioritize accuracy, necessitating comparison with GTs. The [66] focuses on the Hallucination, which is a combination of faithfulness and correctness. All benchmarks consider generation targets due to their critical role in RAG systems, though their focus areas vary.'},\n",
       "  {'content_title': 'Additional Requirement',\n",
       "   'content': \"In addition to evaluating the two primary components outlined, a portion of the works also addressed some additional requirements of RAG (Black and Italics targets in Table 2). The requirements are as follows: Latency [20,32] measures how quickly the system can find information and respond, crucial for user experience. Diversity [4,32] checks if the system retrieves a variety of relevant documents and generates diverse responses. Noise Robustness [6] assesses how well the system handles irrelevant information without affecting response quality. Negative Rejection [6] gauges the system's ability to refrain from providing a response when the available information is insufficient. Counterfactual Robustness [6] evaluates the system's capacity to identify and disregard incorrect information, even when alerted about potential misinformation. More: For more human preferences considerations, there can be more additional requirements, such as readability [57,33], toxicity, perplexity [33], etc. For the exception, CRUD-RAG [39] introduces a comprehensive benchmark addressing the broader spectrum of RAG applications beyond question-answering, categorized into Create, Read, Update, and Delete scenarios. This benchmark evaluates RAG systems across diverse tasks, including text continuation, question answering, hallucination modification, and multi-document summarization. It offers insights for optimizing RAG technology across different scenarios. DomainRAG [58] identifies six complex abilities for RAG systems: conversational, structural information, faithfulness, denoising, time-sensitive problem solving, and multi-doc understanding. ReEval [66] specifically targets hallucination evaluation by employing a cost-effective LLM-based framework that utilizes prompt chaining to create dynamic test cases.\"},\n",
       "  {'content_title': 'Table 2: The evaluation datasets used for each benchmark.',\n",
       "   'content': 'The evaluation datasets used for each benchmark. The dataset without citation was constructed by the benchmark itself. Benchmark: RAGAS [14], Dataset: WikiEval. Benchmark: RECALL [38], Dataset: EventKG [19], UJ [22]. Benchmark: ARES [49], Dataset: NQ [29], Hotpot [63], FEVER [53], WoW [11], MultiRC [10], ReCoRD [71]. Benchmark: RGB [6], Dataset: Generated (Source: News). Benchmark: MultiHop-RAG [52], Dataset: Generated (Source: News). Benchmark: CRUD-RAG [39], Dataset: Generated (Source: News). Benchmark: MedRAG [61], Dataset: UHGEval [36]. Benchmark: FeB4RAG [57], Dataset: MIRAGE. Benchmark: CDQA [62], Dataset: FeB4RAG, BEIR [26]. Benchmark: DomainRAG [58], Dataset: Generation (Source: News), Labeller. Benchmark: ReEval [66], Dataset: Generation (Source: College Admission Information), RealTimeQA [27], NQ [15,29]).'},\n",
       "  {'content_title': '3.2 Evaluation Dataset (How to evaluate?)',\n",
       "   'content': \"In Table 2, distinct benchmarks employ varying strategies for dataset construction, ranging from leveraging existing resources to generating entirely new data tailored for specific evaluation aspects. Several benchmarks draw upon the part of KILT (Knowledge Intensive Language Tasks) benchmark [44] (Natural Questions [29], HotpotQA [63], and FEVER [53]) and other established datasets such as SuperGLUE [56] (MultiRC [10], and ReCoRD [71]) [49]. However, the drawback of using such datasets can't solve the challenges in dynamic real-world scenarios. A similar situation can be observed in WikiEval, from Wikipedia pages post 2022, constructed by RAGAs [14]. The advent of powerful LLMs has revolutionized the process of dataset construction. With the ability to design queries and ground truths for specific evaluation targets using these frameworks, authors can now create datasets in the desired format with ease. Benchmarks like RGB, MultiHop-RAG, CRUD-RAG, and CDQA [6,52,39,62] have taken this approach further by building their own datasets using online news articles to test RAG systems' ability to handle real-world information beyond the training data of LM frameworks. Most recently, DomainRAG [58] combines various types of QA datasets with single-doc, multi-doc, single-round, and multi-round. These datasets are generated from the yearly changed information from the college website for admission and enrollment, which forces the LLMs to use the provided and updated information.\"},\n",
       "  {'content_title': 'Summary of Dataset Selection (Continuation)',\n",
       "   'content': 'In summary, the creation and selection of datasets are crucial for evaluating RAG systems. Datasets tailored for specific metrics or tasks improve evaluation accuracy and guide the development of adaptable RAG systems for real-world information needs.'},\n",
       "  {'content_title': '3.3 Evaluation Metric (How to quantify?)',\n",
       "   'content': 'Navigating the intricate terrain of evaluating RAG systems necessitates a nuanced understanding of the metrics that can precisely quantify the evaluation targets. However, creating evaluative criteria that align with human preferences and address practical considerations is challenging. Each component within the RAG systems requires a tailored evaluative approach that reflects its distinct functionalities and objectives.'},\n",
       "  {'content_title': 'Retrieval Metrics',\n",
       "   'content': \"Various targets can be evaluated with various metrics that correspond to the given datasets. This section will introduce several commonly used metrics for retrieval and generation targets. The metrics for additional requirements can also be found in these commonly used metrics. The more specifically designed metrics can be explored in the original paper via Table 1 as a reference.For the retrieval evaluation, the focus is on metrics that can accurately capture the relevance, accuracy, diversity, and robustness of the information retrieved in response to queries. These metrics must not only reflect the system's precision in fetching pertinent information but also its resilience in navigating the dynamic, vast, and sometimes misleading landscape of available data. The deployment of metrics like Misleading Rate, Mistake Reappearance Rate, and Error Detection Rate within the [38] benchmark underscores a heightened awareness of RAG systems' inherent intricacies. The integration of MAP@K, MRR@K, and Tokenization with F1 into benchmarks like [52,62] mirrors a deepening comprehension of traditional retrieval's multifaceted evaluation. While the [17] also emphasizes that this ranking-based evaluation methodology is not unsuitable for the RAG system, and should have more RAG-specific retrieval evaluation metrics. These metrics not only capture the precision and recall of retrieval systems but also account for the diversity and relevance of retrieved documents, aligning with the complex and dynamic nature of information needs in RAG systems. The introduction of LLMs as evaluative judges, as seen in [14], further underscores the adaptability and versatility of retrieval evaluation, offering a comprehensive and context-aware approach to assessing retrieval quality.\"},\n",
       "  {'content_title': 'Non-Rank Based Metrics',\n",
       "   'content': 'often assess binary outcomes—whether an item is relevant or not without considering the position of the item in a ranked list. Notice, that the following formula is just one format of these metrics, the definition of each metric may vary by the different evaluating tasks.Accuracy is the proportion of true results (both true positives and true negatives) among the total number of cases examined.Precision is the fraction of relevant instances among the retrieved instances, Precision = TP / (TP + FP) where TP represents true positives and FP represents false positives.Recall at k (Recall@k) is the fraction of relevant instances that have been retrieved over the total amount of relevant cases, considering only the top k results. Recall@k = |RD ∩ Topkd| / |RD| where RD is the relevant documents, and Topkd is the top-k retrieved documents.'},\n",
       "  {'content_title': 'Rank-Based Metrics',\n",
       "   'content': 'evaluate the order in which relevant items are presented, with higher importance placed on the positioning of relevant items at the ranking list.Mean Reciprocal Rank (MRR) is the average of the reciprocal ranks of the first correct answer for a set of queries. MRR = (1/|Q|) * Σ_{i=1}^{|Q|} (1/rank_i) where |Q| is the number of queries and rank_i is the rank position of the first relevant document for the i-th query.Mean Average Precision (MAP) is the mean of the average precision scores for each query. MAP = (1/|Q|) * Σ_{q=1}^{|Q|} (Σ_{k=1}^{n} (P(k) × rel(k))) / |relevant documents_q| where P(k) is the precision at cutoff k in the list, rel(k) is an indicator function equaling 1 if the item at rank k is a relevant document, 0 otherwise, and n is the number of retrieved documents.'},\n",
       "  {'content_title': 'Generation Metrics',\n",
       "   'content': \"In the realm of generation, evaluation transcends the mere accuracy of generated responses, venturing into the quality of text in terms of coherence, relevance, fluency, and alignment with human judgment. This necessitates metrics that can assess the nuanced aspects of language production, including factual correctness, readability, and user satisfaction with the generated content. The traditional metrics like BLEU, ROUGE, and F1 Score continue to play a crucial role, emphasizing the significance of precision and recall in determining response quality. Yet, the advent of metrics such as Misleading Rate, Mistake Reappearance Rate, and Error Detection Rate highlights an evolving understanding of RAG systems' distinct challenges [38].The evaluation done by humans is still a very significant standard to compare the performance of generation models with one another or with the ground truth. The approach of employing LLMs as evaluative judges [75] is a versatile and automatic method for quality assessment, catering to instances where traditional ground truths may be elusive [14]. This methodology benefits from employing prediction-powered inference (PPI) and context relevance scoring, offering a nuanced lens through which LLM output can be assessed. [49] The strategic use of detailed prompt templates ensures a guided assessment aligned with human preferences, effectively standardizing evaluations across various content dimensions [1]. This shift towards leveraging LLMs\"},\n",
       "  {'content_title': 'ROUGE',\n",
       "   'content': 'as arbiters mark a significant progression towards automated and context-responsive evaluation frameworks, enriching the evaluation landscape with minimal reliance on reference comparisons. ROUGE Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [37] is a set of metrics designed to evaluate the quality of summaries by comparing them to human-generated reference summaries. ROUGE can be indicative of the content overlap between the generated text and the reference text. The variants of ROUGES measure the overlap of n-grams (ROUGE-N, ROUGGE-W), word subsequences (ROUGE-L, ROUGGE-S), and word pairs between the system-generated summary and the reference summaries.'},\n",
       "  {'content_title': 'BLEU',\n",
       "   'content': 'Bilingual Evaluation Understudy (BLEU) [43] is a metric for evaluating the quality of machine-translated text against one or more reference translations. BLEU calculates the precision of n-grams in the generated text compared to the reference text and then applies a brevity penalty to discourage overly short translations. BLEU has limitations, such as not accounting for the fluency or grammaticality of the generated text.'},\n",
       "  {'content_title': 'BertScore',\n",
       "   'content': 'BertScore [72] leverages the contextual embedding from pre-trained transformers like BERT to evaluate the semantic similarity between generated text and reference text. BertScore computes token-level similarity using contextual embedding and produces precision, recall, and F1 scores. Unlike n-gram-based metrics, BertScore captures the meaning of words in context, making it more robust to paraphrasing and more sensitive to semantic equivalence.'},\n",
       "  {'content_title': 'LLM as a Judge',\n",
       "   'content': 'Using \"LLM as a Judge\" for evaluating generated text is a more recent approach. [75] In this method, LLMs are used to score the generated text based on criteria such as coherence, relevance, and fluency. The LLM can be optionally finetuned on human judgments to predict the quality of unseen text or used to generate evaluations in a zero-shot or few-shot setting. This approach leverages the LLM\\'s understanding of language and context to provide a more nuanced text quality assessment. For instance, [1] illustrates how providing LLM judges with detailed scoring guidelines, such as a scale from 1 to 5, can standardize the evaluation process. This methodology encompasses critical aspects of content assessment, including coherence, relevance, fluency, coverage, diversity, and detail - both in the context of answer evaluation and query formulation.'},\n",
       "  {'content_title': 'Additional Requirements',\n",
       "   'content': 'These additional requirements, such as latency, diversity, noise robustness, negative rejection, and counterfactual robustness, are used to ensure the practical applicability of RAG systems in real-world scenarios aligned with human preference. This section delves into the metrics used for evaluating these additional requirements, highlighting their significance in the comprehensive assessment of RAG systems.'},\n",
       "  {'content_title': 'Latency',\n",
       "   'content': 'measures the time taken by the RAG system to finish the response of one query. It is a critical factor for user experience, especially in interactive applications such as chatbots or search engines [20]. Single Query Latency: The mean time is taken to process a single query, including both retrieval and generating phases.'},\n",
       "  {'content_title': 'Diversity',\n",
       "   'content': 'evaluates the variety and breadth of information retrieved and generated by the RAG system. It ensures that the system can provide a wide range of perspectives and avoid redundancy in responses [4]. Cosine Similarity / Cosine Distance: The cosine similarity/distance calculates embeddings of retrieved documents or generated responses. [30] Lower cosine similarity scores indicate higher diversity, suggesting that the system can retrieve or generate a broader spectrum of information.'},\n",
       "  {'content_title': 'Noise Robustness',\n",
       "   'content': \"measures the RAG system's ability to handle irrelevant or misleading information without compromising the quality of the response [38]. The metrics Misleading Rate and Mistake Reappearance Rate are described in [38], providing detailed descriptions tailored to the specific dataset and experimental setup. [58]\"},\n",
       "  {'content_title': 'Negative Rejection',\n",
       "   'content': \"evaluates the system's capability to withhold responses when the available information is insufficient or too ambiguous to provide an accurate answer [6]. Rejection Rate: The rate at which the system refrains from generating a response.\"},\n",
       "  {'content_title': 'Counterfactual Robustness',\n",
       "   'content': \"Counterfactual robustness assesses the system's ability to identify and disregard incorrect or counterfactual information within the retrieved documents [39]. Error Detection Rate: The ratio of counterfactual statements detected in retrieved information.\"},\n",
       "  {'content_title': '4 Discussion',\n",
       "   'content': \"For RAG systems, traditional Question Answering (QA) datasets and metrics remain a common format for interaction. [14,49,38,6,61,62,58,66] While these provide a basic verification of RAG's capabilities, it becomes challenging to distinguish the impact of retrieval components when faced with strong Language Models (LLMs) capable of excelling in QA benchmarks. To comprehensively evaluate the performance of entire RAG systems, there is a need for diverse and RAG-specific benchmarks. Several papers offer guidance on improving QA format benchmarks, including variations in question types: from simple Wikipedia filling questions to multi-hop [52], multi-document questions [66] and single-round to multi-round dialogue [39,58]. For answers, aspects such as structural output [58], content moderation [6,54], and hallucination [66] can be considered when evaluating relevance, faithfulness, and correctness. In addition to these, RAG systems require additional requirements such as robustness to noisy documents, language expression, latency, and result diversity. [32,33,38,6,39,57,58,20,4] Furthermore, research is needed on performance changes involving intermediate outputs and retrieved documents, as well as the relationship and analysis between retrieval metrics and final generation outputs. Regarding datasets, creating a universal dataset was challenging due to the target-specific nature of different RAG benchmarks. Tailored datasets [14,38,49,39,57] are necessary for a thorough evaluation, but this approach increases the effort and resources required. Moreover, the diversity of datasets, from news articles to structured databases [66], reflects the adaptability required of RAG systems but also poses a barrier to streamlined evaluation. Recently, with the cutting-edge performance of LLMs, complex data processing and automatic QA pair generation can be automated to achieve daily or finer-grained time resolution, preventing LLMs from cheating and evaluating the robustness of RAG systems in rapidly changing data. [6,52,39,62,58,66]\"},\n",
       "  {'content_title': 'Evaluation Metrics and Challenges',\n",
       "   'content': 'When it comes to metrics, the use of LLMs as automatic evaluative judges signifies a burgeoning trend, promising versatility and depth in generative outputs with reasoning on a large scale compared to human evaluation. However, using \"LLMs as a Judge\" [75] for responses presents challenges in aligning with human judgment, establishing effective grading scales, and applying consistent evaluation across varied use cases. Determining correctness, clarity, and richness can differ between automated and human assessments. Moreover, the effectiveness of example-based scoring can vary, and there\\'s no universally applicable grading scale and prompting text, complicating the standardization of “LLM as a Judge\". [33] In addition to the challenges mentioned above, it is important to consider the resource-intensive nature [76] of using Large Language Models (LLMs) for data generation and validation. RAG benchmarks must balance the need for thorough evaluation with the practical constraints of limited computational resources. As such, it is desirable to develop evaluation methodologies that can effectively assess RAG systems using smaller amounts of data while maintaining the validity and reliability of the results.'},\n",
       "  {'content_title': '5 Conclusion',\n",
       "   'content': 'This survey systematically explores the complexities of evaluating RAG systems, highlighting the challenges in assessing their performance. Through the proposed A Unified Evaluation Process of RAG, we outline a structured approach to analyzing RAG evaluations, focusing on targets, datasets and measures. Our analysis emphasizes the need for targeted benchmarks that reflect the dynamic interplay between retrieval accuracy and generative quality and practical considerations for real-world applications. By identifying gaps in current methodologies and suggesting future research directions, we aim to contribute to more effective, and user-aligned benchmarks of RAG systems.'},\n",
       "  {'content_title': 'References',\n",
       "   'content': \"1. Balaguer, A., Benara, V., Cunha, R.L.d.F., Filho, R.d.M.E., Hendry, T., Holstein, D., Marsman, J., Mecklenburg, N., Malvar, S., Nunes, L.O., Padilha, R., Sharp, M., Silva, B., Sharma, S., Aski, V., Chandra, R.: RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. Tech. rep. (Jan 2024), http://arxiv.org/abs/2401.08406, arXiv:2401.08406 [cs] type: article 2. Barnett, S., Kurniawan, S., Thudumu, S., Brannelly, Z., Abdelrazek, M.: Seven failure points when engineering a retrieval augmented generation system (Jan 2024). https://doi.org/10.48550/ARXIV.2401.05856 3. Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Podstawski, M., Gianinazzi, L., Gajda, J., Lehmann, T., Niewiadomski, H., Nyczyk, P., Hoefler, T.: Graph of thoughts: Solving elaborate problems with large language models. Proceedings of the AAAI Conference on Artificial Intelligence 2024 (AAAI'24) (Aug 2023). https://doi.org/10.48550/ARXIV.2308.09687 4. Blagojevic, V.: Enhancing RAG Pipelines in Haystack: Introducing DiversityRanker and LostInTheMiddleRanker (Aug 2023), https://towardsdatascience.com/enhancing-rag-pipelines-in-haystack-45f14e2bc9f5 5. Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., et al.: A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology 15(3), 1–45 (2024) 6. Chen, J., Lin, H., Han, X., Sun, L.: Benchmarking large language models in retrieval-augmented generation (Sep 2023). https://doi.org/10.48550/ARXIV.2309.01431 7. Cuconasu, F., Trappolini, G., Siciliano, F., Filice, S., Campagnano, C., Maarek, Y., Tonellotto, N., Silvestri, F.: The power of noise: Redefining retrieval for rag systems (Jan 2024). https://doi.org/10.48550/ARXIV.2401.14887 8. Deng, Y., Zhang, W., Chen, Z., Gu, Q.: Rephrase and respond: Let large language models ask better questions for themselves (Nov 2023). https://doi.org/10.48550/ARXIV.2311.04205 9. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In: Burstein, J., Doran, C., Solorio, T. (eds.) Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 4171-4186. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019). https://doi.org/10.18653/v1/N19-1423, https://aclanthology.org/N19-1423 10. De Young, J., Jain, S., Rajani, N.F., Lehman, E., Xiong, C., Socher, R., Wallace, B.C.: Eraser: A benchmark to evaluate rationalized nlp models 11. Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., Weston, J.: Wizard of Wikipedia: Knowledge-powered conversational agents. In: Proceedings of the International Conference on Learning Representations (ICLR) (2019) 12. Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazaré, P.E., Lomeli, M., Hosseini, L., Jégou, H.: The faiss library (2024) 13. DuckDuckGo: DuckDuckGo Privacy, simplified. (2024), https://duckduckgo.com//home 14. Es, S., James, J., Espinosa-Anke, L., Schockaert, S.: Ragas: Automated evaluation of retrieval augmented generation (Sep 2023). https://doi.org/10.48550/ARXIV.2309.15217\"},\n",
       "  {'content_title': '15.',\n",
       "   'content': 'Fisch, A., Talmor, A., Jia, R., Seo, M., Choi, E., Chen, D.: MRQA 2019 shared task: Evaluating generalization in reading comprehension. In: Fisch, A., Talmor, A., Jia, R., Seo, M., Choi, E., Chen, D. (eds.) Proceedings of the 2nd Workshop on Machine Reading for Question Answering. pp. 1-13. Association for Computational Linguistics, Hong Kong, China (Nov 2019). https://doi.org/10.18653/v1/D19-5801, https://aclanthology.org/D19-5801'},\n",
       "  {'content_title': '16.',\n",
       "   'content': 'Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Guo, Q., Wang, M., Wang, H.: Retrieval-Augmented Generation for Large Language Models: A Survey. Tech. rep. (Jan 2024), http://arxiv.org/abs/2312.10997, arXiv:2312.10997 [cs] type: article'},\n",
       "  {'content_title': '17.',\n",
       "   'content': 'Gienapp, L., Scells, H., Deckers, N., Bevendorff, J., Wang, S., Kiesel, J., Syed, S., Fröbe, M., Zuccon, G., Stein, B., Hagen, M., Potthast, M.: Evaluating Generative Ad Hoc Information Retrieval. Tech. rep. (Nov 2023), http://arxiv.org/abs/2311.04694, arXiv:2311.04694 [cs] type: article'},\n",
       "  {'content_title': '18.',\n",
       "   'content': 'Google: Programmable Search Engine | Google for Developers (2024), https://developers.google.com/custom-search'},\n",
       "  {'content_title': '19.',\n",
       "   'content': 'Gottschalk, S., Demidova, E.: Eventkg: A multilingual event-centric temporal knowledge graph (Apr 2018). https://doi.org/10.48550/ARXIV.1804.04526'},\n",
       "  {'content_title': '20.',\n",
       "   'content': \"Hofstätter, S., Chen, J., Raman, K., Zamani, H.: FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation. In: Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 1437-1447. SIGIR '23, Association for Computing Machinery, New York, NY, USA (Jul 2023). https://doi.org/10.1145/3539618.3591687, https://doi.org/10.1145/3539618.3591687\"},\n",
       "  {'content_title': '21.',\n",
       "   'content': 'Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: LORA: Low-Rank Adaptation of Large Language Models. Tech. rep. (Oct 2021). https://doi.org/10.48550/arXiv.2106.09685, http://arxiv.org/abs/2106.09685, arXiv:2106.09685 [cs] type: article'},\n",
       "  {'content_title': '22.',\n",
       "   'content': 'Huang, J., Shao, H., Chang, K.C.C., Xiong, J., Hwu, W.m.: Understanding jargon: Combining extraction and generation for definition modeling. In: Proceedings of EMNLP (2022)'},\n",
       "  {'content_title': '23.',\n",
       "   'content': 'Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., Liu, T.: A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions (Nov 2023). https://doi.org/10.48550/ARXIV.2311.05232'},\n",
       "  {'content_title': '24.',\n",
       "   'content': 'Huang, Y., Huang, J.: A survey on retrieval-augmented text generation for large language models (Apr 2024). https://doi.org/10.48550/ARXIV.2404.10981'},\n",
       "  {'content_title': '25.',\n",
       "   'content': 'Johnson, J., Douze, M., Jégou, H.: Billion-scale similarity search with GPUs. IEEE Transactions on Big Data 7(3), 535-547 (2019)'},\n",
       "  {'content_title': '26.',\n",
       "   'content': 'Kamalloo, E., Thakur, N., Lassance, C., Ma, X., Yang, J.H., Lin, J.: Resources for brewing beir: Reproducible reference models and an official leaderboard (2023)'},\n",
       "  {'content_title': '27.',\n",
       "   'content': \"Kasai, J., Sakaguchi, K., Takahashi, Y., Bras, R.L., Asai, A., Yu, X., Radev, D., Smith, N.A., Choi, Y., Inui, K.: Realtime qa: What's the answer right now? (Jul 2022). https://doi.org/10.48550/ARXIV.2207.13332, https://arxiv.org/abs/2207.13332\"},\n",
       "  {'content_title': '28.',\n",
       "   'content': 'Khattab, O., Zaharia, M.: Colbert: Efficient and effective passage search via contextualized late interaction over bert (Apr 2020). https://doi.org/10.48550/ARXIV.2004.12832'},\n",
       "  {'content_title': '29.',\n",
       "   'content': 'Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.W., Dai, A.M., Uszkoreit, J., Le, Q., Petrov, S.: Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics 7, 453-466 (2019). https://doi.org/10.1162/tacl_a_00276, https://doi.org/10.1162/tacl_a_00276'},\n",
       "  {'content_title': '30.',\n",
       "   'content': 'Lahitani, A.R., Permanasari, A.E., Setiawan, N.A.: Cosine similarity to determine similarity measure: Study case in online essay assessment. In: 2016 4th International Conference on Cyber and IT Service Management. pp. 1-6 (2016). https://doi.org/10.1109/CITSM.2016.7577578'},\n",
       "  {'content_title': '31.',\n",
       "   'content': 'Lanchantin, J., Toshniwal, S., Weston, J., Szlam, A., Sukhbaatar, S.: Learning to reason and memorize with self-notes (May 2023). https://doi.org/10.48550/ARXIV.2305.00833'},\n",
       "  {'content_title': '32.',\n",
       "   'content': 'LangChain: Evaluating rag architectures on benchmark tasks (Nov 2023), https://langchain-ai.github.io/langchain-benchmarks/notebooks/retrieval/langchain_docs_qa.html'},\n",
       "  {'content_title': '33.',\n",
       "   'content': 'Leng, Q., Uhlenhuth, K., Polyzotis, A.: Best Practices for LLM Evaluation of RAG Applications (Dec 2023), https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG'},\n",
       "  {'content_title': '34.',\n",
       "   'content': \"Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.t., Rocktäschel, T., Riedel, S., Kiela, D.: Retrieval-augmented generation for knowledge-intensive NLP tasks. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. pp. 9459–9474. NIPS'20, Curran Associates Inc., Red Hook, NY, USA (Dec 2020)\"},\n",
       "  {'content_title': '35.',\n",
       "   'content': 'Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.t., Rocktäschel, T., Riedel, S., Kiela, D.: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Tech. rep. (Apr 2021), http://arxiv.org/abs/2005.11401, arXiv:2005.11401 [cs] type: article'},\n",
       "  {'content_title': '36.',\n",
       "   'content': 'Liang, X., Song, S., Niu, S., Li, Z., Xiong, F., Tang, B., Wy, Z., He, D., Cheng, P., Wang, Z., Deng, H.: Uhgeval: Benchmarking the hallucination of chinese large language models via unconstrained generation. arXiv preprint arXiv:2311.15296 (2023)'},\n",
       "  {'content_title': '37.',\n",
       "   'content': 'Lin, C.Y.: ROUGE: A package for automatic evaluation of summaries. In: Text Summarization Branches Out. pp. 74-81. Association for Computational Linguistics, Barcelona, Spain (Jul 2004), https://aclanthology.org/W04-1013'},\n",
       "  {'content_title': '38.',\n",
       "   'content': 'Liu, Y., Huang, L., Li, S., Chen, S., Zhou, H., Meng, F., Zhou, J., Sun, X.: Recall: A benchmark for llms robustness against external counterfactual knowledge (Nov 2023). https://doi.org/10.48550/ARXIV.2311.08147'},\n",
       "  {'content_title': '39.',\n",
       "   'content': 'Lyu, Y., Li, Z., Niu, S., Xiong, F., Tang, B., Wang, W., Wu, H., Liu, H., Xu, T., Chen, E., Luo, Y., Cheng, P., Deng, H., Wang, Z., Lu, Z.: Crud-rag: A comprehensive chinese benchmark for retrieval-augmented generation of large language models (Jan 2024). https://doi.org/10.48550/ARXIV.2401.17043'},\n",
       "  {'content_title': '40.',\n",
       "   'content': 'Microsoft: Web Search API | Microsoft Bing, https://www.microsoft.com/en-us/bing/apis/bing-web-search-api'},\n",
       "  {'content_title': '41.',\n",
       "   'content': 'OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.L., Brockman, G., Brooks, T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson, C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H.W., Cummings, D., Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S.P., Forte, J., Fulford, I., Gao, L., Georges, E., Gibson, C., Goel, V., Gogineni, T., ...'},\n",
       "  {'content_title': 'References',\n",
       "   'content': \"Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S.S., Guo, Y., Hallacy, C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Kamali, A., Kanitscheider, I., Keskar, N.S., Khan, T., Kilpatrick, L., Kim, J.W., Kim, C., Kim, Y., Kirchner, J.H., Kiros, J., Knight, M., Kokotajlo, D., Kondraciuk, A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D., Li, C.M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju, A., Malfacini, K., Manning, S., Markov, T., Markovski, Y., Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S.M., McLeavey, C., McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V., Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O., Mély, D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang, L., O'Keefe, C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo, G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A., Peres, F.d.A.B., Petrov, M., Pinto, H.P.d.O., Michael, Pokorny, Pokrass, M., Pong, V.H., Powell, T., Power, A., Power, B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D., Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N., Such, F.P., Summers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M.B., Tillet, P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J.F.C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J.J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann, C., Welihinda, A., Welinder, P., Weng, J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., Zoph, B.: GPT-4 Technical Report (Mar 2023). https://doi.org/10.48550/ARXIV.2303.08774\\n42. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., Lowe, R.: Training language models to follow instructions with human feedback. Tech. rep. (Mar 2022). https://doi. org/10.48550/arXiv.2203.02155, http://arxiv.org/abs/2203.02155, arXiv:2203.02155 [cs] type: article\\n43. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of machine translation. In: Isabelle, P., Charniak, E., Lin, D. (eds.) Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311–318. Association for Computational Linguistics, Philadelphia, Pennsylvania, USA (Jul 2002). https://doi. org/10.3115/1073083.1073135, https://aclanthology.org/P02-1040\\n44. Petroni, F., Piktus, A., Fan, A., Lewis, P., Yazdani, M., De Cao, N., Thorne, J., Jernite, Y., Karpukhin, V., Maillard, J., Plachouras, V., Rocktäschel, T., Riedel, S.: KILT: a benchmark for knowledge intensive language tasks. In: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2523-2544. Association for Computational Linguistics, Online (Jun 2021). https://doi.org/10.18653/v1/2021.naacl-main.200, https://aclanthology.org/2021.naacl-main.200\\n45. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019)\\n46. Ramos, J., et al.: Using tf-idf to determine word relevance in document queries. In: Proceedings of the first instructional conference on machine learning. vol. 242, pp. 29-48. Citeseer (2003)\\n47. Robertson, S., Zaragoza, H., et al.: The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval 3(4), 333–389 (2009)\\n48. Rosset, C., Chung, H.L., Qin, G., Chau, E.C., Feng, Z., Awadallah, A., Neville, J., Rao, N.: Researchy questions: A dataset of multi-perspective, decompositional questions for llm web agents (Feb 2024). https://doi.org/10.48550/ARXIV.2402.17896\\n49. Saad-Falcon, J., Khattab, O., Potts, C., Zaharia, M.: Ares: An automated evaluation framework for retrieval-augmented generation systems (Nov 2023). https://doi.org/10. 48550/ARXIV.2311.09476\\n50. Sai, A.B., Mohankumar, A.K., Khapra, M.M.: A survey of evaluation metrics used for nlg systems. ACM Computing Surveys (CSUR) 55(2), 1–39 (2022)\\n51. Shahabi, C., Kolahdouzan, M.R., Sharifzadeh, M.: A road network embedding technique for k-nearest neighbor search in moving object databases. In: Proceedings of the 10th ACM international symposium on advances in geographic information systems. pp. 94–100 (2002)\\n52. Tang, Y., Yang, Y.: Multihop-rag: Benchmarking retrieval-augmented generation for multi- hop queries (Jan 2024). https://doi.org/10.48550/ARXIV.2401.15391\\n53. Thorne, J., Vlachos, A., Christodoulopoulos, C., Mittal, A.: FEVER: a large-scale dataset for fact extraction and VERification. In: NAACL-HLT (2018)\\n54. TruLens: TruLens (2023), https://www.trulens.org/trulens_eval/ getting_started/quickstarts/quickstart/\\n55. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need (Jun 2017). https://doi.org/10.48550/ ARXIV.1706.03762\\n56. Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: SuperGLUE: A stickier benchmark for general-purpose language understanding systems. arXiv preprint 1905.00537 (2019)\\n57. Wang, S., Khramtsova, E., Zhuang, S., Zuccon, G.: Feb4rag: Evaluating federated search in the context of retrieval augmented generation (Feb 2024). https://doi.org/10. 48550/ARXIV.2402.11891\\n58. Wang, S., Liu, J., Song, S., Cheng, J., Fu, Y., Guo, P., Fang, K., Zhu, Y., Dou, Z.: Domainrag: A chinese benchmark for evaluating domain-specific retrieval-augmented generation (Jun 2024). https://doi.org/10.48550/ARXIV.2406.05654\\n59. Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E.H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., Fedus, W.: Emergent abilities of large language models (Jun 2022). https://doi.org/10. 48550/ARXIV.2206.07682\\n60. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., Zhou, D.: Chain-of-thought prompting elicits reasoning in large language models (Jan 2022). https: //doi.org/10.48550/ARXIV.2201.11903\\n61. Xiong, G., Jin, Q., Lu, Z., Zhang, A.: Benchmarking retrieval-augmented generation for medicine (Feb 2024). https://doi.org/10.48550/ARXIV.2402.13178\\n62. Xu, Z., Li, Y., Ding, R., Wang, X., Chen, B., Jiang, Y., Zheng, H.T., Lu, W., Xie, P., Huang, F.: Let llms take on the latest challenges! a chinese dynamic question answering benchmark (Feb 2024). https://doi.org/10.48550/ARXIV.2402.19248\\n63. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W.W., Salakhutdinov, R., Manning, C.D.: HotpotQA: A dataset for diverse, explainable multi-hop question answering. In: Conference on Empirical Methods in Natural Language Processing (EMNLP) (2018)\\n64. Yao, J.Y., Ning, K.P., Liu, Z.H., Ning, M.N., Yuan, L.: Llm lies: Hallucinations are not bugs, but features as adversarial examples. arXiv preprint arXiv:2310.01469 (2023)\\n65. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.L., Cao, Y., Narasimhan, K.: Tree of Thoughts: Deliberate problem solving with large language models (2023)\"},\n",
       "  {'content_title': 'References (66-77)',\n",
       "   'content': '66. Yu, X., Cheng, H., Liu, X., Roth, D., Gao, J.: ReEval: Automatic hallucination evaluation for retrieval-augmented large language models via transferable adversarial attacks. In: Duh, K., Gomez, H., Bethard, S. (eds.) Findings of the Association for Computational Linguistics: NAACL 2024. pp. 1333-1351. Association for Computational Linguistics, Mexico City, Mexico (Jun 2024), https://aclanthology.org/2024.findings-naacl.85\\n67. Zhang, K., Liu, Q., Qian, H., Xiang, B., Cui, Q., Zhou, J., Chen, E.: Eatn: An efficient adaptive transfer network for aspect-level sentiment analysis. IEEE Transactions on Knowledge and Data Engineering 35(1), 377–389 (2021)\\n68. Zhang, K., Zhang, H., Liu, Q., Zhao, H., Zhu, H., Chen, E.: Interactive attention transfer network for cross-domain sentiment classification. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 33, pp. 5773–5780 (2019)\\n69. Zhang, K., Zhang, K., Zhang, M., Zhao, H., Liu, Q., Wu, W., Chen, E.: Incorporating dynamic semantics into pre-trained language model for aspect-based sentiment analysis. arXiv preprint arXiv:2203.16369 (2022)\\n70. Zhang, Q., Chen, S., Xu, D., Cao, Q., Chen, X., Cohn, T., Fang, M.: A Survey for Efficient Open Domain Question Answering. In: Rogers, A., Boyd-Graber, J., Okazaki, N. (eds.) Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 14447–14465. Association for Computational Linguistics, Toronto, Canada (Jul 2023). https://doi.org/10.18653/v1/2023.acl-long.808, https://aclanthology.org/2023.acl-long.808\\n71. Zhang, S., Liu, X., Liu, J., Gao, J., Duh, K., Van Durme, B.: Record: Bridging the gap between human and machine commonsense reading comprehension (Oct 2018). https://doi.org/10.48550/ARXIV.1810.12885\\n72. Zhang, T., Kishore, V., Wu, F., Weinberger, K.Q., Artzi, Y.: BERTScore: Evaluating Text Generation with BERT. In: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net (2020), https://openreview.net/forum?id=SkeHuCVFDr\\n73. Zhang, Y., Khalifa, M., Logeswaran, L., Lee, M., Lee, H., Wang, L.: Merging Generated and Retrieved Knowledge for Open-Domain QA. In: Bouamor, H., Pino, J., Bali, K. (eds.) Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp. 4710-4728. Association for Computational Linguistics, Singapore (Dec 2023). https://doi.org/10.18653/v1/2023.emnlp-main.286, https://aclanthology.org/2023.emnlp-main.286\\n74. Zhao, P., Zhang, H., Yu, Q., Wang, Z., Geng, Y., Fu, F., Yang, L., Zhang, W., Cui, B.: Retrieval-augmented generation for ai-generated content: A survey (Feb 2024). https://doi.org/10.48550/ARXIV.2402.19473\\n75. Zheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E.P., Zhang, H., Gonzalez, J.E., Stoica, I.: Judging llm-as-a-judge with mt-bench and chatbot arena (Jun 2023). https://doi.org/10.48550/ARXIV.2306.05685\\n76. Zhou, Y., Lin, X., Zhang, X., Wang, M., Jiang, G., Lu, H., Wu, Y., Zhang, K., Yang, Z., Wang, K., Sui, Y., Jia, F., Tang, Z., Zhao, Y., Zhang, H., Yang, T., Chen, W., Mao, Y., Li, Y., Bao, D., Li, Y., Liao, H., Liu, T., Liu, J., Guo, J., Zhao, X., WEI, Y., Qian, H., Liu, Q., Wang, X., Kin, W., Chan, Li, C., Li, Y., Yang, S., Yan, J., Mou, C., Han, S., Jin, W., Zhang, G., Zeng, X.: On the opportunities of green computing: A survey (Nov 2023)\\n77. Zhu, F., Lei, W., Wang, C., Zheng, J., Poria, S., Chua, T.S.: Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering. Tech. rep. (May 2021), http://arxiv.org/abs/2101.00774, arXiv:2101.00774 [cs] type: article'},\n",
       "  {'content_title': 'A Structure of RAG System', 'content': None},\n",
       "  {'content_title': 'A.1 Retrieval Component',\n",
       "   'content': 'The retrieval component of RAG systems in Figure 1 can be categorized into three types: sparse retrieval, dense retrieval [77], and web search engine. The standard for evaluation is the output of relevant documents with numerical scores or rankings. Before the introduction of neural networks, sparse retrievals are widely used for retrieving relative text content. Methods like TF-IDF [46] and BM25 [47] rely on keyword matching and word frequency but may miss semantically relevant documents without keyword overlap. By leveraging deep learning models such as BERT [9], dense retrieval can capture the semantic meaning of texts, which allows them to find relevant documents even when keyword overlap is minimal. This is crucial for complex queries that require a contextual understanding to retrieve accurate information. With advanced fusion structure for queries and documents [28] and the more efficient implementation of K-Nearest Neighbors (KNN) [51], Approximate Nearest Neighbor (ANN) [12,25] search techniques, dense retrieval methods have become practical for large-scale use. Web search engine employs the complex online search engine to provide relevant documents, such as Google Search [18], Bing Search [40], DuckDuckGo [13]. RAG systems can traverse the web\\'s extensive information, potentially returning a more diverse and semantically relevant set of documents via the API of the search provider. The black box of the search engine and the expense of large-scale search are not affordable sometimes. It is observed that dense retrieval techniques, particularly those leveraging embeddings, stand out as the preferred choice within the RAG ecosystem. These methods are frequently employed in tandem with sparse retrieval strategies, creating a hybrid approach that balances precision and breadth in information retrieval. Moreover, the adoption of sophisticated web search engines for benchmark assessment underscores their growing significance in enhancing the robustness and comprehensiveness of evaluations. Indexing The indexing component processes and indexes document collections, such as HuggingFace datasets or Wikipedia pages. Chunking before indexing can improve retrieval by limiting similarity scores to individual chunks, as semantic embedding is less accurate for long articles, and desired content is often brief [32]. Index creation is designed for fast and efficient search. For example, the inverted index for sparse retrieval and the ANN index for dense retrieval. Sparse Retrieval involves calculating IDF for each term and storing values in a database for quick look-up and scoring when queried. Dense Retrieval encodes documents into dense vectors using a pre-trained language model like BERT. These vectors are then indexed using an Approximate Nearest Neighbor (ANN) search technique, like graph-based Hierarchical Navigable Small World (HNSW) or Inverted File Index (IVF) [12]. This process allows for the efficient retrieval of \"closed\" items by given predefined distance metrics.'},\n",
       "  {'content_title': 'Search',\n",
       "   'content': 'This step is responsible for retrieving relevant documents based on a given query. Queries are submitted using the respective API to retrieve relevant documents for web search engine retrieval. For local resources, the query component is responsible for formatting the query in the format required by different sparse or dense retrieval methods. Then, the query is submitted to the retrieval system, which returns a set of relevant documents along with their scores. In both local and web-based scenarios, an optional reranker can be employed to refine the ranking of retrieved documents further. The reranker usually comprises a more complex and larger model that considers additional features of the documents and the given query. These additional features often include the semantic relationship between the query and the document content, document importance or popularity, and other custom measures specific to the information need at hand.'},\n",
       "  {'content_title': 'A.2 Generation Component',\n",
       "   'content': 'The evaluable output for the generation component is the response of LLMs and the structured or formatted output from the phrased response.'},\n",
       "  {'content_title': 'Prompting',\n",
       "   'content': \"The generation process critically hinges on prompting, where a query, retrieval outcomes, and instructions converge into a single input for the language model. Research showcases various strategic prompting tactics such as the Chain of Thought (CoT) [60], Tree of Thought (ToT) [3], and Self-Note [31], each significantly shaping the model's output. These methods, especially the step-by-step approach, are pivotal in augmenting LLMs for intricate tasks. Prompting innovations have introduced methods like Rephrase and Respond (RaR) [8], enhancing LLMs by refining queries within prompts for better comprehension and response. This technique has proven to boost performance across diverse tasks. The latest RAG benchmarks [61,62] in the specific domains start to evaluate the robustness of various prompting engineering skills, including CoT, RaR, etc.\"},\n",
       "  {'content_title': 'Inference',\n",
       "   'content': 'The final input string prepared in the prompting step is then passed on to the LLMs as input, which generates the output. The inference stage is where the LLM operates on the input derived from the retrieval and the prompting stages in the pipeline to generate the final output. This is usually the answer to the initial query and is used for downstream tasks. Depending on the specifics of the task or expected output structure, a post-processing step may be implemented here to format the generated output suitably or extract specific information from the response. For example, the classification problems (multi-choice questions) or if the task requires the extraction of specific information from the generated text, this step could involve additional named entity recognition or parsing operations.'}]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1f0be42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved structured data to 2405.07437v2_structured.json\n",
      "\n",
      "Cleaning up temporary chunk files...\n"
     ]
    }
   ],
   "source": [
    "# Save combined results for each PDF\n",
    "for pdf_name, data in all_results.items():\n",
    "    out_file = f\"{pdf_name.rsplit('.',1)[0]}_structured.json\"\n",
    "    with open(out_file, \"w\") as f_json:\n",
    "        json.dump(data, f_json, indent=2)\n",
    "    print(f\"Saved structured data to {out_file}\")\n",
    "\n",
    "print(\"\\nCleaning up temporary chunk files...\")\n",
    "for pdf_path in pdf_paths:\n",
    "    pdf_base = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    chunk_pattern = os.path.join(\"/content\", f\"{pdf_base}_pages_*.pdf\")\n",
    "    for chunk_file in glob.glob(chunk_pattern):\n",
    "        os.remove(chunk_file)\n",
    "        print(f\"Removed: {chunk_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb48a798",
   "metadata": {},
   "source": [
    "## List of available models in the Purchase Tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386fbf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-3-pro-preview\n",
      "models/gemini-3-flash-preview\n",
      "models/gemini-3-pro-image-preview\n",
      "models/nano-banana-pro-preview\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n",
      "models/deep-research-pro-preview-12-2025\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "models/aqa\n",
      "models/imagen-4.0-generate-preview-06-06\n",
      "models/imagen-4.0-ultra-generate-preview-06-06\n",
      "models/imagen-4.0-generate-001\n",
      "models/imagen-4.0-ultra-generate-001\n",
      "models/imagen-4.0-fast-generate-001\n",
      "models/veo-2.0-generate-001\n",
      "models/veo-3.0-generate-001\n",
      "models/veo-3.0-fast-generate-001\n",
      "models/veo-3.1-generate-preview\n",
      "models/veo-3.1-fast-generate-preview\n",
      "models/gemini-2.5-flash-native-audio-latest\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025\n",
      "models/gemini-2.5-flash-native-audio-preview-12-2025\n"
     ]
    }
   ],
   "source": [
    "import google.genai as genai\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "for model in client.models.list():\n",
    "    print(model.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0cf8b7",
   "metadata": {},
   "source": [
    "Why we created `convert_to_dict`, this is the demo code that shows what is will be the use of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310072b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content_title': 'Abstract', 'content': 'This paper introduces a new method called RAG...'}\n",
      "{'content_title': '1. Introduction', 'content': 'Large Language Models have shown...'}\n",
      "{'content_title': '2. Methodology', 'content': 'We used a dual-encoder architecture...'}\n"
     ]
    }
   ],
   "source": [
    "data = [ # THis is what process_pdf_chunk will return\n",
    "    ResearchPaperData(\n",
    "        content_title=\"Abstract\",\n",
    "        content=\"This paper introduces a new method called RAG...\"\n",
    "    ),\n",
    "    ResearchPaperData(\n",
    "        content_title=\"1. Introduction\",\n",
    "        content=\"Large Language Models have shown...\"\n",
    "    ),\n",
    "    ResearchPaperData(\n",
    "        content_title=\"2. Methodology\",\n",
    "        content=\"We used a dual-encoder architecture...\"\n",
    "    )\n",
    "]\n",
    "\n",
    "for item in data:\n",
    "    print(convert_to_dict(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0808e85e",
   "metadata": {},
   "source": [
    "## Testing with 1 PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "623ecb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction_prompt = \"\"\"\n",
    "You are a data extraction bot. \n",
    "Output ONLY valid JSON. \n",
    "CRITICAL: Do not use tabs (\\\\t) or multiple newlines (\\\\n). \n",
    "Keep the JSON compact. If you find headers or footers in the PDF, ignore them. \n",
    "Do not repeat the paper title in the output.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b49ec04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma response: {\n",
      "  \"research_paper_data\": [\n",
      "    {\n",
      "      \"content\": \"The combination of EOs and GTs in the RAG system can generate all possible targets, which is the fundamental concept of the Auepora (as shown in Figure 1). Once identified, these targets can be defined based on a specific pair of EOs or EO with GT, as illustrated in Figure 2, and used to analyze all aspects of current RAG benchmarks.Fig. 2: The Target modular of the Auepora. The figure illustrates the flow and relationships between Query, Result, Ground Truth, Retrieval, and Generation components, mapping them to specific evaluation targets (Relevance, Accuracy, Faithfulness, Correctness) and Additional Requirements (Latency, Noise Robustness, Negative Rejection, Diversity, etc.). Retrieval uses Query, Relevant Docs, and Docs Candidates to determine Relevance (Relevant Docs \\tleftrightarrow Query) and Accuracy (Relevant Docs \\tleftrightarrow Docs Candidates). Generation uses Response, Sample Response, Output, and Label to determine Relevance (Response \\tleftrightarrow Query), Faithfulness (Response \\trightarrow Relevant Docs), and Correctness (Response \\trightarrow Sample Response).\",\n",
      "      \"content_title\": \"3.1 Evaluation Target (What to Evaluate?)\"\n",
      "    },\n",
      "    {\n",
      "      \"content\": \"The EOs are the relevant documents for evaluating the retrieval component depending on the query. Then we can construct two pairwise relationships for the retrieval component, which are Relevant Documents \\tleftrightarrow Query, Relevant Documents \\tleftrightarrow Documents Candidates.Relevance (Relevant Documents \\tleftrightarrow Query) evaluates how well the retrieved documents match the information needed expressed in the query. It measures the precision and specificity of the retrieval process.Accuracy (Relevant Documents \\tleftrightarrow Documents Candidates) assesses how accurate the retrieved documents are in comparison to a set of candidate documents. It is a measure of the system's ability to identify and score relevant documents higher than less relevant or irrelevant ones.\",\n",
      "      \"content_title\": \"Retrieval\"\n",
      "    },\n",
      "    {\n",
      "      \"content\": \"The similar pairwise relations for the generation components are listed below. The EOs are the generated text and phrased structured content. Then we need to compare these EOs with the provided GTs and labels.Relevance (Response \\tleftrightarrow Query) measures how well the generated response aligns with the intent and content of the initial query. It ensures that the response is related to the query topic and meets the query's specific requirements.Faithfulness (Response \\trightarrow Relevant Documents) evaluates if the generated response accurately reflects the information contained within the relevant documents and measures the consistency between generated content and the source documents.Correctness (Response \\trightarrow Sample Response) Similar to the accuracy in the retrieval component, this measures the accuracy of the generated response against a sample response, which serves as a ground truth. It checks if the response is correct in terms of factual information and appropriate in the context of the query.\",\n",
      "      \"content_title\": \"Generation\"\n",
      "    },\n",
      "    {\n",
      "      \"content\": \"The targets of Retrieval and Generation components are introduced. Table 1 lists the relative work on improving and evaluating RAG and its benchmarks cut off in June.Table 1: The evaluating targets and corresponding metrics across various frameworks for evaluating RAG systems. The presentation distinguishes between the core areas of Retrieval and Generation considered in the evaluation. The different aspects of the evaluation are set as different colours in the table: Relevance, Accuracy of Retrieval and Faithfulness, Correctness and Relevance of Generation. The consideration of the Additional Requirements beyond the retrieval and generation component is also collected. Noted that quite a few of the works employed multiple methods or evaluated multiple aspects simultaneously.\",\n",
      "      \"content_title\": \"Table 1 Description\"\n",
      "    },\n",
      "    {\n",
      "      \"content\": \"| Category | Framework | Time | Raw Targets | Retrieval | Generation ||---|---|---|---|---|---|| Tool | TruEra RAG Triad [54] | 2023.10 | Context Relevance, Answer Relevance, Groundedness | LLM as a Judge | LLM as a Judge || Tool | LangChain Bench. [32] | 2023.11 | Accuracy, Faithfulness, Execution Time, Embed. CosDistance | Accuracy | LLM as a Judge || Tool | Databricks Eval [33] | 2023.12 | Correctness, Readability, Comprehensiveness | - | LLM as a Judge || Benchmark | RAGAS [14] | 2023.09 | Context Relevance, Answer Relevance, Faithfulness | LLM as a Judge | LLM Gen + CosSim, LLM as a Judge || Benchmark | RECALL [38] | 2023.11 | Response Quality, Robustness | - | BLEU, ROUGE-L || Benchmark | ARES [49] | 2023.11 | Context Relevance, Answer Faithfulness, Answer Relevance | LLM + Classifier | LLM + Classifier, LLM + Classifier || Benchmark | RGB [6] | 2023.12 | Information Integration, Noise Robustness, Negative Rejection, Counterfactual Robustness | - | Accuracy || Benchmark | MultiHop-RAG [52] | 2024.01 | Retrieval Quality, Response Correctness | MAP, MRR, Hit@K | LLM as a Judge || Benchmark | CRUD-RAG [39] | 2024.02 | CREATE, READ, UPDATE, DELETE | - | ROUGE, BLEU, RAGQuestEval || Benchmark | MedRAG [61] | 2024.02 | Accuracy, Consistency | - | Accuracy || Benchmark | FeB4RAG [57] | 2024.02 | Correctness, Clarity, Coverage | - | Human Evaluation, Human Evaluation || Benchmark | CDQA [62] | 2024.03 | Accuracy | - | F1 || Benchmark | DomainRAG [58] | 2024.06 | Correctness, Faithfulness, Noise Robustness, Structural Output | - | F1, Exact-Match, Rouge-L, LLM as a Judge || Benchmark | ReEval [66] | 2024.06 | Hallucination | - | F1, Exacct-Match, LLM as a Judge, Human Evaluation || Research | FiD-Light [20] | 2023.07 | Latency | - | - || Research | Diversity Reranker [4] | 2023.08 | Diversity | Cosine Distance | - |\",\n",
      "      \"content_title\": \"Table 1: The evaluating targets and corresponding metrics across various frameworks for evaluating RAG systems.\"\n",
      "    }\n",
      "  ],\n",
      "  \"source_id\": \"None\",\n",
      "  \"source_name\": \"Evaluation of Retrieval-Augmented Generation: A Survey\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "file_ref = genai.upload_file(\"./data/rag_research_paper/chunks_data/2405.07437v2_pages_5_6.pdf\", mime_type=\"application/pdf\")\n",
    "chunk = {'pages': '5-6'}\n",
    "\n",
    "try:\n",
    "    config = genai.GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=RESPONSE_FORMAT,\n",
    "            temperature=0.0)\n",
    "\n",
    "    model = genai.GenerativeModel('gemini-2.5-flash-preview-09-2025',\n",
    "        system_instruction=system_instruction_prompt\n",
    "    )\n",
    "\n",
    "    response = model.generate_content(\n",
    "        [file_ref, f\"Extract structured content from pages {chunk['pages']}.\"],\n",
    "        generation_config=config\n",
    "        )\n",
    "\n",
    "    print(\"Gemma response:\", response.text)\n",
    "except Exception as e:\n",
    "    print(f\"Gemma doesn't support this: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0667d93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResearchPaperData(content_title='3.1 Evaluation Target (What to Evaluate?)', content='The combination of EOs and GTs in the RAG system can generate all possible targets, which is the fundamental concept of the Auepora (as shown in Figure 1). Once identified, these targets can be defined based on a specific pair of EOs or EO with GT, as illustrated in Figure 2, and used to analyze all aspects of current RAG benchmarks.Fig. 2: The Target modular of the Auepora. The figure illustrates the flow and relationships between Query, Result, Ground Truth, Retrieval, and Generation components, mapping them to specific evaluation targets (Relevance, Accuracy, Faithfulness, Correctness) and Additional Requirements (Latency, Noise Robustness, Negative Rejection, Diversity, etc.). Retrieval uses Query, Relevant Docs, and Docs Candidates to determine Relevance (Relevant Docs \\tleftrightarrow Query) and Accuracy (Relevant Docs \\tleftrightarrow Docs Candidates). Generation uses Response, Sample Response, Output, and Label to determine Relevance (Response \\tleftrightarrow Query), Faithfulness (Response \\trightarrow Relevant Docs), and Correctness (Response \\trightarrow Sample Response).'),\n",
       " ResearchPaperData(content_title='Retrieval', content=\"The EOs are the relevant documents for evaluating the retrieval component depending on the query. Then we can construct two pairwise relationships for the retrieval component, which are Relevant Documents \\tleftrightarrow Query, Relevant Documents \\tleftrightarrow Documents Candidates.Relevance (Relevant Documents \\tleftrightarrow Query) evaluates how well the retrieved documents match the information needed expressed in the query. It measures the precision and specificity of the retrieval process.Accuracy (Relevant Documents \\tleftrightarrow Documents Candidates) assesses how accurate the retrieved documents are in comparison to a set of candidate documents. It is a measure of the system's ability to identify and score relevant documents higher than less relevant or irrelevant ones.\"),\n",
       " ResearchPaperData(content_title='Generation', content=\"The similar pairwise relations for the generation components are listed below. The EOs are the generated text and phrased structured content. Then we need to compare these EOs with the provided GTs and labels.Relevance (Response \\tleftrightarrow Query) measures how well the generated response aligns with the intent and content of the initial query. It ensures that the response is related to the query topic and meets the query's specific requirements.Faithfulness (Response \\trightarrow Relevant Documents) evaluates if the generated response accurately reflects the information contained within the relevant documents and measures the consistency between generated content and the source documents.Correctness (Response \\trightarrow Sample Response) Similar to the accuracy in the retrieval component, this measures the accuracy of the generated response against a sample response, which serves as a ground truth. It checks if the response is correct in terms of factual information and appropriate in the context of the query.\"),\n",
       " ResearchPaperData(content_title='Table 1 Description', content='The targets of Retrieval and Generation components are introduced. Table 1 lists the relative work on improving and evaluating RAG and its benchmarks cut off in June.Table 1: The evaluating targets and corresponding metrics across various frameworks for evaluating RAG systems. The presentation distinguishes between the core areas of Retrieval and Generation considered in the evaluation. The different aspects of the evaluation are set as different colours in the table: Relevance, Accuracy of Retrieval and Faithfulness, Correctness and Relevance of Generation. The consideration of the Additional Requirements beyond the retrieval and generation component is also collected. Noted that quite a few of the works employed multiple methods or evaluated multiple aspects simultaneously.'),\n",
       " ResearchPaperData(content_title='Table 1: The evaluating targets and corresponding metrics across various frameworks for evaluating RAG systems.', content='| Category | Framework | Time | Raw Targets | Retrieval | Generation ||---|---|---|---|---|---|| Tool | TruEra RAG Triad [54] | 2023.10 | Context Relevance, Answer Relevance, Groundedness | LLM as a Judge | LLM as a Judge || Tool | LangChain Bench. [32] | 2023.11 | Accuracy, Faithfulness, Execution Time, Embed. CosDistance | Accuracy | LLM as a Judge || Tool | Databricks Eval [33] | 2023.12 | Correctness, Readability, Comprehensiveness | - | LLM as a Judge || Benchmark | RAGAS [14] | 2023.09 | Context Relevance, Answer Relevance, Faithfulness | LLM as a Judge | LLM Gen + CosSim, LLM as a Judge || Benchmark | RECALL [38] | 2023.11 | Response Quality, Robustness | - | BLEU, ROUGE-L || Benchmark | ARES [49] | 2023.11 | Context Relevance, Answer Faithfulness, Answer Relevance | LLM + Classifier | LLM + Classifier, LLM + Classifier || Benchmark | RGB [6] | 2023.12 | Information Integration, Noise Robustness, Negative Rejection, Counterfactual Robustness | - | Accuracy || Benchmark | MultiHop-RAG [52] | 2024.01 | Retrieval Quality, Response Correctness | MAP, MRR, Hit@K | LLM as a Judge || Benchmark | CRUD-RAG [39] | 2024.02 | CREATE, READ, UPDATE, DELETE | - | ROUGE, BLEU, RAGQuestEval || Benchmark | MedRAG [61] | 2024.02 | Accuracy, Consistency | - | Accuracy || Benchmark | FeB4RAG [57] | 2024.02 | Correctness, Clarity, Coverage | - | Human Evaluation, Human Evaluation || Benchmark | CDQA [62] | 2024.03 | Accuracy | - | F1 || Benchmark | DomainRAG [58] | 2024.06 | Correctness, Faithfulness, Noise Robustness, Structural Output | - | F1, Exact-Match, Rouge-L, LLM as a Judge || Benchmark | ReEval [66] | 2024.06 | Hallucination | - | F1, Exacct-Match, LLM as a Judge, Human Evaluation || Research | FiD-Light [20] | 2023.07 | Latency | - | - || Research | Diversity Reranker [4] | 2023.08 | Diversity | Cosine Distance | - |')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESPONSE_FORMAT.model_validate_json(response.text).research_paper_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "96188cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FinishReason.STOP: 1>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates[0].finish_reason"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
