{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a50358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import nest_asyncio\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad7f23b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://developers.llamaindex.ai/python/framework/understanding/', 'title': 'Building an LLM application', 'text': 'Using LLMs: hit the ground running by getting started working with LLMs. We’ll show you how to use any of our dozens of supported LLMs, whether via remote API calls or running locally on your machine.\\n\\nBuilding agents: agents are LLM-powered knowledge workers that can interact with the world via a set of tools. Those tools can retrieve information (such as RAG, see below) or take action. This tutorial includes:\\n\\nBuilding a single agent: We show you how to build a simple agent that can interact with the world via a set of tools.\\n\\nUsing existing tools: LlamaIndex provides a registry of pre-built agent tools at LlamaHub that you can incorporate into your agents.\\n\\nMaintaining state: agents can maintain state, which is important for building more complex applications.\\n\\nStreaming output and events: providing visibility and feedback to the user is important, and streaming allows you to do that.\\n\\nHuman in the loop: getting human feedback to your agent can be critical.\\n\\nMulti-agent systems with AgentWorkflow: combining multiple agents to collaborate is a powerful technique for building more complex systems; this section shows you how to do so.\\n\\nWorkflows: Workflows are a lower-level, event-driven abstraction for building agentic applications. They’re the base layer you should be using to build any advanced agentic application. You can use the pre-built abstractions you learned above, or build agents completely from scratch. This tutorial covers:\\n\\nBuilding a simple workflow: a simple workflow that shows you how to use the Workflow class to build a basic agentic application.\\n\\nLooping and branching: these core control flow patterns are the building blocks of more complex workflows.\\n\\nConcurrent execution: you can run steps in parallel to split up work efficiently.\\n\\nStreaming events: your agents can emit user-facing events just like the agents you built above.\\n\\nStateful workflows: workflows can maintain state, which is important for building more complex applications.\\n\\nObservability: workflows can be traced and debugged using various integrations like Arize Pheonix, OpenTelemetry, and more.\\n\\nAdding RAG to your agents: Retrieval-Augmented Generation (RAG) is a key technique for getting your data to an LLM, and a component of more sophisticated agentic systems. We’ll show you how to enhance your agents with a full-featured RAG pipeline that can answer questions about your data. This includes:\\n\\nLoading & Ingestion: Getting your data from wherever it lives, whether that’s unstructured text, PDFs, databases, or APIs to other applications. LlamaIndex has hundreds of connectors to every data source over at LlamaHub.\\n\\nIndexing and Embedding: Once you’ve got your data there are an infinite number of ways to structure access to that data to ensure your applications is always working with the most relevant data. LlamaIndex has a huge number of these strategies built-in and can help you select the best ones.\\n\\nStoring: You will probably find it more efficient to store your data in indexed form, or pre-processed summaries provided by an LLM, often in a specialized database known as a Vector Store (see below). You can also store your indexes, metadata and more.\\n\\nQuerying: Every indexing strategy has a corresponding querying strategy and there are lots of ways to improve the relevance, speed and accuracy of what you retrieve and what the LLM does with it before returning it to you, including turning it into structured responses such as an API.\\n\\nPutting it all together: whether you are building question & answering, chatbots, an API, or an autonomous agent, we show you how to get your application into production.\\n\\nTracing and debugging: also called observability, it’s especially important with LLM applications to be able to look into the inner workings of what’s going on to help you debug problems and spot places to improve.\\n\\nEvaluating: every strategy has pros and cons and a key part of building, shipping and evolving your application is evaluating whether your change has improved your application in terms of accuracy, performance, clarity, cost and more. Reliably evaluating your changes is a crucial part of LLM application development.'}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import newspaper\n",
    "from newspaper import Config\n",
    "\n",
    "# 1. Create a configuration object\n",
    "config = Config()\n",
    "# 2. Tell it to allow \"binary\" urls (bypassing the bad check)\n",
    "config.allow_binary_content = True \n",
    "\n",
    "urls = [\n",
    "    \"https://developers.llamaindex.ai/python/framework/understanding/\",\n",
    "    \"https://developers.llamaindex.ai/python/framework/understanding/using_llms/\",\n",
    "    \"https://developers.llamaindex.ai/python/framework/understanding/rag/indexing/\",\n",
    "    \"https://developers.llamaindex.ai/python/framework/understanding/rag/querying/\",\n",
    "]   \n",
    "\n",
    "\n",
    "pages_content = []\n",
    "\n",
    "# Retrieve the Content\n",
    "for url in urls:\n",
    "    try:\n",
    "        article = newspaper.Article(url, config=config)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        if len(article.text) > 0:\n",
    "            pages_content.append(\n",
    "                {\"url\": url, \"title\": article.title, \"text\": article.text}\n",
    "            )\n",
    "    except:\n",
    "        print(f\"Failed to retrieve content from {url}\")\n",
    "        continue\n",
    "\n",
    "print(pages_content[0])\n",
    "print(len(pages_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "777c006a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://developers.llamaindex.ai/python/framework/understanding/',\n",
       "  'title': 'Building an LLM application',\n",
       "  'text': 'Using LLMs: hit the ground running by getting started working with LLMs. We’ll show you how to use any of our dozens of supported LLMs, whether via remote API calls or running locally on your machine.\\n\\nBuilding agents: agents are LLM-powered knowledge workers that can interact with the world via a set of tools. Those tools can retrieve information (such as RAG, see below) or take action. This tutorial includes:\\n\\nBuilding a single agent: We show you how to build a simple agent that can interact with the world via a set of tools.\\n\\nUsing existing tools: LlamaIndex provides a registry of pre-built agent tools at LlamaHub that you can incorporate into your agents.\\n\\nMaintaining state: agents can maintain state, which is important for building more complex applications.\\n\\nStreaming output and events: providing visibility and feedback to the user is important, and streaming allows you to do that.\\n\\nHuman in the loop: getting human feedback to your agent can be critical.\\n\\nMulti-agent systems with AgentWorkflow: combining multiple agents to collaborate is a powerful technique for building more complex systems; this section shows you how to do so.\\n\\nWorkflows: Workflows are a lower-level, event-driven abstraction for building agentic applications. They’re the base layer you should be using to build any advanced agentic application. You can use the pre-built abstractions you learned above, or build agents completely from scratch. This tutorial covers:\\n\\nBuilding a simple workflow: a simple workflow that shows you how to use the Workflow class to build a basic agentic application.\\n\\nLooping and branching: these core control flow patterns are the building blocks of more complex workflows.\\n\\nConcurrent execution: you can run steps in parallel to split up work efficiently.\\n\\nStreaming events: your agents can emit user-facing events just like the agents you built above.\\n\\nStateful workflows: workflows can maintain state, which is important for building more complex applications.\\n\\nObservability: workflows can be traced and debugged using various integrations like Arize Pheonix, OpenTelemetry, and more.\\n\\nAdding RAG to your agents: Retrieval-Augmented Generation (RAG) is a key technique for getting your data to an LLM, and a component of more sophisticated agentic systems. We’ll show you how to enhance your agents with a full-featured RAG pipeline that can answer questions about your data. This includes:\\n\\nLoading & Ingestion: Getting your data from wherever it lives, whether that’s unstructured text, PDFs, databases, or APIs to other applications. LlamaIndex has hundreds of connectors to every data source over at LlamaHub.\\n\\nIndexing and Embedding: Once you’ve got your data there are an infinite number of ways to structure access to that data to ensure your applications is always working with the most relevant data. LlamaIndex has a huge number of these strategies built-in and can help you select the best ones.\\n\\nStoring: You will probably find it more efficient to store your data in indexed form, or pre-processed summaries provided by an LLM, often in a specialized database known as a Vector Store (see below). You can also store your indexes, metadata and more.\\n\\nQuerying: Every indexing strategy has a corresponding querying strategy and there are lots of ways to improve the relevance, speed and accuracy of what you retrieve and what the LLM does with it before returning it to you, including turning it into structured responses such as an API.\\n\\nPutting it all together: whether you are building question & answering, chatbots, an API, or an autonomous agent, we show you how to get your application into production.\\n\\nTracing and debugging: also called observability, it’s especially important with LLM applications to be able to look into the inner workings of what’s going on to help you debug problems and spot places to improve.\\n\\nEvaluating: every strategy has pros and cons and a key part of building, shipping and evolving your application is evaluating whether your change has improved your application in terms of accuracy, performance, clarity, cost and more. Reliably evaluating your changes is a crucial part of LLM application development.'},\n",
       " {'url': 'https://developers.llamaindex.ai/python/framework/understanding/using_llms/',\n",
       "  'title': 'Using LLMs',\n",
       "  'text': 'One of the first steps when building an LLM-based application is which LLM to use; they have different strengths and price points and you may wish to use more than one.\\n\\nLlamaIndex provides a single interface to a large number of different LLMs. Using an LLM can be as simple as installing the appropriate integration:\\n\\nAnd then calling it in a one-liner:\\n\\nNote that this requires an API key called OPENAI_API_KEY in your environment; see the starter tutorial for more details.\\n\\ncomplete is also available as an async method, acomplete.\\n\\nYou can also get a streaming response by calling stream_complete, which returns a generator that yields tokens as they are produced:\\n\\nstream_complete is also available as an async method, astream_complete.\\n\\nThe LLM class also implements a chat method, which allows you to have more sophisticated interactions:\\n\\nstream_chat and astream_chat are also available.\\n\\nMany LLM integrations provide more than one model. You can specify a model by passing the model parameter to the LLM constructor:\\n\\nSome LLMs support multi-modal chat messages. This means that you can pass in a mix of text and other modalities (images, audio, video, etc.) and the LLM will handle it.\\n\\nCurrently, LlamaIndex supports text, images, and audio inside ChatMessages using content blocks.\\n\\nSome LLMs (OpenAI, Anthropic, Gemini, Ollama, etc.) support tool calling directly over API calls — this means tools and functions can be called without specific prompts and parsing mechanisms.\\n\\nFor more details on even more advanced tool calling, check out the in-depth guide using OpenAI. The same approaches work for any LLM that supports tools/functions (e.g. Anthropic, Gemini, Ollama, etc.).\\n\\nYou can learn more about tools and agents in the tools guide.\\n\\nWe support integrations with OpenAI, Anthropic, Mistral, DeepSeek, Hugging Face, and dozens more. Check out our module guide to LLMs for a full list, including how to run a local model.\\n\\nLlamaIndex doesn’t just support hosted LLM APIs; you can also run a local model such as Meta’s Llama 3 locally. For example, if you have Ollama installed and running:'},\n",
       " {'url': 'https://developers.llamaindex.ai/python/framework/understanding/rag/indexing/',\n",
       "  'title': 'Indexing',\n",
       "  'text': 'With your data loaded, you now have a list of Document objects (or a list of Nodes). It’s time to build an Index over these objects so you can start querying them.\\n\\nIn LlamaIndex terms, an Index is a data structure composed of Document objects, designed to enable querying by an LLM. Your Index is designed to be complementary to your querying strategy.\\n\\nLlamaIndex offers several different index types. We’ll cover the two most common here.\\n\\nA VectorStoreIndex is by far the most frequent type of Index you’ll encounter. The Vector Store Index takes your Documents and splits them up into Nodes. It then creates vector embeddings of the text of every node, ready to be queried by an LLM.\\n\\nVector embeddings are central to how LLM applications function.\\n\\nA vector embedding, often just called an embedding, is a numerical representation of the semantics, or meaning of your text. Two pieces of text with similar meanings will have mathematically similar embeddings, even if the actual text is quite different.\\n\\nThis mathematical relationship enables semantic search, where a user provides query terms and LlamaIndex can locate text that is related to the meaning of the query terms rather than simple keyword matching. This is a big part of how Retrieval-Augmented Generation works, and how LLMs function in general.\\n\\nThere are many types of embeddings, and they vary in efficiency, effectiveness and computational cost. By default LlamaIndex uses text-embedding-ada-002, which is the default embedding used by OpenAI. If you are using different LLMs you will often want to use different embeddings.\\n\\nVector Store Index turns all of your text into embeddings using an API from your LLM; this is what is meant when we say it “embeds your text”. If you have a lot of text, generating embeddings can take a long time since it involves many round-trip API calls.\\n\\nWhen you want to search your embeddings, your query is itself turned into a vector embedding, and then a mathematical operation is carried out by VectorStoreIndex to rank all the embeddings by how semantically similar they are to your query.\\n\\nOnce the ranking is complete, VectorStoreIndex returns the most-similar embeddings as their corresponding chunks of text. The number of embeddings it returns is known as k, so the parameter controlling how many embeddings to return is known as top_k. This whole type of search is often referred to as “top-k semantic retrieval” for this reason.\\n\\nTop-k retrieval is the simplest form of querying a vector index; you will learn about more complex and subtler strategies when you read the querying section.\\n\\nTo use the Vector Store Index, pass it the list of Documents you created during the loading stage:\\n\\nYou can also choose to build an index over a list of Node objects directly:\\n\\nWith your text indexed, it is now technically ready for querying! However, embedding all your text can be time-consuming and, if you are using a hosted LLM, it can also be expensive. To save time and money you will want to store your embeddings first.\\n\\nA Summary Index is a simpler form of Index best suited to queries where, as the name suggests, you are trying to generate a summary of the text in your Documents. It simply stores all of the Documents and returns all of them to your query engine.'},\n",
       " {'url': 'https://developers.llamaindex.ai/python/framework/understanding/rag/querying/',\n",
       "  'title': 'Querying',\n",
       "  'text': 'Now you’ve loaded your data, built an index, and stored that index for later, you’re ready to get to the most significant part of an LLM application: querying.\\n\\nAt its simplest, querying is just a prompt call to an LLM: it can be a question and get an answer, or a request for summarization, or a much more complex instruction.\\n\\nMore complex querying could involve repeated/chained prompt + LLM calls, or even a reasoning loop across multiple components.\\n\\nThe basis of all querying is the QueryEngine. The simplest way to get a QueryEngine is to get your index to create one for you, like this:\\n\\nHowever, there is more to querying than initially meets the eye. Querying consists of three distinct stages:\\n\\nRetrieval is when you find and return the most relevant documents for your query from your Index. As previously discussed in indexing, the most common type of retrieval is “top-k” semantic retrieval, but there are many other retrieval strategies.\\n\\nPostprocessing is when the Nodes retrieved are optionally reranked, transformed, or filtered, for instance by requiring that they have specific metadata such as keywords attached.\\n\\nResponse synthesis is when your query, your most-relevant data and your prompt are combined and sent to your LLM to return a response.\\n\\nLlamaIndex features a low-level composition API that gives you granular control over your querying.\\n\\nIn this example, we customize our retriever to use a different number for top_k and add a post-processing step that requires that the retrieved nodes reach a minimum similarity score to be included. This would give you a lot of data when you have relevant results but potentially no data if you have nothing relevant.\\n\\nYou can also add your own retrieval, response synthesis, and overall query logic, by implementing the corresponding interfaces.\\n\\nFor a full list of implemented components and the supported configurations, check out our reference docs.\\n\\nLet’s go into more detail about customizing each step:\\n\\nThere are a huge variety of retrievers that you can learn about in our module guide on retrievers.\\n\\nWe support advanced Node filtering and augmentation that can further improve the relevancy of the retrieved Node objects. This can help reduce the time/number of LLM calls/cost or improve response quality.\\n\\nFor example:\\n\\nKeywordNodePostprocessor: filters nodes by required_keywords and exclude_keywords.\\n\\nSimilarityPostprocessor: filters nodes by setting a threshold on the similarity score (thus only supported by embedding-based retrievers)\\n\\nPrevNextNodePostprocessor: augments retrieved Node objects with additional relevant context based on Node relationships.\\n\\nThe full list of node postprocessors is documented in the Node Postprocessor Reference.\\n\\nTo configure the desired node postprocessors:\\n\\nAfter a retriever fetches relevant nodes, a BaseSynthesizer synthesizes the final response by combining the information.\\n\\nYou can configure it via\\n\\nRight now, we support the following options:\\n\\ndefault: “create and refine” an answer by sequentially going through each retrieved Node; This makes a separate LLM call per Node. Good for more detailed answers.\\n\\ncompact: “compact” the prompt during each LLM call by stuffing as many Node text chunks that can fit within the maximum prompt size. If there are too many chunks to stuff in one prompt, “create and refine” an answer by going through multiple prompts.\\n\\ntree_summarize: Given a set of Node objects and the query, recursively construct a tree and return the root node as the response. Good for summarization purposes.\\n\\nno_text: Only runs the retriever to fetch the nodes that would have been sent to the LLM, without actually sending them. Then can be inspected by checking response.source_nodes. The response object is covered in more detail in Section 5.\\n\\naccumulate: Given a set of Node objects and the query, apply the query to each Node text chunk while accumulating the responses into an array. Returns a concatenated string of all responses. Good for when you need to run the same query separately against each text chunk.\\n\\nYou may want to ensure your output is structured. See our Query Engines + Pydantic Outputs to see how to extract a Pydantic object from a query engine class.\\n\\nAlso make sure to check out our entire Structured Outputs guide.\\n\\nIf you want to design complex query flows, you can compose your own query workflow across many different modules, from prompts/LLMs/output parsers to retrievers to response synthesizers to your own custom components.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e20312",
   "metadata": {},
   "source": [
    "Convert that into the Document so Llama-index understand it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0fd4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Document\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "documents = [\n",
    "    Document(text=row[\"text\"], metadata={\"title\": row[\"title\"], \"url\": row[\"url\"]})\n",
    "    for row in pages_content\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7dfe64",
   "metadata": {},
   "source": [
    "## Now we will use the **crawl4ai** for scrapt the page\n",
    "Given in 04.1 py file\n",
    "\n",
    "Here are the concise notes for your notebook:\n",
    "\n",
    "### 1. What is Crawl4AI?\n",
    "\n",
    "**Crawl4AI** is an asynchronous, browser-based web crawler built specifically for RAG and LLM applications. Unlike traditional crawlers that return raw HTML, Crawl4AI renders JavaScript (handling dynamic content like React apps) and converts the page directly into clean **Markdown**. This structure preserves headers, tables, and lists in a format that LLMs can easily understand, while stripping away \"noise\" like navigation bars and ads.\n",
    "\n",
    "### 2. What does the code do?\n",
    "\n",
    "The script configures a crawler to visit a list of URLs in parallel (concurrently) using a headless browser. It forces a fresh download (bypassing cache), filters out empty pages, and extracts the content as raw Markdown. It also includes a fallback mechanism: if the website's official metadata is missing a title, the script manually scans the Markdown headers (lines starting with `#`) to identify the document's topic. Finally, it structures the output into a clean JSON format separating the text payload from the metadata.\n",
    "\n",
    "### 3. Why we added the sync function?\n",
    "\n",
    "The `crawl4ai` library is **asynchronous** (it uses `await` to handle network delays efficiently), but most standard Python scripts and notebooks run **synchronously** (line-by-line). The `crawl_sync` function acts as a **bridge or adapter** between these two worlds. It manages the complex \"Event Loop\" internally—starting the engine, running the async task, and then shutting it down—allowing you to call the crawler with a simple, blocking function call without rewriting your entire codebase to be async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71cbd4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.core import VectorStoreIndex\n",
    "import google.genai.types as types\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
    "    max_output_tokens=1024,\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    generation_config=config,\n",
    "    )\n",
    "\n",
    "Settings.embed_model = CohereEmbedding(\n",
    "    model_name=\"embed-english-v3.0\",\n",
    "    input_type=\"search_document\",\n",
    "    api_key=COHERE_API_KEY\n",
    ")\n",
    "Settings.llm = llm\n",
    "Settings.text_splitter = SentenceSplitter(chunk_size=300, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56f9f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fb800e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The basis of all querying is the QueryEngine. The simplest way to get a QueryEngine is to get an index to create one for you.\n"
     ]
    }
   ],
   "source": [
    "res = query_engine.query(\"What is a query engine?\")\n",
    "print(res.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f611c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID\t 66bba379-ab53-4924-a7ff-4a6f5f2520bb\n",
      "Title\t Querying\n",
      "URL\t https://developers.llamaindex.ai/python/framework/understanding/rag/querying/\n",
      "Score\t 0.3982705347745195\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t 05274416-3668-4677-b9aa-cbd1da89306c\n",
      "Title\t Querying\n",
      "URL\t https://developers.llamaindex.ai/python/framework/understanding/rag/querying/\n",
      "Score\t 0.3661506833164389\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "# Show the retrieved nodes\n",
    "for src in res.source_nodes:\n",
    "    print(\"Node ID\\t\", src.node_id)\n",
    "    print(\"Title\\t\", src.metadata[\"title\"])\n",
    "    print(\"URL\\t\", src.metadata[\"url\"])\n",
    "    print(\"Score\\t\", src.score)\n",
    "    print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54e7c129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context provided does not contain information about the `show_progress` parameter for the `VectorStoreIndex.from_documents` method. Therefore, I cannot answer whether you can pass this parameter.\n"
     ]
    }
   ],
   "source": [
    "res = query_engine.query(\"Can I pass show_progress parameter to the VectorStoreIndex.from_documents method?\")\n",
    "print(res.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b2f5b9",
   "metadata": {},
   "source": [
    "> This is a classic limitation of Newspaper4k (and its predecessor Newspaper3k). It doesn't extract the Code block and Pro Tip Content\n",
    "The reason it failed is that Newspaper4k is \"opinionated.\" It is programmed to believe that \"content\" means paragraphs of storytelling text (like a CNN or New York Times article).\n",
    "\n",
    "We will use Crawl4kAI for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "412559b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID\t 4ca5cb27-b3e0-4592-bd4a-466b5853232c\n",
      "Title\t Indexing\n",
      "URL\t https://developers.llamaindex.ai/python/framework/understanding/rag/indexing/\n",
      "Text\t With your data loaded, you now have a list of Document objects (or a list of Nodes). It’s time to build an Index over these objects so you can start querying them.\n",
      "\n",
      "In LlamaIndex terms, an Index is a data structure composed of Document objects, designed to enable querying by an LLM. Your Index is designed to be complementary to your querying strategy.\n",
      "\n",
      "LlamaIndex offers several different index types. We’ll cover the two most common here.\n",
      "\n",
      "A VectorStoreIndex is by far the most frequent type of Index you’ll encounter. The Vector Store Index takes your Documents and splits them up into Nodes. It then creates vector embeddings of the text of every node, ready to be queried by an LLM.\n",
      "\n",
      "Vector embeddings are central to how LLM applications function.\n",
      "\n",
      "A vector embedding, often just called an embedding, is a numerical representation of the semantics, or meaning of your text. Two pieces of text with similar meanings will have mathematically similar embeddings, even if the actual text is quite different.\n",
      "\n",
      "This mathematical relationship enables semantic search, where a user provides query terms and LlamaIndex can locate text that is related to the meaning of the query terms rather than simple keyword matching. This is a big part of how Retrieval-Augmented Generation works, and how LLMs function in general.\n",
      "\n",
      "There are many types of embeddings, and they vary in efficiency, effectiveness and computational cost. By default LlamaIndex uses text-embedding-ada-002, which is the default embedding used by OpenAI. If you are using different LLMs you will often want to use different embeddings.\n",
      "\n",
      "Vector Store Index turns all of your text into embeddings using an API from your LLM; this is what is meant when we say it “embeds your text”. If you have a lot of text, generating embeddings can take a long time since it involves many round-trip API calls.\n",
      "\n",
      "When you want to search your embeddings, your query is itself turned into a vector embedding, and then a mathematical operation is carried out by VectorStoreIndex to rank all the embeddings by how semantically similar they are to your query.\n",
      "\n",
      "Once the ranking is complete, VectorStoreIndex returns the most-similar embeddings as their corresponding chunks of text. The number of embeddings it returns is known as k, so the parameter controlling how many embeddings to return is known as top_k.\n",
      "Score\t 0.5285414422239719\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t 158ee0ba-529e-4efa-8639-f4bffeab799a\n",
      "Title\t Indexing\n",
      "URL\t https://developers.llamaindex.ai/python/framework/understanding/rag/indexing/\n",
      "Text\t The number of embeddings it returns is known as k, so the parameter controlling how many embeddings to return is known as top_k. This whole type of search is often referred to as “top-k semantic retrieval” for this reason.\n",
      "\n",
      "Top-k retrieval is the simplest form of querying a vector index; you will learn about more complex and subtler strategies when you read the querying section.\n",
      "\n",
      "To use the Vector Store Index, pass it the list of Documents you created during the loading stage:\n",
      "\n",
      "You can also choose to build an index over a list of Node objects directly:\n",
      "\n",
      "With your text indexed, it is now technically ready for querying! However, embedding all your text can be time-consuming and, if you are using a hosted LLM, it can also be expensive. To save time and money you will want to store your embeddings first.\n",
      "\n",
      "A Summary Index is a simpler form of Index best suited to queries where, as the name suggests, you are trying to generate a summary of the text in your Documents. It simply stores all of the Documents and returns all of them to your query engine.\n",
      "Score\t 0.5153542691019533\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "# Show the retrieved nodes\n",
    "for src in res.source_nodes:\n",
    "    print(\"Node ID\\t\", src.node_id)\n",
    "    print(\"Title\\t\", src.metadata[\"title\"])\n",
    "    print(\"URL\\t\", src.metadata[\"url\"])\n",
    "    print(\"Text\\t\", src.text)\n",
    "    print(\"Score\\t\", src.score)\n",
    "    print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e0637e",
   "metadata": {},
   "source": [
    "Both methods allow you to leverage the extensive data available on the Internet to enhance your chatbot with current and high-quality information. The choice between them depends on your specific needs: use the newspaper library for simple, targeted scraping of known URLs, or employ Crawl4AI when you need more robust crawling capabilities, JavaScript support, or want to avoid external service dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f730ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
