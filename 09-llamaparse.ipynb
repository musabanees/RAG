{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "587688da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "LLAMA_CLOUD_API_KEY = os.environ[\"LLAMA_CLOUD_API_KEY\"]\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e69cc3",
   "metadata": {},
   "source": [
    "## Parse directory to LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66ff7d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "#Parser\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13919760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 96b89163-f61e-4469-8980-cb50a5766dfe\n"
     ]
    }
   ],
   "source": [
    "pdf_dir = \"./data/rag_research_paper/chunks_data/2405.07437v2_pages_3_4.pdf\"\n",
    "\n",
    "document = await parser.aparse(pdf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d56bc84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Evaluation of Retrieval-Augmented Generation: A Survey                3\n",
      "\n",
      "by the external dynamic database and the various downstream tasks, such as content\n",
      "creation or open domain question answering [16,70]. These challenges necessitate the\n",
      "development of comprehensive evaluation metrics that can effectively capture the in-\n",
      "terplay between retrieval accuracy and generative quality [2,7]. To clarify the elements\n",
      "further, we try to address the current gaps in the area, which differs from the prior RAG\n",
      "surveys [74,16,24] that predominantly collected specific RAG methods or data. We have\n",
      "compiled 12 distinct evaluation frameworks, encompassing a range of aspects of the\n",
      "RAG system. Following the procedure of making benchmarks, we analyze through tar-\n",
      "gets, datasets and metrics mentioned in these benchmarks and summarize them into A\n",
      "Unified Evaluation Process of RAG (Auepora) as three corresponding phases.\n",
      "\n",
      "    For this paper, we contribute in the following aspects:\n",
      "\n",
      " 1.  Challenge of Evaluation: This is the first work that summarizes and classifies\n",
      "     the challenges in evaluating RAG systems through the structure of RAG systems,\n",
      "     including three parts retrieval, generation, and the whole system.\n",
      " 2.  Analysis Framework: In light of the challenges posed by RAG systems, we intro-\n",
      "     duce an analytical framework, referred to as A Unified Evaluation Process of RAG\n",
      "     (Auepora), which aims to elucidate the unique complexities inherent to RAG sys-\n",
      "     tems and guide for readers to comprehend the effectiveness of RAG benchmarks\n",
      "     across various dimensions\n",
      " 3.  RAG Benchmark Analysis: With the help of Auepora, we comprehensively an-\n",
      "     alyze existing RAG benchmarks, highlighting their strengths and limitations and\n",
      "     proposing recommendations for future developments in RAG system evaluation.\n",
      "\n",
      "2    Challenges in Evaluating RAG Systems\n",
      "\n",
      "Evaluating hybrid RAG systems entails evaluating retrieval, generation and the RAG\n",
      "system as a whole. These evaluations are multifaceted, requiring careful consideration\n",
      "and analysis. Each of them encompasses specific difficulties that complicate the devel-\n",
      "opment of a comprehensive evaluation framework and benchmarks for RAG systems.\n",
      "\n",
      "Retrieval The retrieval component is critical for fetching relevant information that in-\n",
      "forms the generation process. One primary challenge is the dynamic and vast nature of\n",
      "potential knowledge bases, ranging from structured databases to the entire web. This\n",
      "vastness requires evaluation metrics that can effectively measure the precision, recall,\n",
      "and relevance of retrieved documents in the context of a given query [52,32]. More-\n",
      "over, the temporal aspect of information, where the relevance and accuracy of data can\n",
      "change over time, adds another layer of complexity to the evaluation process [6]. Addi-\n",
      "tionally, the diversity of information sources and the possibility of retrieving misleading\n",
      "or low-quality information pose significant challenges in assessing the effectiveness of\n",
      "filtering and selecting the most pertinent information [39]. The traditional evaluation\n",
      "indicators for retrieval, such as Recall and Precision, cannot fully capture the nuances\n",
      "of RAG retrieval systems, necessitating the development of more nuanced and task-\n",
      "specific evaluation metrics [49].\n"
     ]
    }
   ],
   "source": [
    "print(document.pages[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63746a9",
   "metadata": {},
   "source": [
    "## LlamaParse JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adc47973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 997bdef5-4a07-42d8-8471-0fd934dd7959\n"
     ]
    }
   ],
   "source": [
    "json_obj = parser.get_json_result(pdf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57892fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': 1,\n",
       " 'text': '          Evaluation of Retrieval-Augmented Generation: A Survey                3\\n\\nby the external dynamic database and the various downstream tasks, such as content\\ncreation or open domain question answering [16,70]. These challenges necessitate the\\ndevelopment of comprehensive evaluation metrics that can effectively capture the in-\\nterplay between retrieval accuracy and generative quality [2,7]. To clarify the elements\\nfurther, we try to address the current gaps in the area, which differs from the prior RAG\\nsurveys [74,16,24] that predominantly collected specific RAG methods or data. We have\\ncompiled 12 distinct evaluation frameworks, encompassing a range of aspects of the\\nRAG system. Following the procedure of making benchmarks, we analyze through tar-\\ngets, datasets and metrics mentioned in these benchmarks and summarize them into A\\nUnified Evaluation Process of RAG (Auepora) as three corresponding phases.\\n\\n    For this paper, we contribute in the following aspects:\\n\\n 1.  Challenge of Evaluation: This is the first work that summarizes and classifies\\n     the challenges in evaluating RAG systems through the structure of RAG systems,\\n     including three parts retrieval, generation, and the whole system.\\n 2.  Analysis Framework: In light of the challenges posed by RAG systems, we intro-\\n     duce an analytical framework, referred to as A Unified Evaluation Process of RAG\\n     (Auepora), which aims to elucidate the unique complexities inherent to RAG sys-\\n     tems and guide for readers to comprehend the effectiveness of RAG benchmarks\\n     across various dimensions\\n 3.  RAG Benchmark Analysis: With the help of Auepora, we comprehensively an-\\n     alyze existing RAG benchmarks, highlighting their strengths and limitations and\\n     proposing recommendations for future developments in RAG system evaluation.\\n\\n2    Challenges in Evaluating RAG Systems\\n\\nEvaluating hybrid RAG systems entails evaluating retrieval, generation and the RAG\\nsystem as a whole. These evaluations are multifaceted, requiring careful consideration\\nand analysis. Each of them encompasses specific difficulties that complicate the devel-\\nopment of a comprehensive evaluation framework and benchmarks for RAG systems.\\n\\nRetrieval The retrieval component is critical for fetching relevant information that in-\\nforms the generation process. One primary challenge is the dynamic and vast nature of\\npotential knowledge bases, ranging from structured databases to the entire web. This\\nvastness requires evaluation metrics that can effectively measure the precision, recall,\\nand relevance of retrieved documents in the context of a given query [52,32]. More-\\nover, the temporal aspect of information, where the relevance and accuracy of data can\\nchange over time, adds another layer of complexity to the evaluation process [6]. Addi-\\ntionally, the diversity of information sources and the possibility of retrieving misleading\\nor low-quality information pose significant challenges in assessing the effectiveness of\\nfiltering and selecting the most pertinent information [39]. The traditional evaluation\\nindicators for retrieval, such as Recall and Precision, cannot fully capture the nuances\\nof RAG retrieval systems, necessitating the development of more nuanced and task-\\nspecific evaluation metrics [49].',\n",
       " 'md': '\\n# Evaluation of Retrieval-Augmented Generation: A Survey\\n\\n# 1. Introduction\\n\\nby the external dynamic database and the various downstream tasks, such as content creation or open domain question answering [16,70]. These challenges necessitate the development of comprehensive evaluation metrics that can effectively capture the interplay between retrieval accuracy and generative quality [2,7]. To clarify the elements further, we try to address the current gaps in the area, which differs from the prior RAG surveys [74,16,24] that predominantly collected specific RAG methods or data. We have compiled 12 distinct evaluation frameworks, encompassing a range of aspects of the RAG system. Following the procedure of making benchmarks, we analyze through targets, datasets and metrics mentioned in these benchmarks and summarize them into A Unified Evaluation Process of RAG (Auepora) as three corresponding phases.\\n\\n# 2. Challenges in Evaluating RAG Systems\\n\\nEvaluating hybrid RAG systems entails evaluating retrieval, generation and the RAG system as a whole. These evaluations are multifaceted, requiring careful consideration and analysis. Each of them encompasses specific difficulties that complicate the development of a comprehensive evaluation framework and benchmarks for RAG systems.\\n\\n# Retrieval\\n\\nThe retrieval component is critical for fetching relevant information that informs the generation process. One primary challenge is the dynamic and vast nature of potential knowledge bases, ranging from structured databases to the entire web. This vastness requires evaluation metrics that can effectively measure the precision, recall, and relevance of retrieved documents in the context of a given query [52,32]. Moreover, the temporal aspect of information, where the relevance and accuracy of data can change over time, adds another layer of complexity to the evaluation process [6]. Additionally, the diversity of information sources and the possibility of retrieving misleading or low-quality information pose significant challenges in assessing the effectiveness of filtering and selecting the most pertinent information [39]. The traditional evaluation indicators for retrieval, such as Recall and Precision, cannot fully capture the nuances of RAG retrieval systems, necessitating the development of more nuanced and task-specific evaluation metrics [49].\\n\\n# 3. Contributions\\n\\nFor this paper, we contribute in the following aspects:\\n\\n1. Challenge of Evaluation: This is the first work that summarizes and classifies the challenges in evaluating RAG systems through the structure of RAG systems, including three parts retrieval, generation, and the whole system.\\n2. Analysis Framework: In light of the challenges posed by RAG systems, we introduce an analytical framework, referred to as A Unified Evaluation Process of RAG (Auepora), which aims to elucidate the unique complexities inherent to RAG systems and guide for readers to comprehend the effectiveness of RAG benchmarks across various dimensions.\\n3. RAG Benchmark Analysis: With the help of Auepora, we comprehensively analyze existing RAG benchmarks, highlighting their strengths and limitations and proposing recommendations for future developments in RAG system evaluation.\\n\\n',\n",
       " 'images': [],\n",
       " 'charts': [],\n",
       " 'items': [{'type': 'heading',\n",
       "   'md': '# Evaluation of Retrieval-Augmented Generation: A Survey',\n",
       "   'value': 'Evaluation of Retrieval-Augmented Generation: A Survey',\n",
       "   'lvl': 1,\n",
       "   'bBox': {'x': 239.06,\n",
       "    'y': 90.93,\n",
       "    'w': 208.49,\n",
       "    'h': 8.97,\n",
       "    'confidence': 1,\n",
       "    'label': 'heading'}},\n",
       "  {'type': 'heading',\n",
       "   'md': '# 1. Introduction',\n",
       "   'value': '1. Introduction',\n",
       "   'lvl': 1,\n",
       "   'bBox': {'x': 139.25,\n",
       "    'y': 268.88,\n",
       "    'w': 7.47,\n",
       "    'h': 9.96,\n",
       "    'confidence': 1,\n",
       "    'label': 'heading'}},\n",
       "  {'type': 'text',\n",
       "   'md': 'by the external dynamic database and the various downstream tasks, such as content creation or open domain question answering [16,70]. These challenges necessitate the development of comprehensive evaluation metrics that can effectively capture the interplay between retrieval accuracy and generative quality [2,7]. To clarify the elements further, we try to address the current gaps in the area, which differs from the prior RAG surveys [74,16,24] that predominantly collected specific RAG methods or data. We have compiled 12 distinct evaluation frameworks, encompassing a range of aspects of the RAG system. Following the procedure of making benchmarks, we analyze through targets, datasets and metrics mentioned in these benchmarks and summarize them into A Unified Evaluation Process of RAG (Auepora) as three corresponding phases.',\n",
       "   'value': 'by the external dynamic database and the various downstream tasks, such as content creation or open domain question answering [16,70]. These challenges necessitate the development of comprehensive evaluation metrics that can effectively capture the interplay between retrieval accuracy and generative quality [2,7]. To clarify the elements further, we try to address the current gaps in the area, which differs from the prior RAG surveys [74,16,24] that predominantly collected specific RAG methods or data. We have compiled 12 distinct evaluation frameworks, encompassing a range of aspects of the RAG system. Following the procedure of making benchmarks, we analyze through targets, datasets and metrics mentioned in these benchmarks and summarize them into A Unified Evaluation Process of RAG (Auepora) as three corresponding phases.',\n",
       "   'bBox': {'x': 134.76,\n",
       "    'y': 115.84,\n",
       "    'w': 345.83,\n",
       "    'h': 117.56,\n",
       "    'confidence': 1,\n",
       "    'label': 'text'}},\n",
       "  {'type': 'heading',\n",
       "   'md': '# 2. Challenges in Evaluating RAG Systems',\n",
       "   'value': '2. Challenges in Evaluating RAG Systems',\n",
       "   'lvl': 1,\n",
       "   'bBox': {'x': 139.25,\n",
       "    'y': 305.07,\n",
       "    'w': 213.94,\n",
       "    'h': 127.47,\n",
       "    'confidence': 1,\n",
       "    'label': 'heading'}},\n",
       "  {'type': 'text',\n",
       "   'md': 'Evaluating hybrid RAG systems entails evaluating retrieval, generation and the RAG system as a whole. These evaluations are multifaceted, requiring careful consideration and analysis. Each of them encompasses specific difficulties that complicate the development of a comprehensive evaluation framework and benchmarks for RAG systems.',\n",
       "   'value': 'Evaluating hybrid RAG systems entails evaluating retrieval, generation and the RAG system as a whole. These evaluations are multifaceted, requiring careful consideration and analysis. Each of them encompasses specific difficulties that complicate the development of a comprehensive evaluation framework and benchmarks for RAG systems.',\n",
       "   'bBox': {'x': 134.77,\n",
       "    'y': 448.11,\n",
       "    'w': 345.82,\n",
       "    'h': 45.83,\n",
       "    'confidence': 1,\n",
       "    'label': 'text'}},\n",
       "  {'type': 'heading',\n",
       "   'md': '# Retrieval',\n",
       "   'value': 'Retrieval',\n",
       "   'lvl': 1,\n",
       "   'bBox': {'x': 134.77,\n",
       "    'y': 509.5,\n",
       "    'w': 37.08,\n",
       "    'h': 9.96,\n",
       "    'confidence': 1,\n",
       "    'label': 'heading'}},\n",
       "  {'type': 'text',\n",
       "   'md': 'The retrieval component is critical for fetching relevant information that informs the generation process. One primary challenge is the dynamic and vast nature of potential knowledge bases, ranging from structured databases to the entire web. This vastness requires evaluation metrics that can effectively measure the precision, recall, and relevance of retrieved documents in the context of a given query [52,32]. Moreover, the temporal aspect of information, where the relevance and accuracy of data can change over time, adds another layer of complexity to the evaluation process [6]. Additionally, the diversity of information sources and the possibility of retrieving misleading or low-quality information pose significant challenges in assessing the effectiveness of filtering and selecting the most pertinent information [39]. The traditional evaluation indicators for retrieval, such as Recall and Precision, cannot fully capture the nuances of RAG retrieval systems, necessitating the development of more nuanced and task-specific evaluation metrics [49].',\n",
       "   'value': 'The retrieval component is critical for fetching relevant information that informs the generation process. One primary challenge is the dynamic and vast nature of potential knowledge bases, ranging from structured databases to the entire web. This vastness requires evaluation metrics that can effectively measure the precision, recall, and relevance of retrieved documents in the context of a given query [52,32]. Moreover, the temporal aspect of information, where the relevance and accuracy of data can change over time, adds another layer of complexity to the evaluation process [6]. Additionally, the diversity of information sources and the possibility of retrieving misleading or low-quality information pose significant challenges in assessing the effectiveness of filtering and selecting the most pertinent information [39]. The traditional evaluation indicators for retrieval, such as Recall and Precision, cannot fully capture the nuances of RAG retrieval systems, necessitating the development of more nuanced and task-specific evaluation metrics [49].',\n",
       "   'bBox': {'x': 134.77,\n",
       "    'y': 90.93,\n",
       "    'w': 345.83,\n",
       "    'h': 571.99,\n",
       "    'confidence': 1,\n",
       "    'label': 'text'}},\n",
       "  {'type': 'heading',\n",
       "   'md': '# 3. Contributions',\n",
       "   'value': '3. Contributions',\n",
       "   'lvl': 1,\n",
       "   'bBox': {'x': 139.25,\n",
       "    'y': 365.17,\n",
       "    'w': 7.47,\n",
       "    'h': 9.96,\n",
       "    'confidence': 1,\n",
       "    'label': 'heading'}},\n",
       "  {'type': 'text',\n",
       "   'md': 'For this paper, we contribute in the following aspects:',\n",
       "   'value': 'For this paper, we contribute in the following aspects:',\n",
       "   'bBox': {'x': 149.71,\n",
       "    'y': 247.67,\n",
       "    'w': 214.01,\n",
       "    'h': 9.96,\n",
       "    'confidence': 1,\n",
       "    'label': 'text'}},\n",
       "  {'type': 'text',\n",
       "   'md': '1. Challenge of Evaluation: This is the first work that summarizes and classifies the challenges in evaluating RAG systems through the structure of RAG systems, including three parts retrieval, generation, and the whole system.\\n2. Analysis Framework: In light of the challenges posed by RAG systems, we introduce an analytical framework, referred to as A Unified Evaluation Process of RAG (Auepora), which aims to elucidate the unique complexities inherent to RAG systems and guide for readers to comprehend the effectiveness of RAG benchmarks across various dimensions.\\n3. RAG Benchmark Analysis: With the help of Auepora, we comprehensively analyze existing RAG benchmarks, highlighting their strengths and limitations and proposing recommendations for future developments in RAG system evaluation.',\n",
       "   'value': '1. Challenge of Evaluation: This is the first work that summarizes and classifies the challenges in evaluating RAG systems through the structure of RAG systems, including three parts retrieval, generation, and the whole system.\\n2. Analysis Framework: In light of the challenges posed by RAG systems, we introduce an analytical framework, referred to as A Unified Evaluation Process of RAG (Auepora), which aims to elucidate the unique complexities inherent to RAG systems and guide for readers to comprehend the effectiveness of RAG benchmarks across various dimensions.\\n3. RAG Benchmark Analysis: With the help of Auepora, we comprehensively analyze existing RAG benchmarks, highlighting their strengths and limitations and proposing recommendations for future developments in RAG system evaluation.',\n",
       "   'bBox': {'x': 139.25,\n",
       "    'y': 268.88,\n",
       "    'w': 341.35,\n",
       "    'h': 130.16,\n",
       "    'confidence': 1,\n",
       "    'label': 'text'}}],\n",
       " 'status': 'OK',\n",
       " 'originalOrientationAngle': 0,\n",
       " 'links': [],\n",
       " 'width': 612,\n",
       " 'height': 792,\n",
       " 'triggeredAutoMode': False,\n",
       " 'parsingMode': 'accurate',\n",
       " 'structuredData': None,\n",
       " 'noStructuredContent': False,\n",
       " 'noTextContent': False,\n",
       " 'pageHeaderMarkdown': '\\n# Evaluation of Retrieval-Augmented Generation: A Survey\\n\\n',\n",
       " 'pageFooterMarkdown': '\\n',\n",
       " 'printedPageNumber': '',\n",
       " 'slideSpeakerNotes': None,\n",
       " 'slideSectionName': None,\n",
       " 'confidence': 0.972}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_obj[0]['pages'][0] # Page 1\n",
    "json_obj[0]['pages'][1] # Page 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42386bbe",
   "metadata": {},
   "source": [
    "Parse the invoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "279347f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/rag_research_paper/chunks_data/2405.07437v2_pages_3_4.pdf'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c318a3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 87d2ebc0-c116-413d-8776-fada9ffc10aa\n"
     ]
    }
   ],
   "source": [
    "pdf_dir = \"./data/rag_research_paper/chunks_data/Your_Order_Towards_AI_Academy.pdf\"\n",
    "json_obj = parser.get_json_result(pdf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a40a792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'text',\n",
       "  'md': 'Your Order - Towards AIAcademy',\n",
       "  'value': 'Your Order - Towards AIAcademy',\n",
       "  'bBox': {'x': 291,\n",
       "   'y': 15.25,\n",
       "   'w': 124.75,\n",
       "   'h': 7.99,\n",
       "   'confidence': 1,\n",
       "   'label': 'text'}},\n",
       " {'type': 'table',\n",
       "  'md': '| Date                    | Product                   | Type             | Price   | Subtotal | Tax   | Total     |\\n| ----------------------- | ------------------------- | ---------------- | ------- | -------- | ----- | --------- |\\n| 12/03/20                | Full Stack AI Engineering | One time payment | $349.00 | $174.50  | $0.00 | $174.50   |\\n| Coupon: louis\\\\_discount |                           |                  |         |          |       | - $174.50 |',\n",
       "  'rows': [['Date', 'Product', 'Type', 'Price', 'Subtotal', 'Tax', 'Total'],\n",
       "   ['12/03/20',\n",
       "    'Full Stack AI Engineering',\n",
       "    'One time payment',\n",
       "    '$349.00',\n",
       "    '$174.50',\n",
       "    '$0.00',\n",
       "    '$174.50'],\n",
       "   ['Coupon: louis_discount', '', '', '', '', '', '- $174.50']],\n",
       "  'isPerfectTable': True,\n",
       "  'csv': '\"Date\",\"Product\",\"Type\",\"Price\",\"Subtotal\",\"Tax\",\"Total\"\\n\"12/03/20\",\"Full Stack AI Engineering\",\"One time payment\",\"$349.00\",\"$174.50\",\"$0.00\",\"$174.50\"\\n\"Coupon: louis_discount\",\"\",\"\",\"\",\"\",\"\",\"- $174.50\"',\n",
       "  'bBox': {'x': 24,\n",
       "   'y': 15.25,\n",
       "   'w': 563.95,\n",
       "   'h': 759.49,\n",
       "   'confidence': 1,\n",
       "   'label': 'table'}},\n",
       " {'type': 'text',\n",
       "  'md': 'Total Amount: $174.50 USD',\n",
       "  'value': 'Total Amount: $174.50 USD',\n",
       "  'bBox': {'x': 48.49,\n",
       "   'y': 91.5,\n",
       "   'w': 516.01,\n",
       "   'h': 130.5,\n",
       "   'confidence': 1,\n",
       "   'label': 'text'}},\n",
       " {'type': 'text',\n",
       "  'md': 'Print Receipt | Buy Again',\n",
       "  'value': 'Print Receipt | Buy Again',\n",
       "  'bBox': {'x': 60,\n",
       "   'y': 271.5,\n",
       "   'w': 198.17,\n",
       "   'h': 12,\n",
       "   'confidence': 1,\n",
       "   'label': 'text'}}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_obj[0]['pages'][1]['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94d028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1548d4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  BERT HuggingFace Model Deployment using Kubernetes [ Github Repo]  03/07/2024\n",
      "Content:  Github Repo : https://github.com/vaibhawkhemka/ML-Umbrella/tree/main/MLops/Model_Deployment/Bert_Kubernetes_deployment   Model development is useless if you dont deploy it to production  which comes with a lot of issues of scalability and portability.   I have deployed a basic BERT model from the huggingface transformer on Kubernetes with the help of docker  which will give a feel of how to deploy and manage pods on production.   Model Serving and Deployment:ML Pipeline:Workflow:   Model server (using FastAPI  uvicorn) for BERT uncased model    Containerize model and inference scripts to create a docker image    Kubernetes deployment for these model servers (for scalability)  Testing   Components:Model serverUsed BERT uncased model from hugging face for prediction of next word [MASK]. Inference is done using transformer-cli which uses fastapi and uvicorn to serve the model endpoints   Server streaming:   Testing: (fastapi docs)   http://localhost:8888/docs/   { output: [ { score: 0.21721847355365753  token: 2204  token_str: good  sequence: today is a good day }  { score: 0.16623663902282715  token: 2047  token_str: new  sequence: today is a new day }  { score: 0.07342924177646637  token: 2307  token_str: great  sequence: today is a great day }  { score: 0.0656224861741066  token: 2502  token_str: big  sequence: today is a big day }  { score: 0.03518620505928993  token: 3376  token_str: beautiful  sequence: today is a beautiful day } ]   ContainerizationCreated a docker image from huggingface GPU base image and pushed to dockerhub after testing.   Testing on docker container:   You can directly pull the image vaibhaw06/bert-kubernetes:latest   K8s deploymentUsed minikube and kubectl commands to create a single pod container for serving the model by configuring deployment and service config   deployment.yaml   apiVersion: apps/v1 kind: Deployment metadata: name: bert-deployment labels: app: bertapp spec: replicas: 1 selector: matchLabels: app: bertapp template: metadata: labels: app: bertapp spec: containers: - name: bertapp image: vaibhaw06/bert-kubernetes ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: bert-service spec: type: NodePort selector: app: bertapp ports: - protocol: TCP port: 8080 targetPort: 8080 nodePort: 30100Setting up minikube and running pods using kubectl and deployment.yaml   minikube start kubectl apply -f deployment.yamlFinal Testing:kubectl get allIt took around 15 mins to pull and create container pods.   kubectl image listkubectl get svcminikube service bert-serviceAfter running the last command minikube service bert-service  you can verify the resulting deployment on the web endpoint.   Find the GitHub Link: https://github.com/vaibhawkhemka/ML-Umbrella/tree/main/MLops/Model_Deployment/Bert_Kubernetes_deployment   If you have any questions  ping me on my LinkedIn: https://www.linkedin.com/in/vaibhaw-khemka-a92156176/   Follow ML Umbrella for more such detailed  actionable projects.   Future Extension:Scaling with pod replicas and load balancer -   Self-healing\n",
      "URL:  https://towardsai.net/p/machine-learning/bert-huggingface-model-deployment-using-kubernetes-github-repo-03-07-2024\n",
      "Source:  tai_blog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dell\\.cache\\huggingface\\hub\\datasets--jaiganesan--ai_tutor_knowledge. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "\n",
    "file_path = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"ai_tutor_knowledge.jsonl\", repo_type=\"dataset\")\n",
    "\n",
    "# Exploring the dataset content\n",
    "with open(file_path, \"r\") as file:\n",
    "    ai_tutor_knowledge = [json.loads(line) for line in file]\n",
    "\n",
    "print(\"Title: \", ai_tutor_knowledge[0]['name'])\n",
    "print(\"Content: \", ai_tutor_knowledge[0]['content'])\n",
    "print(\"URL: \", ai_tutor_knowledge[0]['url'])\n",
    "print(\"Source: \", ai_tutor_knowledge[0]['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e02e95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 2027,\n",
       " 'doc_id': 'b0e7e0de-0f90-5814-b69a-827d25d3f094',\n",
       " 'name': 'Making Bayesian Optimization Algorithm Simple for Practical Applications',\n",
       " 'url': 'https://towardsai.net/p/machine-learning/making-bayesian-optimization-algorithm-simple-for-practical-applications',\n",
       " 'source': 'tai_blog',\n",
       " 'content': \"The Goal of this writing is to show an easy implementation of Bayesian Optimization to solve real-world problems.   Contrary to Machine Learning modeling which the goal is to find a mapping between input and output by utilizing a rather large set of data  in Optimization  defining the exact algorithm inside the Black-Box is not of interest  we do not have the luxury of applying many inputs  Maybe because of the constraints of the process that is too timely or too costly all we are looking for is to find one magical combination of input variables that produces the smallest output and be able to achieve that by examining only a limited number of input values applied to Black-Box.   This problem is prevalent in every discipline  regardless of where you work  you will face this problem where you want to optimize a metric in your process  whether is cost  resources  time to market  quality  reliability  etc.  and in all cases  you have few parameters or knobs you can turn in your process  and you want to find out that magical input values that give you the best-optimized output value with the smallest number of trials.   The situation becomes trickier if black-box output may have some local minimum and maybe one large global minimum  and how can we avoid being trapped in one of those local minimums and missing the largest global minimum?   In this paper  we show how the Bayesian Optimization algorithm  In conjunction with data coming from the field  can work together to discover the optimum point for the process.   You might be sitting at your computer and running a Bayesian Optimization Algorithm  while the physical Black-Box might be sitting in a Lab at some distance. You act as a middleman  talking to both sides. For the Algorithm  we use the SKOPT package of SciKit-learn. You can install this open-source package using the command:   pip install scikit-optimize   Sequential model-based optimization toolbox.pypi.org   The heart of the Algorithm is a Gaussian Process called gp_minimize; for simplicity  Lets call this magical function AI Genie  and You are acting in between this AI-Genie  which is running in your PC  and your physical Black box. The goal of the AI-Genie is to find the minimum output of the black box with as small a number of trials as possible. Also  to make it even simpler  assume that we have only one input in the black box; this process could easily be expanded to a multi-input case. The Picture below shows all the characters in this process:   Here is the actual code:   import numpy as np from skopt import gp_minimize from skopt.space import Real from skopt.utils import use_named_args import matplotlib.pyplot as plt # Define the search space (let's assume we're searching within -100 to 100) search_space = [Real(-100  100  name='X')] # Objective function that interacts with the user @use_named_args(search_space) def objective_function(X): # Print the value of X print(fEnter this value into the black box: X={X}  flush=True) # Ask the user to input the corresponding Y value from the black box Y = float(input(Enter the value returned by the black box (Y): )) # Return the Y value as the result of the objective function return Y # Perform Bayesian Optimization result = gp_minimize(objective_function  search_space  n_calls=15  random_state=0) # Print the result print(fOptimal value found: X = {result.x[0]}) print(fMinimum value of the function: Y = {result.fun}) # Plot the convergence plt.plot(result.func_vals) plt.xlabel('Number of calls') plt.ylabel('Function value') plt.title('Convergence Plot') plt.show()Lets examine the code in more detail:   1- Import required libraries   import numpy as np from skopt import gp_minimize from skopt.space import Real from skopt.utils import use_named_args import matplotlib.pyplot as pltgp_minimize is the main function driving the optimization process   for input parameters to the black box  you can have Integer  Real  and Category. Here  we assume we have just one Real value input   use_name_args is a decorator supplied in SKOPT; its job is to select different values of input parameters and send them to be processed in Black-Box   2- Define search space   # Define the search space (let's assume we're searching within -100 to 100) search_space = [Real(-100  100  name='X')]Offers the system a range of valid values that input can take. For example  here we have one input called X which can take a float value between -100 to 100   3- Black-Box Representation   # Objective function that interacts with the user @use_named_args(search_space) def objective_function(X): # Print the value of X print(fEnter this value into the black box: X={X}  flush=True) # Ask the user to input the corresponding Y value from the black box Y = float(input(Enter the value returned by the black box (Y): )) # Return the Y value as the result of the objective function return YObjective function is the Function representing the Black-Box functionality. The Black-Box is inside the Objective function and receives the input values given to the Objective function from Search Space; the black box accepts that value  it processes the input  and provides the output to objective function  which will then be returned to the optimizing algorithm.   What makes this paper different is that we are acting like a black box inside the Objective function; we get the parameter passed to it  by printing that input value.   Then we pause the program to take that input back to the lab and give it to the physical or virtual black box  get the output of the black box  and then come back to the objective function  which was holding the execution to receive the value we enter as the output coming from the black- box.   and finally  Return the value to the Optimizer and wait for the next input from optimizer.   4- Main Bayesian Optimizer function   # Perform Bayesian Optimization result = gp_minimize(objective_function  search_space  n_calls=15  random_state=0)This is the heart of the algorithm  which we call Ai-Genie; the First parameter for this function is the Objective-function (which holds the black Box inside)  the next parameter is Search_Space  the next parameter is n_calls which the user choose to limit the number of trials  here user is asking the Ai-Genie to provide the minimum value of the output of black box within 15 trials  and last parameter is random_state to initialize the random state.   5- Printing the results   # Print the result print(fOptimal value found: X = {result.x[0]}) print(fMinimum value of the function: Y = {result.fun})This will print the minimum value out of the black box (Y) and the input value (X)  which will get you the minimum output.   Execution   Assume you have set everything and are ready to run the experiment; you have no idea what is inside the black box. You just know for any input you give it  it provides you an out put so lets start the experiment:   1- The First number the optimizer model give you is: 18.568924; the optimizer picks this very first number at random form the range of available input variables.   2- Take this number to the black box  enter it  and wait for the output  The black box returns: 363.373849   3- Take this out  put back to Optimizer  and enter it  wait for Optimizer to provide you with the next number: 68.853150   4- You have finished one round; continue this process till you exhaust the number of trial n_call.   Here X is the number suggested by the Ai-Genie to try on Black Box  and   Y is the output from Black-Box   The final result is given below:   Optimal value found: X = -0.49669415594226507   The minimum value of the function: Y = -0.24998907139506593   Lets plot convergence   # Plot the convergence plt.plot(result.func_vals) plt.xlabel('Number of calls') plt.ylabel('Function value') plt.title('Convergence Plot') plt.show()Notice in a range of -100 to 100  there are an infinite number of float values that Ai-Genie could choose from  but Ai-Genie is so awesome that after testing a few values  it almost knows what the minimum value is after only 10 trials.   Verification   Now that the experiment is concluded  How do I know that the Ai-genie really found the optimum value  and how do I verify it.   In real-world situations  we absolutely do not know what is inside the black box  and we also do not want to know  we are interested just in minimum output  but here just to test the accuracy of the Ai-genie in finding the optimum value  I did not expose this to Ai-genie but I went to black box in the lab  and placed a function that I know inside of it  the function I placed there was :   Y = X**2 + X   We can find the minimum value of this function using Differential equation and set it qual to zero and solve it.   dY/dX = 2X + 1   2X +1 = 0   X = -0.5   Y = -0.25   The values the Bayesian Optimization found without knowing this equation were extremely close  which verifies the power of the algorithm.   This is what makes the Bayesian Optimization algorithm so powerful. We should seriously consider using it more often to find optimal points for any process wherever possible.\"}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ai_tutor_knowledge)\n",
    "ai_tutor_knowledge[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
