{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# pip install llama-index llama-index-embeddings-huggingface llama-index-llms-google-genai llama-index-llms-groq llama-index-vector-stores-chroma chromadb huggingface-hub"
      ],
      "metadata": {
        "id": "CP8UQROTxWo1"
      },
      "id": "CP8UQROTxWo1",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5c5c963b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476,
          "referenced_widgets": [
            "63b37b367a6444a8a3dda72d20a8dd8f",
            "e266f04dab724832af8376c9c3c46239",
            "e8caea8a342c4750bfee0fc12261191b",
            "09045bf02d104361a51b9f86ae3d7a8c",
            "7505028812f14fb68832912b48f811db",
            "c14c37ffad664ad1a9ba8a45f47725ce",
            "a04346f7ef7446b08727ec13454aa22a",
            "45d31cf4633c45bd8e28c39fd3ae4a9b",
            "1bd1694650654dc6b25b6f65352c40eb",
            "3be869dcbc7442699267b0b32c11a9b4",
            "a247073dcf7f46df95ed951a6b24a2f2",
            "b09991e2a380497ab401b5c820b7fa6a",
            "ffef672d80f940feb5ba6314cdfa6f2d",
            "100855218c154b2fa953fc2be4b77034",
            "fb6a06485e4e4efd8760a6251c9e264b",
            "d35bf76a4a6f40f4b39daae5ba4d6469",
            "ce1ea020073c4586bd9b75fc5f14943f",
            "0869e45ac4784ee585a8e0800cd47b9c",
            "e9a31c1ae9d84eb4b390517b1d2c75b8",
            "35bb258ca0d9455791f5c6a6395fd094",
            "95ce0df4e77e4b67a9f03f0dc4f21262",
            "67a831d434954c5bb81d804dd41bef93",
            "410a512142dc4a819ea2f2e284a5a860",
            "f648c32588e8490daba3bd90b3aeaf74",
            "7eceead8f49840f9a126a5456738972a",
            "bf500b55ffb0430c806012e567101305",
            "eddf209692284c5cb5776125236b1b08",
            "3ffeeeb2593c4f8eaad56973f7fedaf6",
            "bcc3bf34e9a54c229ea0ec2d8c21e9c9",
            "032bd5e527d444e29cc0f4e5759d4a1c",
            "06a521b937ea40cfa0696e107040fec9",
            "d52796c05b134c389394886f6dbb9737",
            "f21387dd45574c2893b924c1d618774c",
            "2e2e0eaed0aa408cb7fb9411a29fb881",
            "faeeb8b41a944e08a7d6aa6f391a15ee",
            "cbe1c9aa48ba4bfb95b291370ffd7b53",
            "779c530b2c6a4a719a5a1cba9dc710fc",
            "5fb565d1603949aab7376c2d04e828d5",
            "844a1b8d92764742a6c88750185a3ed0",
            "b055dfe041ec4a8fa25aeb012878a501",
            "a70c7570100d4d9d8f156945bc48807f",
            "c116a343988c47ae94a2b14fbd5f2700",
            "e73396e8b7454fcb9a81bfd5bfab5ffb",
            "2d7a5e2904a240e7bf0cbbefd59f0fb0",
            "95c69573bea34ce78381092144a4dbaa",
            "36cdf7a638174a259249094dfd49b9f6",
            "48c959a66a934d759ffeed98ba647dfe",
            "98e6d146b6724485b54aea15b2aaaac7",
            "e069769875194ceeb61569e4fc847a6c",
            "1bc120c66a71417f97b89e5cc3ca2370",
            "c93ef644a58d403eab0ceed21b752d03",
            "5d985e97ce7743a99e5143b8adbe981a",
            "62d24eebf060474d817332e76661bca9",
            "9926ecc1fe4a43db89a9c64bf3d07000",
            "5e5bfb70e16d4262ba206972c24472a8",
            "e58c88a70c7e4556a7f9ae89315fc3e9",
            "231388f5f4e946898b9bd44d435667e2",
            "8b755d22787a48c0a56bcb2199a16a01",
            "b6a71d3b744f414db640a205bc2046d3",
            "81be6ae07f3c4a6188c473207fc6ef17",
            "3d555092bf374bd39e8f5cbb533b55ac",
            "a990d167611d464b9cdab126d2542608",
            "b373c5e4a0b04bfa8db2a3e179b55289",
            "baec7b704b3448238dab1c3168c5d555",
            "e368c6db1d4c4963a22c6249e7588280",
            "0c12c88452a441049bc968c506cfa9dc",
            "4ca343c625bf4979b8deef05ef2c4e45",
            "3dd906c86f0f4dedb1992c566965ef27",
            "ce42b3545fcb4336b3ebf3e1421fc512",
            "378b6e6cf7b84351b6bea508f28007e6",
            "9c2e2797d5bb4b368f55428b6283b491",
            "5306c45517624d57adba617eadce0b3e",
            "1250b448bd494b15bcf3381db62e5291",
            "27d19f75c57d4800961024da0f708112",
            "663bb6598b1646cca1f1c0bcb33e083b",
            "f9c7b3c5101d411f8c5f91ac0acaaa7b",
            "2888863a19254d8f80232c8044ae1374",
            "20f7dbea018a4866b74bc3da34fa53c9",
            "cfa57ac8b553438988b75008c8d18b01",
            "e5abb1244a1a4e2f911677b7ab20cd34",
            "dd5e67cf4fbb4da8b9f60577cf3cbde7",
            "1678aab067cc43ac8c68bad1f422c758",
            "552e308b81ec48348193b0231998769e",
            "030d93aa9c0a45d19aa74016276aeac5",
            "8cbee76d20824d8c812149de4c93c3ff",
            "6568beabe11c428999cd9e52d172f7b9",
            "cafc57f9f2164fe99a9bb9ca5c9b1a7a",
            "71638c8310d649eb819bf610af1a4872",
            "ba370583d423493e96a17b348d46787e",
            "340383e8940c4840ad8a25140792a327",
            "1e88f0958393489bbc5b043d61b06c93",
            "4ca08dcc21f84d088b2def1ea5cd3d78",
            "cc42ec88c7134c8db3dd7fd995a7c3db",
            "4af9f80f58da4e3fad6398da6d6c30b4",
            "b73d5e3b429545188acea813ed4cbfe5",
            "783bcf99b0f3459993964065c62c3ba6",
            "f2793d1af1fa44e9a1ee787bb2872c97",
            "450d3379f17a43d98aef0fdc56ba7bdd",
            "0a6a26f6ffde40d9aabc985ec96d40a1",
            "2180c7c96d3f44cfbd423b9b432555b4",
            "2c43cc94974545c5856fa367b6db6681",
            "9853618bcbca44ff8f9e129835f380b0",
            "2e690af96402456b9c86f3a8f3a19bac",
            "0aef0c6acfb1455198927e573ad7504f",
            "37cf22ec15c445869f29bbaf8a88e033",
            "7377daea0aee47efbfd5db7ecabd1203",
            "d0fe87234da14c33bc0ba4973395e375",
            "31668d254d7c43bdb37d62d503974aa6",
            "a96270bb5962414e956897c78eaae81a",
            "03acfa0ba1b64db496bd84fdc272fef4"
          ]
        },
        "id": "5c5c963b",
        "outputId": "935db405-acfd-4310-9efc-467b3b8afbec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63b37b367a6444a8a3dda72d20a8dd8f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b09991e2a380497ab401b5c820b7fa6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "410a512142dc4a819ea2f2e284a5a860"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e2e0eaed0aa408cb7fb9411a29fb881"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95c69573bea34ce78381092144a4dbaa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e58c88a70c7e4556a7f9ae89315fc3e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ca343c625bf4979b8deef05ef2c4e45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20f7dbea018a4866b74bc3da34fa53c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba370583d423493e96a17b348d46787e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2180c7c96d3f44cfbd423b9b432555b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_info.input_token_limit=1048576\n",
            "model_info.output_token_limit=65536\n"
          ]
        }
      ],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.core import VectorStoreIndex\n",
        "import google.genai.types as types\n",
        "from google import genai\n",
        "import os\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "\n",
        "# model = \"gemini-2.5-flash-preview-09-2025\"\n",
        "model = \"gemini-2.5-flash\"\n",
        "# model = \"llama-3.1-8b-instant\"\n",
        "embedding_model = \"intfloat/e5-small-v2\"\n",
        "\n",
        "config = types.GenerateContentConfig(\n",
        "    thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "    temperature=0.4 # set this to make this less chatty and more deterministic to save the tokens\n",
        ")\n",
        "\n",
        "Settings.llm = GoogleGenAI(\n",
        "    model=model,\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    generation_config=config,\n",
        ")\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=embedding_model,\n",
        "    device=\"cuda\"                    # Use \"cuda\" if you have a GPU\n",
        ")\n",
        "Settings.text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=128)\n",
        "\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "model_info = client.models.get(model=model)\n",
        "print(f\"{model_info.input_token_limit=}\")\n",
        "print(f\"{model_info.output_token_limit=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14439321",
      "metadata": {
        "id": "14439321"
      },
      "source": [
        "Downloading vector store from Huggingface hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7bc76de1",
      "metadata": {
        "id": "7bc76de1"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import hf_hub_download\n",
        "# vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\",repo_type=\"dataset\",local_dir=\"/content\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cHUn7t7IJF03",
        "outputId": "ab4072a1-e299-4361-c191-ed11ef3b85a6"
      },
      "id": "cHUn7t7IJF03",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "25e92570",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25e92570",
        "outputId": "fd661a73-b2e3-45bf-bf35-57a2ebe20daa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ai_tutor_knowledge.zip\n",
            "   creating: content/ai_tutor_knowledge/\n",
            "   creating: content/ai_tutor_knowledge/3a3bc9f8-5760-45af-b368-453dd0ecec22/\n",
            "  inflating: content/ai_tutor_knowledge/3a3bc9f8-5760-45af-b368-453dd0ecec22/link_lists.bin  \n",
            "  inflating: content/ai_tutor_knowledge/3a3bc9f8-5760-45af-b368-453dd0ecec22/length.bin  \n",
            "  inflating: content/ai_tutor_knowledge/3a3bc9f8-5760-45af-b368-453dd0ecec22/data_level0.bin  \n",
            "  inflating: content/ai_tutor_knowledge/3a3bc9f8-5760-45af-b368-453dd0ecec22/index_metadata.pickle  \n",
            "  inflating: content/ai_tutor_knowledge/3a3bc9f8-5760-45af-b368-453dd0ecec22/header.bin  \n",
            "  inflating: content/ai_tutor_knowledge/chroma.sqlite3  \n"
          ]
        }
      ],
      "source": [
        "!unzip \"ai_tutor_knowledge.zip\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf \"ai_tutor_knowledge\""
      ],
      "metadata": {
        "id": "lEIBXYaeKKlC"
      },
      "id": "lEIBXYaeKKlC",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "90dcbdd6",
      "metadata": {
        "id": "90dcbdd6"
      },
      "outputs": [],
      "source": [
        "# Load the vector store from the local storage.\n",
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "# try:\n",
        "#     db.delete_collection(\"./mini-llama-articles/ai_tutor_knowledge\")\n",
        "#     print(\"Collection deleted. Starting fresh!\")\n",
        "# except:\n",
        "#     print(\"Collection didn't exist or was already deleted.\")\n",
        "\n",
        "db2 = chromadb.PersistentClient(path=\"/content/ai_tutor/ai_tutor_knowledge\")\n",
        "chroma_collection = db2.get_or_create_collection(\"ai_tutor_knowledge\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many chunks are in your database\n",
        "print(f\"Total items in collection: {chroma_collection.count()}\")\n",
        "\n",
        "# List existing collections to ensure the name matches perfectly\n",
        "print(f\"Existing collections: {db2.list_collections()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EviAfW3G2B-",
        "outputId": "395bcdf7-2ad8-497a-be9d-7cb71923047f"
      },
      "id": "3EviAfW3G2B-",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total items in collection: 2573\n",
            "Existing collections: [Collection(name=ai_tutor_knowledge)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "50f79f6a",
      "metadata": {
        "id": "50f79f6a"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "# Create the index based on the vector store.\n",
        "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e2b1b93b",
      "metadata": {
        "id": "e2b1b93b"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine(similarity_top_k=10)\n",
        "\n",
        "res = query_engine.query(\"Explain how Advance RAG works?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res.response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x5RBqPyRze5B",
        "outputId": "aab6365a-068f-437c-d5d5-82f772476eab"
      },
      "id": "x5RBqPyRze5B",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The provided context does not contain information on how Advanced RAG works, only that it was used for POC purposes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os._exit(0)"
      ],
      "metadata": {
        "id": "xy1KuqfUFpZ_"
      },
      "id": "xy1KuqfUFpZ_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RankGPT\n"
      ],
      "metadata": {
        "id": "V2W2TN_Bz3uy"
      },
      "id": "V2W2TN_Bz3uy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In LlamaIndex, **Node Postprocessors** are specialized modules that transform, filter, or re-rank a set of nodes after they have been retrieved from an index but **before** they are sent to the LLM for response generation.\n",
        "\n",
        "They are a critical part of an \"Advanced RAG\" workflow, ensuring that the context sent to the model is as relevant, high-quality, and cost-effective as possible.\n",
        "\n",
        "### **How They Work in the Pipeline**\n",
        "\n",
        "Postprocessors sit between the **Retriever** and the **Response Synthesizer**:\n",
        "\n",
        "1. **Retrieval**: The system finds the top  nodes (chunks) most similar to your query.\n",
        "2. **Postprocessing**: The postprocessors take those  nodes and apply rules to change them (e.g., delete irrelevant ones, reorder them, or add more text).\n",
        "3. **Synthesis**: The final refined list of nodes is sent to the LLM to write the answer.\n",
        "\n",
        "---\n",
        "\n",
        "### **The TimeWeightedPostprocessor**\n",
        "\n",
        "The **`TimeWeightedPostprocessor`** is a specific module used to handle time-sensitive data. It helps the system choose the most **recent** information when multiple versions of a document exist or when information naturally decays over time.\n",
        "\n",
        "#### **Key Features**\n",
        "\n",
        "* **Recency Ranking**: It re-ranks nodes based on a combination of their original similarity score and their \"age\" (stored in metadata like a timestamp).\n",
        "* **Time Decay**: It uses a `time_decay` factor to determine how quickly a document's relevance drops as it gets older.\n",
        "* **Use Case**: If you have three versions of a policy manual from 2022, 2023, and 2024, this postprocessor ensures the 2024 version is prioritized, even if the older versions have slightly higher keyword similarity.\n",
        "\n",
        "---\n",
        "\n",
        "### **Other Common Postprocessors**\n",
        "\n",
        "| Postprocessor | Purpose |\n",
        "| --- | --- |\n",
        "| **SimilarityPostprocessor** | Filters out any nodes that fall below a specific similarity score (e.g., discard anything with a score < 0.7). |\n",
        "| **KeywordNodePostprocessor** | Ensures retrieved nodes contain mandatory keywords or do not contain excluded \"negative\" keywords. |\n",
        "| **LLM Rerank** | Uses a second, more powerful LLM to double-check and re-sort the retrieved nodes to ensure they truly answer the question. |\n",
        "| **LongContextReorder** | Reorders nodes so the most important info is at the beginning or end of the context, preventing the LLM from getting \"lost in the middle\". |\n",
        "| **MetadataReplacement** | Swaps a small chunk of text (like a single sentence) with its larger surrounding context (the full paragraph) before sending it to the LLM. |\n",
        "\n",
        "> LongContextReorder is from the paper \"Lost In The Middle\"\n"
      ],
      "metadata": {
        "id": "OrwKeDnpKmsB"
      },
      "id": "OrwKeDnpKmsB"
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.postprocessor.rankGPT_rerank import RankGPTRerank\n",
        "\n",
        "rankgpt = RankGPTRerank(\n",
        "    top_n = 3,\n",
        "    llm = Settings.llm,\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "OD117nmrzzX5"
      },
      "id": "OD117nmrzzX5",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a query engine that is responsible for retrieving related pieces of text,\n",
        "# and using a LLM to formulate the final answer.\n",
        "# The `node_postprocessors` function will be applied to the retrieved nodes.\n",
        "query_engine = index.as_query_engine(similarity_top_k=10, node_postprocessors=[rankgpt])\n",
        "\n",
        "res = query_engine.query(\"Explain how Retrieval Augmented Generation (RAG) works?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RecwbNS6zzUd",
        "outputId": "ef19497c-39a5-4d80-ee72-76fdb54a926b"
      },
      "id": "RecwbNS6zzUd",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Reranking, new rank list for nodes: [4, 6, 1, 5, 0, 3, 9, 2, 7, 8]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res.response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "9n5IxdCJOgqS",
        "outputId": "69c96475-27d1-4de8-b663-f02253d402d8"
      },
      "id": "9n5IxdCJOgqS",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Retrieval Augmented Generation (RAG) models integrate pretrained dense retrieval (DPR) with sequence-to-sequence (seq2seq) models. These systems function by retrieving relevant documents and then passing them to a seq2seq model to generate outputs. The retriever and seq2seq components are initialized from pretrained models and are fine-tuned together, enabling both retrieval and generation processes to adapt to specific tasks.\\n\\nRAG addresses the issue of outdated knowledge in Large Language Models (LLMs) by connecting them to external, real-time data sources through retrieval mechanisms. This allows the LLM to combine its generative capabilities with the ability to search for and incorporate pertinent information from one or more knowledge bases.\\n\\nRAG systems can be classified based on several factors:\\n*   **Source of Information:** This can include traditional databases, vector databases, knowledge graphs, or the internet.\\n*   **Retrieval Mechanism:** Methods for collecting information vary and can include search engines, APIs, or customized database searches.\\n*   **Integration Method:** RAG processes can be integrated either before the information reaches the LLM for completion (where the RAG process enhances the user prompt) or after the prompt reaches the LLM (where the model acts as a reasoning engine to decide if and how to trigger RAG processes).\\n\\nTwo main formulations of RAG have been compared: one that conditions on the same retrieved passages for the entire generated sequence, and another that can use different passages for each token generated.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the retrieved nodes\n",
        "for src in res.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"Text\\t\", src.text.strip())\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj2qhFOSOjDk",
        "outputId": "9c8fb131-1368-422d-8774-2296684cb4d5"
      },
      "id": "Uj2qhFOSOjDk",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t 68528fe3-e101-4358-b3db-f0894346b348\n",
            "Title\t Fine-Tuning LLMs with Synthetic Data for High-Quality Content Generation\n",
            "Text\t during the training sessions  and this is usually true for the entire machine learning field. As the training process for LLMs is resource-intensive  costly  and time-consuming  it happens only at intervals of months (sometimes more)  and the model knowledge quickly becomes outdated. Frequent custom fine-tuning cycles are an option  but beyond being expensive  doing so indiscriminately can lead to a problem known as Catastrophic Forgetting (Catastrophic inferencing is also a common term for this phenomenon)  where the models forget previously learned knowledge. Plus  the models dont have access to real-time data. A more viable solution to deal with this is RAG.   RAG stands for Retrieval Augmented Generation  the name given to a family of processes that focuses on connecting the LLM to external sources through retrieval mechanisms. A combination of the generative capabilities of the model with the ability to search for and incorporate relevant information from one knowledge base (or several).   There are different ways of classifying such systems  but most of them vary based on a few factors:   Source of Information: Those sources can be literally anything from traditional databases  vector databases  knowledge graphs  to the internet itself.Retrieval Mechanism: As the sources are so varied  the same is true for the methods used to collect information  such as search engines  APIs  customized database searches  etc.Integration Method: It is also common to classify RAG systems based on how they are incorporated with the LLM to generate the completion process.I will only focus on explaining the difference in the integration logic in this article  as it was the only noticeable change I made regarding the original prototype.   The RAG mechanism can be integrated as soon as the user prompts the input BEFORE the information reaches the LLM for completion. In this case  the RAG process happens every time a new input prompt is entered by the user  and the results of this process are used to enhance the user prompt by the time it hits the model.   Or the RAG process can occur AFTER the prompt reaches the LLM. In this scenario  the model is used as a reasoning engine to decide whether it needs to trigger RAG processes or not (and what mechanisms to use) to generate the appropriate completion based on the perceivable context. This process is usually known as Agentic\n",
            "Score\t 0.8027662815663229\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 5f8d4c91-bd53-4a7a-b1f7-d8f74debb425\n",
            "Title\t RAG\n",
            "Text\t # RAG<div class=\"flex flex-wrap space-x-1\"><a href=\"https://huggingface.co/models?filter=rag\"><img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-rag-blueviolet\"></a></div>## OverviewRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) andsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generateoutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowingboth retrieval and generation to adapt to downstream tasks.It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, VladimirKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.The abstract from the paper is the following:*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achievestate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and preciselymanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behindtask-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledgeremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametricmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive\n",
            "Score\t 0.8017680751522839\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 5cbb84b5-c5e5-4e94-a189-43fa6bc9b138\n",
            "Title\t RAG\n",
            "Text\t extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generationtasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-artparametric-only seq2seq baseline.*This model was contributed by [ola13](https://huggingface.co/ola13).## Usage tipsRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks.## RagConfig[[autodoc]] RagConfig## RagTokenizer[[autodoc]] RagTokenizer## Rag specific outputs[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput## RagRetriever[[autodoc]] RagRetriever<frameworkcontent><pt>## RagModel[[autodoc]] RagModel    - forward## RagSequenceForGeneration[[autodoc]] RagSequenceForGeneration    - forward    - generate## RagTokenForGeneration[[autodoc]] RagTokenForGeneration    - forward    - generate</pt><tf>## TFRagModel[[autodoc]] TFRagModel    - call## TFRagSequenceForGeneration[[autodoc]] TFRagSequenceForGeneration    - call\n",
            "Score\t 0.8081449020602206\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I will use the **LongContextReorder** to arrage the responses in the start and the end to gives the best related response and LLM doesn't lost in the middle. This doesn't use LLM behind"
      ],
      "metadata": {
        "id": "dxfOGEbMOLos"
      },
      "id": "dxfOGEbMOLos"
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.postprocessor import LongContextReorder\n",
        "\n",
        "reorder = LongContextReorder()\n",
        "query_engine = index.as_query_engine(similarity_top_k=10, node_postprocessors=[reorder])\n",
        "\n",
        "res = query_engine.query(\"Explain how Retrieval Augmented Generation (RAG) works?\")"
      ],
      "metadata": {
        "id": "BsrJgWssMxJE"
      },
      "id": "BsrJgWssMxJE",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res.response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "vM1zmTeK2LnJ",
        "outputId": "ab283441-b169-4ebf-9a44-d07f51a78f9d"
      },
      "id": "vM1zmTeK2LnJ",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Retrieval Augmented Generation (RAG) models combine a pre-trained dense retrieval (DPR) system with sequence-to-sequence (seq2seq) models. These models function by retrieving relevant documents, passing them to a seq2seq model, and then marginalizing to generate outputs. Both the retriever and seq2seq modules are initialized from pre-trained models and are jointly fine-tuned to adapt to specific downstream tasks.\\n\\nThere are two main formulations for RAG:\\n1.  One approach conditions on the same retrieved passages for the entire generated sequence.\\n2.  Another approach allows for the use of different retrieved passages for each token generated.\\n\\nRAG addresses the limitations of outdated knowledge in large language models (LLMs) by connecting them to external, real-time data sources through retrieval mechanisms. This allows the LLM to combine its generative capabilities with the ability to search for and incorporate relevant information from various knowledge bases. RAG systems can be classified based on their information source (e.g., databases, the internet), retrieval mechanism (e.g., search engines, APIs), and how they integrate with the LLM (either before the prompt reaches the LLM or after, where the LLM acts as a reasoning engine to decide when to trigger retrieval).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the retrieved nodes\n",
        "for src in res.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"Text\\t\", src.text.strip())\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dbbW0xRLrFB",
        "outputId": "b32b68e0-dba9-43fe-d85b-d66731b89ad9"
      },
      "id": "1dbbW0xRLrFB",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t 5cbb84b5-c5e5-4e94-a189-43fa6bc9b138\n",
            "Title\t RAG\n",
            "Text\t extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generationtasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-artparametric-only seq2seq baseline.*This model was contributed by [ola13](https://huggingface.co/ola13).## Usage tipsRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks.## RagConfig[[autodoc]] RagConfig## RagTokenizer[[autodoc]] RagTokenizer## Rag specific outputs[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput## RagRetriever[[autodoc]] RagRetriever<frameworkcontent><pt>## RagModel[[autodoc]] RagModel    - forward## RagSequenceForGeneration[[autodoc]] RagSequenceForGeneration    - forward    - generate## RagTokenForGeneration[[autodoc]] RagTokenForGeneration    - forward    - generate</pt><tf>## TFRagModel[[autodoc]] TFRagModel    - call## TFRagSequenceForGeneration[[autodoc]] TFRagSequenceForGeneration    - call\n",
            "Score\t 0.8081449020602206\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t ee44937c-a668-4bd5-b09c-2a2487ea432c\n",
            "Title\t RAG Architecture: Advanced RAG\n",
            "Text\t that automates this process. Essentially  this is another graph  implementing a state machine (surprise U+1F642)  which looks something like this:   To implement it  its easiest to use LangGraph  which will be discussed further.   Self-RAGSelf-reflective RAG is based on research claiming that this approach provides better results than regular RAG. Overall  the idea is very similar to the previous one (CRAG) but goes further. The idea is to fine-tune the LLM to generate self-reflection tokens in addition to the regular ones. This is very convenient  as there is no need to guess how confident the LLM is and what to do with it. The following tokens are generated:   Retrieve token determines whether D chunks need to be retrieved for a given prompt x. Options: Yes  No  ContinueISREL token determines whether chunk d from D is relevant to the given prompt x. Options: relevant and irrelevantISSUP token determines whether the LLMs response y to chunk d is supported by chunk d. Options: fully supported  partially supported  no supportISUSE token determines whether the LLMs response to each chunk d is a useful answer to the query x. Options represent a usefulness scale from 5 to 1.Using these tokens  a state machine can be built  using the aforementioned LangGraph  which looks something like this:   For more details  see here.   HyDeAnother method  similar to RAG Fusion  is that it modifies the usual RAG retrieval process. HyDe stands for Hypothetical Document Embeddings and is based on the study Precise Zero-Shot Dense Retrieval without Relevance Labels. The idea is very simple  instead of using the users question for searching in the vector database  we use the LLM to generate a response (a virtual hypothetical document) and then use the response for searching in the vector database (to find similar answers).   Why all this? Sometimes users questions are too abstract and require more context  which the LLM can provide  and without which the search in the database makes no sense.   I think this is not an exhaustive review of the new changes; if I forgot something  write in the comments.\n",
            "Score\t 0.8031992247354013\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t dec5d5d3-d325-4ee3-b7bc-bd41887d3e10\n",
            "Title\t Building an LLM application\n",
            "Text\t # Building an LLM applicationWelcome to the beginning of Understanding LlamaIndex. This is a series of short, bite-sized tutorials on every stage of building an LLM application to get you acquainted with how to use LlamaIndex before diving into more advanced and subtle strategies. If you're an experienced programmer new to LlamaIndex, this is the place to start.## Key steps in building an LLM application!!! tip    If you've already read our [high-level concepts](../getting_started/concepts.md) page you'll recognize several of these steps.This tutorial has two main parts: **Building a RAG pipeline** and **Building an agent**, with some smaller sections before and after. Here's what to expect:- **[Using LLMs](./using_llms/using_llms.md)**: hit the ground running by getting started working with LLMs. We'll show you how to use any of our [dozens of supported LLMs](../module_guides/models/llms/modules/), whether via remote API calls or running locally on your machine.- **Building a RAG pipeline**: Retrieval-Augmented Generation (RAG) is a key technique for getting your data into an LLM, and a component of more sophisticated agentic systems. We'll show you how to build a full-featured RAG pipeline that can answer questions about your data. This includes:    - **[Loading & Ingestion](./loading/loading.md)**: Getting your data from wherever it lives, whether that's unstructured text, PDFs, databases, or APIs to other applications. LlamaIndex has hundreds of connectors to every data source over at [LlamaHub](https://llamahub.ai/).    - **[Indexing and Embedding](./indexing/indexing.md)**: Once you've got your data there are an infinite number of ways to structure access to that data to ensure your applications is always working with the most relevant data. LlamaIndex has a huge number of these strategies built-in and can help you select the best ones.    - **[Storing](./storing/storing.md)**: You will probably find it more efficient to store your data in indexed form, or pre-processed summaries provided by an LLM, often in a specialized database known as a `Vector Store` (see below). You can also store your\n",
            "Score\t 0.8021715285904161\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 1910ac9c-2d2a-4894-b1e8-4e82dd435b03\n",
            "Title\t GraphRAG Analysis  Part 2: Graph Creation and Retrieval vs Vector Database Retrieval\n",
            "Text\t item.page_content if isinstance(item  Document) else item text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000  chunk_overlap=200) all_splits = text_splitter.split_text(' '.join(get_text(doc) for doc in texts)) ground_truth = [] question_prompt = ChatPromptTemplate.from_template( Given the following text  generate {num_questions} diverse and specific questions that can be answered based on the information in the text.  Provide the questions as a numbered list.\\n\\nText: {text}\\n\\nQuestions: ) all_questions = [] for split in tqdm(all_splits  desc=Generating questions): response = llm_ground_truth(question_prompt.format_messages(num_questions=3  text=split)) questions = response.content.strip().split('\\n') all_questions.extend([q.split('. '  1)[1] if '. ' in q else q for q in questions]) random.shuffle(all_questions) selected_questions = all_questions[:num_questions] llm = ChatOpenAI(model_name=gpt-3.5-turbo  temperature=0) for question in tqdm(selected_questions  desc=Generating ground truth): answer_prompt = ChatPromptTemplate.from_template( Given the following question  provide a concise and accurate answer based on the information available.  If the answer is not directly available  respond with 'Information not available in the given context.'\\n\\nQuestion: {question}\\n\\nAnswer: ) answer_response = llm(answer_prompt.format_messages(question=question)) answer = answer_response.content.strip() context_prompt = ChatPromptTemplate.from_template( Given the following question and answer  provide a brief  relevant context that supports this answer.  If no relevant context is available  respond with 'No relevant context available.'\\n\\n Question: {question}\\nAnswer: {answer}\\n\\nRelevant context: ) context_response = llm(context_prompt.format_messages(question=question  answer=answer)) context = context_response.content.strip() ground_truth.append({ question: question  answer: answer  context: context  }) return ground_truth async def evaluate_rag_async(rag_chain  ground_truth  name): # ... (evaluation function implementation) async def run_evaluations(rag_chains  ground_truth): results = {} for name  chain in rag_chains.items(): result = await evaluate_rag_async(chain  ground_truth\n",
            "Score\t 0.7996934672023613\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t abc1339f-3efb-41a4-9b8f-1a7834491431\n",
            "Title\t GraphRAG Analysis  Part 2: Graph Creation and Retrieval vs Vector Database Retrieval\n",
            "Text\t {record['name']}\\nRelated: {'  '.join(record['related'])} documents.append(Document(page_content=content)) return documentsThe FAISS retriever uses vector similarity to find relevant information  while the Neo4j retrievers leverage the graph structure to find related entities and their relationships.   Creating RAG ChainsNow  lets create our RAG chains:   def create_rag_chain(retriever): llm = ChatOpenAI(model_name=gpt-3.5-turbo) template = Answer the question based on the following context: {context} Question: {question} Answer: prompt = PromptTemplate.from_template(template) if callable(retriever): # For Cypher retriever retriever_func = lambda q: retriever(q) else: # For FAISS retriever retriever_func = retriever return ( {context: retriever_func  question: RunnablePassthrough()} U+007C prompt U+007C llm U+007C StrOutputParser() ) # Create RAG chains faiss_rag_chain = create_rag_chain(faiss_retriever) cypher_rag_chain = create_rag_chain(cypher_retriever)These chains associate the retrievers with a language model to generate answers based on the retrieved context.   Evaluation SetupTo evaluate our RAG systems  well create a ground truth dataset and use the RAGAS framework:   def create_ground_truth(texts: List[Union[str  Document]]  num_questions: int = 100) -> List[Dict]: llm_ground_truth = ChatOpenAI(model_name=gpt-3.5-turbo  temperature=0.2) def get_text(item): return item.page_content if isinstance(item  Document) else item text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000  chunk_overlap=200) all_splits = text_splitter.split_text(' '.join(get_text(doc) for doc in texts)) ground_truth = [] question_prompt = ChatPromptTemplate.from_template( Given the following text  generate {num_questions} diverse and specific questions that can be answered based on the information in the text.  Provide the questions as a numbered list.\\n\\nText: {text}\\n\\nQuestions: ) all_questions = [] for split in tqdm(all_splits  desc=Generating questions): response\n",
            "Score\t 0.7937841353670847\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t f5ddd7e4-4e0e-4b1c-8528-964cbb2929e5\n",
            "Title\t RAG\n",
            "Text\t RagRetriever<frameworkcontent><pt>## RagModel[[autodoc]] RagModel    - forward## RagSequenceForGeneration[[autodoc]] RagSequenceForGeneration    - forward    - generate## RagTokenForGeneration[[autodoc]] RagTokenForGeneration    - forward    - generate</pt><tf>## TFRagModel[[autodoc]] TFRagModel    - call## TFRagSequenceForGeneration[[autodoc]] TFRagSequenceForGeneration    - call    - generate## TFRagTokenForGeneration[[autodoc]] TFRagTokenForGeneration    - call    - generate</tf></frameworkcontent>\n",
            "Score\t 0.7938195501033072\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 5f8d4c91-bd53-4a7a-b1f7-d8f74debb425\n",
            "Title\t RAG\n",
            "Text\t # RAG<div class=\"flex flex-wrap space-x-1\"><a href=\"https://huggingface.co/models?filter=rag\"><img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-rag-blueviolet\"></a></div>## OverviewRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) andsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generateoutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowingboth retrieval and generation to adapt to downstream tasks.It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, VladimirKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.The abstract from the paper is the following:*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achievestate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and preciselymanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behindtask-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledgeremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametricmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive\n",
            "Score\t 0.8017680751522839\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 68528fe3-e101-4358-b3db-f0894346b348\n",
            "Title\t Fine-Tuning LLMs with Synthetic Data for High-Quality Content Generation\n",
            "Text\t during the training sessions  and this is usually true for the entire machine learning field. As the training process for LLMs is resource-intensive  costly  and time-consuming  it happens only at intervals of months (sometimes more)  and the model knowledge quickly becomes outdated. Frequent custom fine-tuning cycles are an option  but beyond being expensive  doing so indiscriminately can lead to a problem known as Catastrophic Forgetting (Catastrophic inferencing is also a common term for this phenomenon)  where the models forget previously learned knowledge. Plus  the models dont have access to real-time data. A more viable solution to deal with this is RAG.   RAG stands for Retrieval Augmented Generation  the name given to a family of processes that focuses on connecting the LLM to external sources through retrieval mechanisms. A combination of the generative capabilities of the model with the ability to search for and incorporate relevant information from one knowledge base (or several).   There are different ways of classifying such systems  but most of them vary based on a few factors:   Source of Information: Those sources can be literally anything from traditional databases  vector databases  knowledge graphs  to the internet itself.Retrieval Mechanism: As the sources are so varied  the same is true for the methods used to collect information  such as search engines  APIs  customized database searches  etc.Integration Method: It is also common to classify RAG systems based on how they are incorporated with the LLM to generate the completion process.I will only focus on explaining the difference in the integration logic in this article  as it was the only noticeable change I made regarding the original prototype.   The RAG mechanism can be integrated as soon as the user prompts the input BEFORE the information reaches the LLM for completion. In this case  the RAG process happens every time a new input prompt is entered by the user  and the results of this process are used to enhance the user prompt by the time it hits the model.   Or the RAG process can occur AFTER the prompt reaches the LLM. In this scenario  the model is used as a reasoning engine to decide whether it needs to trigger RAG processes or not (and what mechanisms to use) to generate the appropriate completion based on the perceivable context. This process is usually known as Agentic\n",
            "Score\t 0.8027662815663229\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 63abb781-a6ca-4249-8e34-7e936f2f68a6\n",
            "Title\t GraphRAG Analysis  Part 2: Graph Creation and Retrieval vs Vector Database Retrieval\n",
            "Text\t context available.'\\n\\n Question: {question}\\nAnswer: {answer}\\n\\nRelevant context: ) context_response = llm(context_prompt.format_messages(question=question  answer=answer)) context = context_response.content.strip() ground_truth.append({ question: question  answer: answer  context: context  }) return ground_truth async def evaluate_rag_async(rag_chain  ground_truth  name): # ... (evaluation function implementation) async def run_evaluations(rag_chains  ground_truth): results = {} for name  chain in rag_chains.items(): result = await evaluate_rag_async(chain  ground_truth  name) results.update(result) return results # Main execution function async def main(): # Ensure vector index ensure_vector_index(recreate=True) # Create retrievers neo4j_retriever = create_neo4j_retriever() # Create RAG chains faiss_rag_chain = create_rag_chain(faiss_retriever) neo4j_rag_chain = create_rag_chain(neo4j_retriever) # Generate ground truth ground_truth = create_ground_truth(texts) # Run evaluations rag_chains = { FAISS: faiss_rag_chain  Neo4j: neo4j_rag_chain } results = await run_evaluations(rag_chains  ground_truth) return results # Run the main function if __name__ == __main__: nest_asyncio.apply() try: results = asyncio.run(asyncio.wait_for(main()  timeout=7200)) # 2 hour timeout plot_results(results) # Print detailed results for name  result in results.items(): print(fResults for {name}:) print(result) print() except asyncio.TimeoutError: print(Evaluation timed out after 2 hours.) finally: # Close the Neo4j driver driver.close()This setup creates a ground truth dataset  evaluates our RAG chains using RAGAS metrics  and visualizes the results.   Results and AnalysisThis analysis revealed a surprising similarity in performance between GraphRAG and vector-based RAG across most metrics  with one difference:   Faithfulness: Neo4j GraphRAG significantly outperformed FAISS (0.54 vs 0.18)  but did not outperform significantly in any other metrics.   The graph-based approach excels in faithfulness likely because it\n",
            "Score\t 0.805026283887231\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 25b89322-bdda-4348-a4b0-3245ee8d524b\n",
            "Title\t Fine-Tuning LLMs with Synthetic Data for High-Quality Content Generation\n",
            "Text\t as soon as the user prompts the input BEFORE the information reaches the LLM for completion. In this case  the RAG process happens every time a new input prompt is entered by the user  and the results of this process are used to enhance the user prompt by the time it hits the model.   Or the RAG process can occur AFTER the prompt reaches the LLM. In this scenario  the model is used as a reasoning engine to decide whether it needs to trigger RAG processes or not (and what mechanisms to use) to generate the appropriate completion based on the perceivable context. This process is usually known as Agentic RAG. In this scenario  the retrieval process doesnt happen all the time  like with the other integration approach.   As a last note  it is also common to classify the RAG process based on its internal logic and complexity. Following this approach  we typically divide it into naive RAG  advanced (complex) RAG  Modular RAG  hybrid RAG  etc. Since this is a diverse and complex area with reliable sources  Ill just mention that we used Advanced RAG for POC purposes because their previous prototype did so. If you are interested in learning more about different RAG mechanisms  I do recommend Vipra Sings article on Advanced RAGs.   The main change I made to the POCs RAG process was related to how it is triggered: I used the agentic RAG approach and made all the changes and enhancements to the existing complex RAG mechanisms to accommodate that. Additionally  I will fine-tune the model to determine which specific RAG strategy is more effective in improving its completion.   Choosing the Right FormatBacking again to the POC  the first step was to decide the best file format for the documents and how exactly the training set was going to be built.   All the available files have PDF and docx formats. None of them seemed to be suitable formats. because they have too much-unneeded data related to text styling and fonts  etc.  and we only needed the semantic content and some level of textual structure.   Considering the requirements  the markdown format (also known as MD) appeared to be a more viable option because it preserves structure (tables  headings  lists) and also some level of semantics (bold  italics  code blocks) and\n",
            "Score\t 0.8100621509005572\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Postprocessor\n"
      ],
      "metadata": {
        "id": "_zrckEFLdojs"
      },
      "id": "_zrckEFLdojs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. The `judger` function (The \"Brain\")**\n",
        "\n",
        "This function is a standalone utility that uses an LLM to evaluate text relevancy.\n",
        "\n",
        "* **Structured Output (Pydantic):** It defines an `OrderedNodes` class. By passing this to `llm.structured_predict`, it forces the LLM to return a strictly formatted JSON object instead of a chatty response. This makes the scores easy to extract in code.\n",
        "* **XML Tagging (Context Locking):** Wrapping data in `<NODE>` and `<QUERY>` tags prevents the LLM from getting confused between the user's question and the content of the articles.\n",
        "* **Listwise Judging:** Unlike some rerankers that look at one node at a time, this function passes **all nodes** to the LLM at once. This allows the LLM to compare them against each other to decide which is truly the \"best\".\n",
        "* **Proximity Scoring:** It asks for a decimal between 0 and 1. This \"normalized\" score allows for easy mathematical sorting in the next step.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. `GerminiAsJudgePostprocessor` (The \"Pipeline Bridge\")**\n",
        "\n",
        "This class is the \"glue\" that allows LlamaIndex to use your `judger` automatically during a query.\n",
        "\n",
        "* **Inheritance:** By inheriting from `BaseNodePostprocessor`, this class gains the ability to \"plug into\" any LlamaIndex `QueryEngine`.\n",
        "**The Internal Hook (`_postprocess_nodes`):**\n",
        "* Link: https://developers.llamaindex.ai/python/framework-api-reference/postprocessor/\n",
        "* **Inputs:** It receives `nodes` (the raw results from the vector database) and `query_bundle` (the user's question).\n",
        "* **Integration:** it calls your `judger` function and retrieves the list of scores.\n",
        "\n",
        "\n",
        "**The Sorting & Truncation Logic:**\n",
        "1. **Sorting:** It sorts the nodes from highest score (1.0) to lowest (0.0).\n",
        "2. **Selection:** It uses `min(3, len(sorted_scores))` as a safety check to select **exactly the top 3 nodes**.\n",
        "3. **Filtering:** It returns only those 3 nodes to the LLM.\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "yEhYxbehdt7z"
      },
      "id": "yEhYxbehdt7z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ac26b38"
      },
      "source": [
        "from pydantic import BaseModel\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "\n",
        "def llmJudge( nodes, query ):\n",
        "\n",
        "  class OrderedNodes(BaseModel):\n",
        "    \"\"\"A node with the id and assigned score.\"\"\"\n",
        "\n",
        "    node_id: list\n",
        "    score: list\n",
        "\n",
        "# Prepare the nodes and wrap them in <NODE></NODE> identifier, as well as the query\n",
        "# Now we are emumerating node (TextNode) in NodeWithScore, that contains node_id, text and metadata\n",
        "  total_nodes = \"\"\n",
        "  for idx, item in enumerate(nodes):\n",
        "    # We could use item.text as well instead of get_text()\n",
        "    total_nodes += f\"\"\"\n",
        "    <NODE{idx+1}>\\n\n",
        "      Node ID: {item.node_id}\\n\n",
        "      Text {item.get_text()}\\n\n",
        "    </NODE{idx+1}>\\n\n",
        "    \"\"\"\n",
        "\n",
        "  query = \"<QUERY>\\n{}\\n</QUERY>\".format(query)\n",
        "\n",
        "  # Define the prompt template\n",
        "  prompt_tmpl = PromptTemplate(\n",
        "      \"\"\"\n",
        "  You receive a qurey along with a list of nodes' text and their ids. Your task is to assign score\n",
        "  to each node based on its contextually closeness to the given query. The final output is each\n",
        "  node id along with its proximity score.\n",
        "  Here is the list of nodes:\n",
        "  {nodes_list}\n",
        "\n",
        "  And the following is the query:\n",
        "  {user_query}\n",
        "\n",
        "  Score each of the nodes based on their text and their relevancy to the provided query.\n",
        "  The score must be a decimal number between 0 an 1 so we can rank them.\"\"\"\n",
        "  )\n",
        "  ordered_nodes = Settings.llm.structured_predict(\n",
        "    OrderedNodes,\n",
        "    prompt_tmpl,\n",
        "    nodes_list=total_nodes,\n",
        "    user_query=query\n",
        "  )\n",
        "\n",
        "  return ordered_nodes\n"
      ],
      "id": "9ac26b38",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Postprocessor\n",
        "\n",
        "The following class will use the `judger` function to rank the nodes, and filter them based on the ranks.\n"
      ],
      "metadata": {
        "id": "Vh0VFr1yiJsi"
      },
      "id": "Vh0VFr1yiJsi"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "from llama_index.core import QueryBundle\n",
        "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
        "from llama_index.core.schema import NodeWithScore\n",
        "\n",
        "\n",
        "class GerminiJudgePostProcessor(BaseNodePostprocessor):\n",
        "  def _postprocess_nodes(self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle] = None) -> List[NodeWithScore]:\n",
        "\n",
        "    # query_bundle: This dataclass contains the original query string and associated transformations.\n",
        "    # nodes:\n",
        "    obj = llmJudge( nodes, query_bundle )\n",
        "    node_ids = obj.node_id\n",
        "    scores = obj.score\n",
        "\n",
        "    print(\"Node IDs:\", node_ids)\n",
        "    print(\"Scores:\", scores)\n",
        "\n",
        "    # sort the nodes and extracted the top 3\n",
        "    sorted_scores = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
        "    num_nodes_to_select = min(3, len(sorted_scores)) # This line prevents your code from crashing if the retriever finds fewer than 3 nodes.\n",
        "    top_nodes = [sorted_scores[i][0] for i in range(num_nodes_to_select)]\n",
        "\n",
        "    selected_nodes_id = [node_ids[item] for item in top_nodes]\n",
        "\n",
        "    final_nodes = []\n",
        "    for item in nodes:\n",
        "        if item.node_id in selected_nodes_id:\n",
        "            final_nodes.append(item)\n",
        "\n",
        "    return final_nodes\n"
      ],
      "metadata": {
        "id": "Hn3nBFQTdxKg"
      },
      "id": "Hn3nBFQTdxKg",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judge = GerminiJudgePostProcessor()"
      ],
      "metadata": {
        "id": "qKHC1RSndxHq"
      },
      "id": "qKHC1RSndxHq",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a query engine that is responsible for retrieving related pieces of text,\n",
        "# and using a LLM to formulate the final answer.\n",
        "# The `node_postprocessors` function will be applied to the retrieved nodes.\n",
        "query_engine = index.as_query_engine(similarity_top_k=10, node_postprocessors=[judge])\n",
        "\n",
        "res = query_engine.query(\"Explain how Retrieval Augmented Generation (RAG) works?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0fxctRZjesX",
        "outputId": "a195adcd-3c74-4a84-ad7e-f7feb8d15b31"
      },
      "id": "i0fxctRZjesX",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nodes [NodeWithScore(node=TextNode(id_='25b89322-bdda-4348-a4b0-3245ee8d524b', embedding=None, metadata={'url': 'https://towardsai.net/p/machine-learning/fine-tuning-llms-with-synthetic-data-for-high-quality-content-generation', 'title': 'Fine-Tuning LLMs with Synthetic Data for High-Quality Content Generation', 'tokens': 7359, 'source': 'tai_blog', 'questions_this_excerpt_can_answer': '1. What are two approaches for integrating RAG with LLMs based on when the RAG process is triggered?\\n2. What file format was chosen for the documents in the POC and why?', 'prev_section_summary': \"1. **Catastrophic Forgetting:** This occurs when LLMs, during frequent custom fine-tuning, forget previously learned knowledge.\\n2. **RAG (Retrieval Augmented Generation):** RAG addresses outdated LLM knowledge by connecting the LLM to external, real-time data sources via retrieval mechanisms. It combines the LLM's generative capabilities with the ability to search and incorporate relevant information from knowledge bases. RAG systems vary by information source (databases, internet), retrieval mechanism (search engines, APIs), and integration method (before or after the prompt reaches the LLM).\", 'section_summary': 'There are two approaches for integrating RAG with LLMs based on when the RAG process is triggered:\\n\\n1.  **Before the LLM:** The RAG process occurs every time a new user input prompt is entered, enhancing the prompt before it reaches the LLM.\\n2.  **After the LLM (Agentic RAG):** The LLM acts as a reasoning engine to decide if and when to trigger RAG processes to generate an appropriate completion.\\n\\nFor the POC, the markdown (MD) format was chosen for documents because it preserves textual structure and some semantic content without including unnecessary styling data found in PDF and docx formats.', 'excerpt_keywords': 'LLM, RAG, Agentic RAG, Markdown, Fine-tuning'}, excluded_embed_metadata_keys=['url', 'tokens', 'source'], excluded_llm_metadata_keys=['title', 'tokens', 'source'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='903c2f00-5fc8-533f-ad54-8057e894bd38', node_type='4', metadata={'url': 'https://towardsai.net/p/machine-learning/fine-tuning-llms-with-synthetic-data-for-high-quality-content-generation', 'title': 'Fine-Tuning LLMs with Synthetic Data for High-Quality Content Generation', 'tokens': 7359, 'source': 'tai_blog'}, hash='885a37a88b7a58b5b14bf9933189f9e97cfd9d6e3c0dc7aa53920d44bf008d5e'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='68528fe3-e101-4358-b3db-f0894346b348', node_type='1', metadata={'url': 'https://towardsai.net/p/machine-learning/fine-tuning-llms-with-synthetic-data-for-high-quality-content-generation', 'title': 'Fine-Tuning LLMs with Synthetic Data for High-Quality Content Generation', 'tokens': 7359, 'source': 'tai_blog'}, hash='7ef52b163c3b1d3ef4cdb967a36fd3b3b3c5ebca48f621acaa3bc95aaa2bf5b7'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='8f42f310-ca26-44fb-8953-bdb014254d42', node_type='1', metadata={}, hash='0dfe330e280c42d72a404c3f2616b20094f63227043111f0ee41c16fe7599bf2')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='as soon as the user prompts the input BEFORE the information reaches the LLM for completion. In this case  the RAG process happens every time a new input prompt is entered by the user  and the results of this process are used to enhance the user prompt by the time it hits the model.   Or the RAG process can occur AFTER the prompt reaches the LLM. In this scenario  the model is used as a reasoning engine to decide whether it needs to trigger RAG processes or not (and what mechanisms to use) to generate the appropriate completion based on the perceivable context. This process is usually known as Agentic RAG. In this scenario  the retrieval process doesnt happen all the time  like with the other integration approach.   As a last note  it is also common to classify the RAG process based on its internal logic and complexity. Following this approach  we typically divide it into naive RAG  advanced (complex) RAG  Modular RAG  hybrid RAG  etc. Since this is a diverse and complex area with reliable sources  Ill just mention that we used Advanced RAG for POC purposes because their previous prototype did so. If you are interested in learning more about different RAG mechanisms  I do recommend Vipra Sings article on Advanced RAGs.   The main change I made to the POCs RAG process was related to how it is triggered: I used the agentic RAG approach and made all the changes and enhancements to the existing complex RAG mechanisms to accommodate that. Additionally  I will fine-tune the model to determine which specific RAG strategy is more effective in improving its completion.   Choosing the Right FormatBacking again to the POC  the first step was to decide the best file format for the documents and how exactly the training set was going to be built.   All the available files have PDF and docx formats. None of them seemed to be suitable formats. because they have too much-unneeded data related to text styling and fonts  etc.  and we only needed the semantic content and some level of textual structure.   Considering the requirements  the markdown format (also known as MD) appeared to be a more viable option because it preserves structure (tables  headings  lists) and also some level of semantics (bold  italics  code blocks) and', mimetype='text/plain', start_char_idx=8842, end_char_idx=11091, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.8100621509005572), NodeWithScore(node=TextNode(id_='5cbb84b5-c5e5-4e94-a189-43fa6bc9b138', embedding=None, metadata={'url': 'https://huggingface.co/docs/transformers/model_doc/rag', 'title': 'RAG', 'tokens': 933, 'source': 'transformers', 'questions_this_excerpt_can_answer': '1. What are RAG models?\\n2. What are the two RAG formulations compared?', 'prev_section_summary': 'RAG models combine pretrained dense retrieval (DPR) and sequence-to-sequence models to retrieve documents, pass them to a seq2seq model, and generate outputs. The retriever and seq2seq modules are jointly fine-tuned. The paper \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" introduces two RAG formulations: one that conditions on the same retrieved passages for the entire generated sequence, and another that can use different passages per token.', 'section_summary': 'RAG models combine a pre-trained seq2seq model (parametric memory) with a dense vector index (non-parametric memory) accessed by a neural retriever for language generation. Two formulations are compared: one that uses the same retrieved passages for the entire generated sequence, and another that can use different passages per token. These models are fine-tuned jointly and excel in knowledge-intensive NLP tasks, outperforming parametric seq2seq models and retrieve-and-extract architectures.', 'excerpt_keywords': 'RAG, retrieval-augmented generation, seq2seq, neural retriever, knowledge-intensive NLP'}, excluded_embed_metadata_keys=['url', 'tokens', 'source'], excluded_llm_metadata_keys=['title', 'tokens', 'source'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f70e029e-f38c-5e9f-94a9-27d2c18ad679', node_type='4', metadata={'url': 'https://huggingface.co/docs/transformers/model_doc/rag', 'title': 'RAG', 'tokens': 933, 'source': 'transformers'}, hash='80fa67eb8fd53c75a852194144025428a7aba7b8ebbb7ebeadf036f1c3d6e2d8'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5f8d4c91-bd53-4a7a-b1f7-d8f74debb425', node_type='1', metadata={'url': 'https://huggingface.co/docs/transformers/model_doc/rag', 'title': 'RAG', 'tokens': 933, 'source': 'transformers'}, hash='4d3f9f51d82106604a4ed1daae1eaf34df721690e2f383353fb54e645e681aa3'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='f5ddd7e4-4e0e-4b1c-8528-964cbb2929e5', node_type='1', metadata={}, hash='0dd82738dbd65e8c47605655f5f86d3dfbfbe1e650279602bd5ad84be6cc127a')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generationtasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-artparametric-only seq2seq baseline.*This model was contributed by [ola13](https://huggingface.co/ola13).## Usage tipsRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks.## RagConfig[[autodoc]] RagConfig## RagTokenizer[[autodoc]] RagTokenizer## Rag specific outputs[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput## RagRetriever[[autodoc]] RagRetriever<frameworkcontent><pt>## RagModel[[autodoc]] RagModel    - forward## RagSequenceForGeneration[[autodoc]] RagSequenceForGeneration    - forward    - generate## RagTokenForGeneration[[autodoc]] RagTokenForGeneration    - forward    - generate</pt><tf>## TFRagModel[[autodoc]] TFRagModel    - call## TFRagSequenceForGeneration[[autodoc]] TFRagSequenceForGeneration    - call', mimetype='text/plain', start_char_idx=1593, end_char_idx=3653, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.8081449020602206), NodeWithScore(node=TextNode(id_='63abb781-a6ca-4249-8e34-7e936f2f68a6', embedding=None, metadata={'url': 'https://towardsai.net/p/machine-learning/graphrag-analysis-part-2-graph-creation-and-retrieval-vs-vector-database-retrieval', 'title': 'GraphRAG Analysis  Part 2: Graph Creation and Retrieval vs Vector Database Retrieval', 'tokens': 3330, 'source': 'tai_blog', 'questions_this_excerpt_can_answer': '1. Which RAGAS metric showed a significant difference between GraphRAG and vector-based RAG?\\n2. What was the faithfulness score for Neo4j GraphRAG compared to FAISS?', 'prev_section_summary': 'This excerpt details the process of generating a \"ground truth\" dataset for evaluating RAG (Retrieval Augmented Generation) systems. It involves:\\n\\n1.  **Text Splitting:** Breaking down source texts into smaller chunks.\\n2.  **Question Generation:** Using an LLM (Large Language Model) to create diverse questions from these text chunks.\\n3.  **Answer Generation:** Employing an LLM to provide concise answers to the generated questions based on the original text, or indicating if information is unavailable.\\n4.  **Context Generation:** Using an LLM to extract relevant context supporting each answer.\\n5.  **Ground Truth Dataset Creation:** Compiling these questions, answers, and contexts into a dataset for RAG evaluation.\\n\\nThe excerpt also briefly mentions an `evaluate_rag_async` function, indicating this ground truth is used to assess different RAG chains.', 'section_summary': '1. The RAGAS metric that showed a significant difference between GraphRAG and vector-based RAG was **Faithfulness**.\\n2. The faithfulness score for Neo4j GraphRAG was **0.54**, compared to **0.18** for FAISS.', 'excerpt_keywords': 'GraphRAG, RAGAS, Faithfulness, Neo4j, FAISS'}, excluded_embed_metadata_keys=['url', 'tokens', 'source'], excluded_llm_metadata_keys=['title', 'tokens', 'source'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='16844201-c42c-5e85-92db-13654bba4d65', node_type='4', metadata={'url': 'https://towardsai.net/p/machine-learning/graphrag-analysis-part-2-graph-creation-and-retrieval-vs-vector-database-retrieval', 'title': 'GraphRAG Analysis  Part 2: Graph Creation and Retrieval vs Vector Database Retrieval', 'tokens': 3330, 'source': 'tai_blog'}, hash='5e38c298611d483ee67e3663ce4190b0e90925cae9c2b47f59a5445ad6944745'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1910ac9c-2d2a-4894-b1e8-4e82dd435b03', node_type='1', metadata={'url': 'https://towardsai.net/p/machine-learning/graphrag-analysis-part-2-graph-creation-and-retrieval-vs-vector-database-retrieval', 'title': 'GraphRAG Analysis  Part 2: Graph Creation and Retrieval vs Vector Database Retrieval', 'tokens': 3330, 'source': 'tai_blog'}, hash='c5fc0b7b87f771c4d0f76b8e0aa0440538bf36c6ccfcf957914564ccab86a43e'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2524453d-1355-47b8-9475-bfeb55b7f331', node_type='1', metadata={}, hash='06b805c9f7689813d99a5d0f5c21da385ab30c00c94c31b86fdd946a3801c737')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"context available.'\\\\n\\\\n Question: {question}\\\\nAnswer: {answer}\\\\n\\\\nRelevant context: ) context_response = llm(context_prompt.format_messages(question=question  answer=answer)) context = context_response.content.strip() ground_truth.append({ question: question  answer: answer  context: context  }) return ground_truth async def evaluate_rag_async(rag_chain  ground_truth  name): # ... (evaluation function implementation) async def run_evaluations(rag_chains  ground_truth): results = {} for name  chain in rag_chains.items(): result = await evaluate_rag_async(chain  ground_truth  name) results.update(result) return results # Main execution function async def main(): # Ensure vector index ensure_vector_index(recreate=True) # Create retrievers neo4j_retriever = create_neo4j_retriever() # Create RAG chains faiss_rag_chain = create_rag_chain(faiss_retriever) neo4j_rag_chain = create_rag_chain(neo4j_retriever) # Generate ground truth ground_truth = create_ground_truth(texts) # Run evaluations rag_chains = { FAISS: faiss_rag_chain  Neo4j: neo4j_rag_chain } results = await run_evaluations(rag_chains  ground_truth) return results # Run the main function if __name__ == __main__: nest_asyncio.apply() try: results = asyncio.run(asyncio.wait_for(main()  timeout=7200)) # 2 hour timeout plot_results(results) # Print detailed results for name  result in results.items(): print(fResults for {name}:) print(result) print() except asyncio.TimeoutError: print(Evaluation timed out after 2 hours.) finally: # Close the Neo4j driver driver.close()This setup creates a ground truth dataset  evaluates our RAG chains using RAGAS metrics  and visualizes the results.   Results and AnalysisThis analysis revealed a surprising similarity in performance between GraphRAG and vector-based RAG across most metrics  with one difference:   Faithfulness: Neo4j GraphRAG significantly outperformed FAISS (0.54 vs 0.18)  but did not outperform significantly in any other metrics.   The graph-based approach excels in faithfulness likely because it\", mimetype='text/plain', start_char_idx=10765, end_char_idx=12794, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.805026283887231), NodeWithScore(node=TextNode(id_='ee44937c-a668-4bd5-b09c-2a2487ea432c', embedding=None, metadata={'url': 'https://towardsai.net/p/machine-learning/rag-architecture-advanced-rag', 'title': 'RAG Architecture: Advanced RAG', 'tokens': 2241, 'source': 'tai_blog', 'questions_this_excerpt_can_answer': '1. What is the primary purpose of self-reflection tokens in Self-RAG?\\n2. How does HyDe modify the traditional RAG retrieval process?', 'prev_section_summary': '1. The primary challenge with parsing PDF documents for RAG systems is extracting all the embedded data, such as tables (especially nested ones), images, text, and graphics, because the PDF format makes it difficult to retrieve this information accurately.\\n2. CRAG (Corrective Retrieval Augmented Generation) and Self-RAG aim to improve answer quality by introducing self-correction and reflection mechanisms. CRAG automates the process of evaluating retrieval results and, if irrelevant, triggers corrective actions like prompt correction or graph/Google searches. Self-RAG fine-tunes the LLM to generate self-reflection tokens alongside regular output, allowing the model to assess and improve its own responses.', 'section_summary': '1. Self-reflection tokens in Self-RAG are used to fine-tune the LLM to generate additional tokens that indicate its confidence and guide its actions. These tokens include: \"Retrieve\" (determines if chunks need retrieval), \"ISREL\" (determines chunk relevance), \"ISSUP\" (determines if the LLM\\'s response is supported by the chunk), and \"ISUSE\" (determines the usefulness of the LLM\\'s response).\\n2. HyDe modifies the traditional RAG retrieval process by using the LLM to generate a hypothetical response (a virtual document) to the user\\'s question, and then uses this generated response to search the vector database for similar answers, rather than using the original user question directly. This is done to address abstract user questions that require more context from the LLM.', 'excerpt_keywords': 'Self-RAG, HyDe, LLM, retrieval, self-reflection'}, excluded_embed_metadata_keys=['url', 'tokens', 'source'], excluded_llm_metadata_keys=['title', 'tokens', 'source'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9c4370e5-91bb-5e4e-8c1a-a8a34b5e8231', node_type='4', metadata={'url': 'https://towardsai.net/p/machine-learning/rag-architecture-advanced-rag', 'title': 'RAG Architecture: Advanced RAG', 'tokens': 2241, 'source': 'tai_blog'}, hash='05587d8545c9916a0747e7852f606c7101543e233d1d5f227f94fc87040339be'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='aa6f76e4-8255-49ce-9e9c-72ae5adf7a8f', node_type='1', metadata={'url': 'https://towardsai.net/p/machine-learning/rag-architecture-advanced-rag', 'title': 'RAG Architecture: Advanced RAG', 'tokens': 2241, 'source': 'tai_blog'}, hash='6afa8e5c1455b4b9d1fc6b7d8e6891020df34148261d079e6f6470748aa8297c')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='that automates this process. Essentially  this is another graph  implementing a state machine (surprise U+1F642)  which looks something like this:   To implement it  its easiest to use LangGraph  which will be discussed further.   Self-RAGSelf-reflective RAG is based on research claiming that this approach provides better results than regular RAG. Overall  the idea is very similar to the previous one (CRAG) but goes further. The idea is to fine-tune the LLM to generate self-reflection tokens in addition to the regular ones. This is very convenient  as there is no need to guess how confident the LLM is and what to do with it. The following tokens are generated:   Retrieve token determines whether D chunks need to be retrieved for a given prompt x. Options: Yes  No  ContinueISREL token determines whether chunk d from D is relevant to the given prompt x. Options: relevant and irrelevantISSUP token determines whether the LLMs response y to chunk d is supported by chunk d. Options: fully supported  partially supported  no supportISUSE token determines whether the LLMs response to each chunk d is a useful answer to the query x. Options represent a usefulness scale from 5 to 1.Using these tokens  a state machine can be built  using the aforementioned LangGraph  which looks something like this:   For more details  see here.   HyDeAnother method  similar to RAG Fusion  is that it modifies the usual RAG retrieval process. HyDe stands for Hypothetical Document Embeddings and is based on the study Precise Zero-Shot Dense Retrieval without Relevance Labels. The idea is very simple  instead of using the users question for searching in the vector database  we use the LLM to generate a response (a virtual hypothetical document) and then use the response for searching in the vector database (to find similar answers).   Why all this? Sometimes users questions are too abstract and require more context  which the LLM can provide  and without which the search in the database makes no sense.   I think this is not an exhaustive review of the new changes; if I forgot something  write in the comments.', mimetype='text/plain', start_char_idx=8774, end_char_idx=10887, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.8031992247354013), NodeWithScore(node=TextNode(id_='68528fe3-e101-4358-b3db-f0894346b348', embedding=None, metadata={'url': 'https://towardsai.net/p/machine-learning/fine-tuning-llms-with-synthetic-data-for-high-quality-content-generation', 'title': 'Fine-Tuning LLMs with Synthetic Data for High-Quality Content Generation', 'tokens': 7359, 'source': 'tai_blog', 'questions_this_excerpt_can_answer': '1. What is Catastrophic Forgetting in LLMs?\\n2. How does RAG address the limitations of outdated LLM knowledge?', 'prev_section_summary': \"An instruction-tuned LLM is a language model trained on Q&A and chat tasks to become a user-centered assistant. Model Alignment is a phase before public release that ensures the model's outputs align with human values and intentions, focusing on responsible content generation.\", 'section_summary': \"1. **Catastrophic Forgetting:** This occurs when LLMs, during frequent custom fine-tuning, forget previously learned knowledge.\\n2. **RAG (Retrieval Augmented Generation):** RAG addresses outdated LLM knowledge by connecting the LLM to external, real-time data sources via retrieval mechanisms. It combines the LLM's generative capabilities with the ability to search and incorporate relevant information from knowledge bases. RAG systems vary by information source (databases, internet), retrieval mechanism (search engines, APIs), and integration method (before or after the prompt reaches the LLM).\", 'excerpt_keywords': 'Catastrophic Forgetting, RAG, LLM, Retrieval Augmented Generation, Synthetic Data'}, excluded_embed_metadata_keys=['url', 'tokens', 'source'], excluded_llm_metadata_keys=['title', 'tokens', 'source'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='903c2f00-5fc8-533f-ad54-8057e894bd38', node_type='4', metadata={'url': 'https://towardsai.net/p/machine-learning/fine-tuning-llms-with-synthetic-data-for-high-quality-content-generation', 'title': 'Fine-Tuning LLMs with Synthetic Data for High-Quality Content Generation', 'tokens': 7359, 'source': 'tai_blog'}, hash='885a37a88b7a58b5b14bf9933189f9e97cfd9d6e3c0dc7aa53920d44bf008d5e'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='08cd174d-8e0d-4b2e-97b3-0dd376fa1d35', node_type='1', metadata={'url': 'https://towardsai.net/p/machine-learning/fine-tuning-llms-with-synthetic-data-for-high-quality-content-generation', 'title': 'Fine-Tuning LLMs with Synthetic Data for High-Quality Content Generation', 'tokens': 7359, 'source': 'tai_blog'}, hash='87a8d1879fc7eac29db92d2b5344dc980f365d732f02bd1ca6429e7a6fcf502f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='25b89322-bdda-4348-a4b0-3245ee8d524b', node_type='1', metadata={}, hash='1533a6852ff3a3c7dbcb16ada602b7a2b94e0359696c75ef1614b7e3c135330f')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='during the training sessions  and this is usually true for the entire machine learning field. As the training process for LLMs is resource-intensive  costly  and time-consuming  it happens only at intervals of months (sometimes more)  and the model knowledge quickly becomes outdated. Frequent custom fine-tuning cycles are an option  but beyond being expensive  doing so indiscriminately can lead to a problem known as Catastrophic Forgetting (Catastrophic inferencing is also a common term for this phenomenon)  where the models forget previously learned knowledge. Plus  the models dont have access to real-time data. A more viable solution to deal with this is RAG.   RAG stands for Retrieval Augmented Generation  the name given to a family of processes that focuses on connecting the LLM to external sources through retrieval mechanisms. A combination of the generative capabilities of the model with the ability to search for and incorporate relevant information from one knowledge base (or several).   There are different ways of classifying such systems  but most of them vary based on a few factors:   Source of Information: Those sources can be literally anything from traditional databases  vector databases  knowledge graphs  to the internet itself.Retrieval Mechanism: As the sources are so varied  the same is true for the methods used to collect information  such as search engines  APIs  customized database searches  etc.Integration Method: It is also common to classify RAG systems based on how they are incorporated with the LLM to generate the completion process.I will only focus on explaining the difference in the integration logic in this article  as it was the only noticeable change I made regarding the original prototype.   The RAG mechanism can be integrated as soon as the user prompts the input BEFORE the information reaches the LLM for completion. In this case  the RAG process happens every time a new input prompt is entered by the user  and the results of this process are used to enhance the user prompt by the time it hits the model.   Or the RAG process can occur AFTER the prompt reaches the LLM. In this scenario  the model is used as a reasoning engine to decide whether it needs to trigger RAG processes or not (and what mechanisms to use) to generate the appropriate completion based on the perceivable context. This process is usually known as Agentic', mimetype='text/plain', start_char_idx=7053, end_char_idx=9450, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.8027662815663229), NodeWithScore(node=TextNode(id_='dec5d5d3-d325-4ee3-b7bc-bd41887d3e10', embedding=None, metadata={'url': 'https://docs.llamaindex.ai/en/stable/understanding/index', 'title': 'Building an LLM application', 'tokens': 979, 'source': 'llama_index', 'questions_this_excerpt_can_answer': '1. What are the two main parts of building an LLM application in LlamaIndex?\\n2. What is Retrieval-Augmented Generation (RAG) and what are its key components?', 'prev_section_summary': 'A Summary Index is a simple index ideal for generating summaries of text within documents. It stores and returns all documents to the query engine. A potential drawback of embedding all text is that it can be time-consuming and expensive, especially with hosted LLMs.', 'section_summary': 'LlamaIndex outlines two main parts for building an LLM application: **Building a RAG pipeline** and **Building an agent**.\\n\\n**Retrieval-Augmented Generation (RAG)** is a technique to integrate your data with an LLM. Its key components include:\\n1.  **Loading & Ingestion**: Connecting to various data sources (text, PDFs, databases, APIs) using LlamaHub connectors.\\n2.  **Indexing and Embedding**: Structuring and accessing data efficiently using built-in strategies.\\n3.  **Storing**: Saving indexed data or LLM-generated summaries, often in a Vector Store.', 'excerpt_keywords': 'LLM application, RAG pipeline, LlamaIndex, data ingestion, vector store'}, excluded_embed_metadata_keys=['url', 'tokens', 'source'], excluded_llm_metadata_keys=['title', 'tokens', 'source'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='553d366f-bbad-5110-aae1-3988cb024d60', node_type='4', metadata={'url': 'https://docs.llamaindex.ai/en/stable/understanding/index', 'title': 'Building an LLM application', 'tokens': 979, 'source': 'llama_index'}, hash='601b94b25d0e96ebd377de54b67179af0553bf231f181056e0e856fa9d32f1d6'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2cd98e27-775a-43a4-9b99-162c258b2a84', node_type='1', metadata={}, hash='8bdb37d3d9ec83921bc28aa0cd32b055d955b591afada0b12ac39ed1920ccb17')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"# Building an LLM applicationWelcome to the beginning of Understanding LlamaIndex. This is a series of short, bite-sized tutorials on every stage of building an LLM application to get you acquainted with how to use LlamaIndex before diving into more advanced and subtle strategies. If you're an experienced programmer new to LlamaIndex, this is the place to start.## Key steps in building an LLM application!!! tip    If you've already read our [high-level concepts](../getting_started/concepts.md) page you'll recognize several of these steps.This tutorial has two main parts: **Building a RAG pipeline** and **Building an agent**, with some smaller sections before and after. Here's what to expect:- **[Using LLMs](./using_llms/using_llms.md)**: hit the ground running by getting started working with LLMs. We'll show you how to use any of our [dozens of supported LLMs](../module_guides/models/llms/modules/), whether via remote API calls or running locally on your machine.- **Building a RAG pipeline**: Retrieval-Augmented Generation (RAG) is a key technique for getting your data into an LLM, and a component of more sophisticated agentic systems. We'll show you how to build a full-featured RAG pipeline that can answer questions about your data. This includes:    - **[Loading & Ingestion](./loading/loading.md)**: Getting your data from wherever it lives, whether that's unstructured text, PDFs, databases, or APIs to other applications. LlamaIndex has hundreds of connectors to every data source over at [LlamaHub](https://llamahub.ai/).    - **[Indexing and Embedding](./indexing/indexing.md)**: Once you've got your data there are an infinite number of ways to structure access to that data to ensure your applications is always working with the most relevant data. LlamaIndex has a huge number of these strategies built-in and can help you select the best ones.    - **[Storing](./storing/storing.md)**: You will probably find it more efficient to store your data in indexed form, or pre-processed summaries provided by an LLM, often in a specialized database known as a `Vector Store` (see below). You can also store your\", mimetype='text/plain', start_char_idx=0, end_char_idx=2135, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.8021715285904161), NodeWithScore(node=TextNode(id_='5f8d4c91-bd53-4a7a-b1f7-d8f74debb425', embedding=None, metadata={'url': 'https://huggingface.co/docs/transformers/model_doc/rag', 'title': 'RAG', 'tokens': 933, 'source': 'transformers', 'questions_this_excerpt_can_answer': '1. What are RAG models?\\n2. What two RAG formulations are compared in the paper?', 'prev_section_summary': '1. When using BLIP-2 for VQA, the textual prompt must follow the format: `Question: {} Answer:`.\\n2. The model used in the example for Visual Question Answering is `Blip2ForConditionalGeneration` from \"Salesforce/blip2-opt-2.7b\".', 'section_summary': 'RAG models combine pretrained dense retrieval (DPR) and sequence-to-sequence models to retrieve documents, pass them to a seq2seq model, and generate outputs. The retriever and seq2seq modules are jointly fine-tuned. The paper \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" introduces two RAG formulations: one that conditions on the same retrieved passages for the entire generated sequence, and another that can use different passages per token.', 'excerpt_keywords': 'RAG, dense retrieval, seq2seq, knowledge-intensive NLP, language generation'}, excluded_embed_metadata_keys=['url', 'tokens', 'source'], excluded_llm_metadata_keys=['title', 'tokens', 'source'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f70e029e-f38c-5e9f-94a9-27d2c18ad679', node_type='4', metadata={'url': 'https://huggingface.co/docs/transformers/model_doc/rag', 'title': 'RAG', 'tokens': 933, 'source': 'transformers'}, hash='80fa67eb8fd53c75a852194144025428a7aba7b8ebbb7ebeadf036f1c3d6e2d8'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='5cbb84b5-c5e5-4e94-a189-43fa6bc9b138', node_type='1', metadata={}, hash='c01e1f9f7b7df2f66e615e63d4979068228e41b5495f890f956a7e5fb73aa9c3')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='# RAG<div class=\"flex flex-wrap space-x-1\"><a href=\"https://huggingface.co/models?filter=rag\"><img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-rag-blueviolet\"></a></div>## OverviewRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) andsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generateoutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowingboth retrieval and generation to adapt to downstream tasks.It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, VladimirKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.The abstract from the paper is the following:*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achievestate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and preciselymanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behindtask-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledgeremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametricmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive', mimetype='text/plain', start_char_idx=0, end_char_idx=2244, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.8017680751522839), NodeWithScore(node=TextNode(id_='1910ac9c-2d2a-4894-b1e8-4e82dd435b03', embedding=None, metadata={'url': 'https://towardsai.net/p/machine-learning/graphrag-analysis-part-2-graph-creation-and-retrieval-vs-vector-database-retrieval', 'title': 'GraphRAG Analysis  Part 2: Graph Creation and Retrieval vs Vector Database Retrieval', 'tokens': 3330, 'source': 'tai_blog', 'questions_this_excerpt_can_answer': '1. What is the primary focus of \"GraphRAG Analysis Part 2\"?\\n2. What are the two main retrieval methods compared in the article?', 'prev_section_summary': '1. RAG chains are created to associate retrievers (FAISS or Neo4j) with a language model (GPT-3.5-turbo) to generate answers based on the retrieved context.\\n2. The evaluation of the RAG systems involves creating a ground truth dataset and utilizing the RAGAS framework. A `create_ground_truth` function is defined to generate a specified number of diverse questions from the provided texts using a language model and a text splitter.', 'section_summary': 'This excerpt details the process of generating a \"ground truth\" dataset for evaluating RAG (Retrieval Augmented Generation) systems. It involves:\\n\\n1.  **Text Splitting:** Breaking down source texts into smaller chunks.\\n2.  **Question Generation:** Using an LLM (Large Language Model) to create diverse questions from these text chunks.\\n3.  **Answer Generation:** Employing an LLM to provide concise answers to the generated questions based on the original text, or indicating if information is unavailable.\\n4.  **Context Generation:** Using an LLM to extract relevant context supporting each answer.\\n5.  **Ground Truth Dataset Creation:** Compiling these questions, answers, and contexts into a dataset for RAG evaluation.\\n\\nThe excerpt also briefly mentions an `evaluate_rag_async` function, indicating this ground truth is used to assess different RAG chains.', 'excerpt_keywords': 'Ground truth, RAG, LLM, Text splitting, Question generation'}, excluded_embed_metadata_keys=['url', 'tokens', 'source'], excluded_llm_metadata_keys=['title', 'tokens', 'source'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='16844201-c42c-5e85-92db-13654bba4d65', node_type='4', metadata={'url': 'https://towardsai.net/p/machine-learning/graphrag-analysis-part-2-graph-creation-and-retrieval-vs-vector-database-retrieval', 'title': 'GraphRAG Analysis  Part 2: Graph Creation and Retrieval vs Vector Database Retrieval', 'tokens': 3330, 'source': 'tai_blog'}, hash='5e38c298611d483ee67e3663ce4190b0e90925cae9c2b47f59a5445ad6944745'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='abc1339f-3efb-41a4-9b8f-1a7834491431', node_type='1', metadata={'url': 'https://towardsai.net/p/machine-learning/graphrag-analysis-part-2-graph-creation-and-retrieval-vs-vector-database-retrieval', 'title': 'GraphRAG Analysis  Part 2: Graph Creation and Retrieval vs Vector Database Retrieval', 'tokens': 3330, 'source': 'tai_blog'}, hash='835dd6498390d5bc1dfe10c97ef8a2ca540759ae892b1e3bedb2911fcdd4896b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='63abb781-a6ca-4249-8e34-7e936f2f68a6', node_type='1', metadata={}, hash='5396bb75be1601da316592055c091d4504c0dcf69edb74b406b8e201b7374b29')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"item.page_content if isinstance(item  Document) else item text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000  chunk_overlap=200) all_splits = text_splitter.split_text(' '.join(get_text(doc) for doc in texts)) ground_truth = [] question_prompt = ChatPromptTemplate.from_template( Given the following text  generate {num_questions} diverse and specific questions that can be answered based on the information in the text.  Provide the questions as a numbered list.\\\\n\\\\nText: {text}\\\\n\\\\nQuestions: ) all_questions = [] for split in tqdm(all_splits  desc=Generating questions): response = llm_ground_truth(question_prompt.format_messages(num_questions=3  text=split)) questions = response.content.strip().split('\\\\n') all_questions.extend([q.split('. '  1)[1] if '. ' in q else q for q in questions]) random.shuffle(all_questions) selected_questions = all_questions[:num_questions] llm = ChatOpenAI(model_name=gpt-3.5-turbo  temperature=0) for question in tqdm(selected_questions  desc=Generating ground truth): answer_prompt = ChatPromptTemplate.from_template( Given the following question  provide a concise and accurate answer based on the information available.  If the answer is not directly available  respond with 'Information not available in the given context.'\\\\n\\\\nQuestion: {question}\\\\n\\\\nAnswer: ) answer_response = llm(answer_prompt.format_messages(question=question)) answer = answer_response.content.strip() context_prompt = ChatPromptTemplate.from_template( Given the following question and answer  provide a brief  relevant context that supports this answer.  If no relevant context is available  respond with 'No relevant context available.'\\\\n\\\\n Question: {question}\\\\nAnswer: {answer}\\\\n\\\\nRelevant context: ) context_response = llm(context_prompt.format_messages(question=question  answer=answer)) context = context_response.content.strip() ground_truth.append({ question: question  answer: answer  context: context  }) return ground_truth async def evaluate_rag_async(rag_chain  ground_truth  name): # ... (evaluation function implementation) async def run_evaluations(rag_chains  ground_truth): results = {} for name  chain in rag_chains.items(): result = await evaluate_rag_async(chain  ground_truth\", mimetype='text/plain', start_char_idx=9122, end_char_idx=11344, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.7996934672023613), NodeWithScore(node=TextNode(id_='f5ddd7e4-4e0e-4b1c-8528-964cbb2929e5', embedding=None, metadata={'url': 'https://huggingface.co/docs/transformers/model_doc/rag', 'title': 'RAG', 'tokens': 933, 'source': 'transformers', 'questions_this_excerpt_can_answer': '1. What is `RagRetriever`?\\n2. What are the generation methods for `RagSequenceForGeneration` and `RagTokenForGeneration`?', 'prev_section_summary': 'RAG models combine a pre-trained seq2seq model (parametric memory) with a dense vector index (non-parametric memory) accessed by a neural retriever for language generation. Two formulations are compared: one that uses the same retrieved passages for the entire generated sequence, and another that can use different passages per token. These models are fine-tuned jointly and excel in knowledge-intensive NLP tasks, outperforming parametric seq2seq models and retrieve-and-extract architectures.', 'section_summary': 'This excerpt defines the `RagRetriever` as a component within the RAG model. It also indicates that `RagSequenceForGeneration` and `RagTokenForGeneration` models use a `generate` method for text generation.', 'excerpt_keywords': 'RagRetriever, RagModel, RagSequenceForGeneration, RagTokenForGeneration, generate'}, excluded_embed_metadata_keys=['url', 'tokens', 'source'], excluded_llm_metadata_keys=['title', 'tokens', 'source'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f70e029e-f38c-5e9f-94a9-27d2c18ad679', node_type='4', metadata={'url': 'https://huggingface.co/docs/transformers/model_doc/rag', 'title': 'RAG', 'tokens': 933, 'source': 'transformers'}, hash='80fa67eb8fd53c75a852194144025428a7aba7b8ebbb7ebeadf036f1c3d6e2d8'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5cbb84b5-c5e5-4e94-a189-43fa6bc9b138', node_type='1', metadata={'url': 'https://huggingface.co/docs/transformers/model_doc/rag', 'title': 'RAG', 'tokens': 933, 'source': 'transformers'}, hash='b6c4d2e38653bd1aa4be61e82f0ed2aa7ff86ce341dddf3c796ad8877a5083c5')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='RagRetriever<frameworkcontent><pt>## RagModel[[autodoc]] RagModel    - forward## RagSequenceForGeneration[[autodoc]] RagSequenceForGeneration    - forward    - generate## RagTokenForGeneration[[autodoc]] RagTokenForGeneration    - forward    - generate</pt><tf>## TFRagModel[[autodoc]] TFRagModel    - call## TFRagSequenceForGeneration[[autodoc]] TFRagSequenceForGeneration    - call    - generate## TFRagTokenForGeneration[[autodoc]] TFRagTokenForGeneration    - call    - generate</tf></frameworkcontent>', mimetype='text/plain', start_char_idx=3270, end_char_idx=3776, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.7938195501033072), NodeWithScore(node=TextNode(id_='abc1339f-3efb-41a4-9b8f-1a7834491431', embedding=None, metadata={'url': 'https://towardsai.net/p/machine-learning/graphrag-analysis-part-2-graph-creation-and-retrieval-vs-vector-database-retrieval', 'title': 'GraphRAG Analysis  Part 2: Graph Creation and Retrieval vs Vector Database Retrieval', 'tokens': 3330, 'source': 'tai_blog', 'questions_this_excerpt_can_answer': '1. What is the purpose of creating RAG chains in the context of the article?\\n2. How is the evaluation of the RAG systems set up, and what framework is used?', 'prev_section_summary': 'Here are the concise answers to your questions:\\n\\n1. **How does the Neo4j retriever utilize graph structure to find relevant information?**\\n   The Neo4j retriever leverages the graph structure to find related entities and their relationships, going beyond simple vector similarity to understand connections within the data.\\n\\n2. **What is the purpose of the `create_rag_chain` function in this context?**\\n   The `create_rag_chain` function is used to construct a Retrieval Augmented Generation (RAG) chain by combining a retriever (like the Neo4j retriever) with a Language Model (LLM) and a prompt template to answer questions based on retrieved context.', 'section_summary': '1. RAG chains are created to associate retrievers (FAISS or Neo4j) with a language model (GPT-3.5-turbo) to generate answers based on the retrieved context.\\n2. The evaluation of the RAG systems involves creating a ground truth dataset and utilizing the RAGAS framework. A `create_ground_truth` function is defined to generate a specified number of diverse questions from the provided texts using a language model and a text splitter.', 'excerpt_keywords': 'RAG chains, FAISS, Neo4j, RAGAS, ground truth'}, excluded_embed_metadata_keys=['url', 'tokens', 'source'], excluded_llm_metadata_keys=['title', 'tokens', 'source'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='16844201-c42c-5e85-92db-13654bba4d65', node_type='4', metadata={'url': 'https://towardsai.net/p/machine-learning/graphrag-analysis-part-2-graph-creation-and-retrieval-vs-vector-database-retrieval', 'title': 'GraphRAG Analysis  Part 2: Graph Creation and Retrieval vs Vector Database Retrieval', 'tokens': 3330, 'source': 'tai_blog'}, hash='5e38c298611d483ee67e3663ce4190b0e90925cae9c2b47f59a5445ad6944745'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='ed8a04e1-4ae4-4f77-911a-f516c4af0b96', node_type='1', metadata={'url': 'https://towardsai.net/p/machine-learning/graphrag-analysis-part-2-graph-creation-and-retrieval-vs-vector-database-retrieval', 'title': 'GraphRAG Analysis  Part 2: Graph Creation and Retrieval vs Vector Database Retrieval', 'tokens': 3330, 'source': 'tai_blog'}, hash='c66a7825db39a0198e36c490e7d731706d03616131012c194538c75a4cf2a833'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='1910ac9c-2d2a-4894-b1e8-4e82dd435b03', node_type='1', metadata={}, hash='1bf30bfc6f45a1cca14cfa891bf697f592262d6de7813f7357f990a10b2b97d9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"{record['name']}\\\\nRelated: {'  '.join(record['related'])} documents.append(Document(page_content=content)) return documentsThe FAISS retriever uses vector similarity to find relevant information  while the Neo4j retrievers leverage the graph structure to find related entities and their relationships.   Creating RAG ChainsNow  lets create our RAG chains:   def create_rag_chain(retriever): llm = ChatOpenAI(model_name=gpt-3.5-turbo) template = Answer the question based on the following context: {context} Question: {question} Answer: prompt = PromptTemplate.from_template(template) if callable(retriever): # For Cypher retriever retriever_func = lambda q: retriever(q) else: # For FAISS retriever retriever_func = retriever return ( {context: retriever_func  question: RunnablePassthrough()} U+007C prompt U+007C llm U+007C StrOutputParser() ) # Create RAG chains faiss_rag_chain = create_rag_chain(faiss_retriever) cypher_rag_chain = create_rag_chain(cypher_retriever)These chains associate the retrievers with a language model to generate answers based on the retrieved context.   Evaluation SetupTo evaluate our RAG systems  well create a ground truth dataset and use the RAGAS framework:   def create_ground_truth(texts: List[Union[str  Document]]  num_questions: int = 100) -> List[Dict]: llm_ground_truth = ChatOpenAI(model_name=gpt-3.5-turbo  temperature=0.2) def get_text(item): return item.page_content if isinstance(item  Document) else item text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000  chunk_overlap=200) all_splits = text_splitter.split_text(' '.join(get_text(doc) for doc in texts)) ground_truth = [] question_prompt = ChatPromptTemplate.from_template( Given the following text  generate {num_questions} diverse and specific questions that can be answered based on the information in the text.  Provide the questions as a numbered list.\\\\n\\\\nText: {text}\\\\n\\\\nQuestions: ) all_questions = [] for split in tqdm(all_splits  desc=Generating questions): response\", mimetype='text/plain', start_char_idx=7726, end_char_idx=9714, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.7937841353670847)]\n",
            "query_bundle Explain how Retrieval Augmented Generation (RAG) works?\n",
            "Node IDs: ['68528fe3-e101-4358-b3db-f0894346b348', '5f8d4c91-bd53-4a7a-b1f7-d8f74debb425', 'dec5d5d3-d325-4ee3-b7bc-bd41887d3e10', '25b89322-bdda-4348-a4b0-3245ee8d524b', '5cbb84b5-c5e5-4e94-a189-43fa6bc9b138', 'ee44937c-a668-4bd5-b09c-2a2487ea432c', 'abc1339f-3efb-41a4-9b8f-1a7834491431', '63abb781-a6ca-4249-8e34-7e936f2f68a6', '1910ac9c-2d2a-4894-b1e8-4e82dd435b03', 'f5ddd7e4-4e0e-4b1c-8528-964cbb2929e5']\n",
            "Scores: [0.9, 0.85, 0.75, 0.7, 0.65, 0.5, 0.4, 0.35, 0.3, 0.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res.response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "g2itPB5NlCKo",
        "outputId": "90cc9944-e720-4bd9-df07-6f45e5be4361"
      },
      "id": "g2itPB5NlCKo",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Retrieval Augmented Generation (RAG) is a technique that integrates external, real-time data sources with large language models (LLMs) to enhance their knowledge and content generation capabilities. It addresses the limitation of LLMs having outdated knowledge by connecting them to retrieval mechanisms that can search and incorporate relevant information from various knowledge bases.\\n\\nThe process involves combining the generative abilities of an LLM with the capacity to search for and integrate pertinent information. RAG systems can vary based on several factors:\\n\\n*   **Source of Information:** These can include traditional databases, vector databases, knowledge graphs, or the internet.\\n*   **Retrieval Mechanism:** Methods for collecting information can range from search engines and APIs to customized database searches.\\n*   **Integration Method:** RAG can be integrated either before the user's prompt reaches the LLM, where the retrieval process enhances the prompt, or after the prompt reaches the LLM, where the model acts as a reasoning engine to decide if and how to trigger RAG processes for completion.\\n\\nIn the context of building LLM applications, RAG involves key steps such as loading and ingesting data from various sources, indexing and embedding the data for efficient access, and storing the indexed data or LLM-generated summaries, often in a vector store.\\n\\nRAG models combine pretrained dense retrieval (DPR) and sequence-to-sequence models. They retrieve documents, pass them to a sequence-to-sequence model, and then marginalize to generate outputs. Both the retriever and the sequence-to-sequence modules are initialized from pretrained models and fine-tuned jointly to adapt to specific tasks. Two formulations of RAG have been explored: one that conditions on the same retrieved passages for the entire generated sequence, and another that can use different passages for each token.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the retrieved nodes\n",
        "for src in res.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsPB9cG5ltzd",
        "outputId": "12a09c5d-e68c-4c26-a4dc-b7c95ab3088f"
      },
      "id": "NsPB9cG5ltzd",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t 5cbb84b5-c5e5-4e94-a189-43fa6bc9b138\n",
            "Title\t RAG\n",
            "Text\t extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generationtasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-artparametric-only seq2seq baseline.*This model was contributed by [ola13](https://huggingface.co/ola13).## Usage tipsRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks.## RagConfig[[autodoc]] RagConfig## RagTokenizer[[autodoc]] RagTokenizer## Rag specific outputs[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput## RagRetriever[[autodoc]] RagRetriever<frameworkcontent><pt>## RagModel[[autodoc]] RagModel    - forward## RagSequenceForGeneration[[autodoc]] RagSequenceForGeneration    - forward    - generate## RagTokenForGeneration[[autodoc]] RagTokenForGeneration    - forward    - generate</pt><tf>## TFRagModel[[autodoc]] TFRagModel    - call## TFRagSequenceForGeneration[[autodoc]] TFRagSequenceForGeneration    - call\n",
            "Score\t 0.8081449020602206\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 68528fe3-e101-4358-b3db-f0894346b348\n",
            "Title\t Fine-Tuning LLMs with Synthetic Data for High-Quality Content Generation\n",
            "Text\t during the training sessions  and this is usually true for the entire machine learning field. As the training process for LLMs is resource-intensive  costly  and time-consuming  it happens only at intervals of months (sometimes more)  and the model knowledge quickly becomes outdated. Frequent custom fine-tuning cycles are an option  but beyond being expensive  doing so indiscriminately can lead to a problem known as Catastrophic Forgetting (Catastrophic inferencing is also a common term for this phenomenon)  where the models forget previously learned knowledge. Plus  the models dont have access to real-time data. A more viable solution to deal with this is RAG.   RAG stands for Retrieval Augmented Generation  the name given to a family of processes that focuses on connecting the LLM to external sources through retrieval mechanisms. A combination of the generative capabilities of the model with the ability to search for and incorporate relevant information from one knowledge base (or several).   There are different ways of classifying such systems  but most of them vary based on a few factors:   Source of Information: Those sources can be literally anything from traditional databases  vector databases  knowledge graphs  to the internet itself.Retrieval Mechanism: As the sources are so varied  the same is true for the methods used to collect information  such as search engines  APIs  customized database searches  etc.Integration Method: It is also common to classify RAG systems based on how they are incorporated with the LLM to generate the completion process.I will only focus on explaining the difference in the integration logic in this article  as it was the only noticeable change I made regarding the original prototype.   The RAG mechanism can be integrated as soon as the user prompts the input BEFORE the information reaches the LLM for completion. In this case  the RAG process happens every time a new input prompt is entered by the user  and the results of this process are used to enhance the user prompt by the time it hits the model.   Or the RAG process can occur AFTER the prompt reaches the LLM. In this scenario  the model is used as a reasoning engine to decide whether it needs to trigger RAG processes or not (and what mechanisms to use) to generate the appropriate completion based on the perceivable context. This process is usually known as Agentic\n",
            "Score\t 0.8027662815663229\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 5f8d4c91-bd53-4a7a-b1f7-d8f74debb425\n",
            "Title\t RAG\n",
            "Text\t # RAG<div class=\"flex flex-wrap space-x-1\"><a href=\"https://huggingface.co/models?filter=rag\"><img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-rag-blueviolet\"></a></div>## OverviewRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) andsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generateoutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowingboth retrieval and generation to adapt to downstream tasks.It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, VladimirKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.The abstract from the paper is the following:*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achievestate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and preciselymanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behindtask-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledgeremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametricmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive\n",
            "Score\t 0.8017680751522839\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CqadBlFOq_bM"
      },
      "id": "CqadBlFOq_bM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **IMPORTANT NOTE** It is better to use the distilled LLM, which is the mini version of the Master (teacher) model, this reduces the cost and letency.\n",
        "\n",
        "> Normally we prefer GPTs model as judge"
      ],
      "metadata": {
        "id": "pkQG7k5Qq4DB"
      },
      "id": "pkQG7k5Qq4DB"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "63b37b367a6444a8a3dda72d20a8dd8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e266f04dab724832af8376c9c3c46239",
              "IPY_MODEL_e8caea8a342c4750bfee0fc12261191b",
              "IPY_MODEL_09045bf02d104361a51b9f86ae3d7a8c"
            ],
            "layout": "IPY_MODEL_7505028812f14fb68832912b48f811db"
          }
        },
        "e266f04dab724832af8376c9c3c46239": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c14c37ffad664ad1a9ba8a45f47725ce",
            "placeholder": "​",
            "style": "IPY_MODEL_a04346f7ef7446b08727ec13454aa22a",
            "value": "modules.json: 100%"
          }
        },
        "e8caea8a342c4750bfee0fc12261191b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45d31cf4633c45bd8e28c39fd3ae4a9b",
            "max": 387,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1bd1694650654dc6b25b6f65352c40eb",
            "value": 387
          }
        },
        "09045bf02d104361a51b9f86ae3d7a8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3be869dcbc7442699267b0b32c11a9b4",
            "placeholder": "​",
            "style": "IPY_MODEL_a247073dcf7f46df95ed951a6b24a2f2",
            "value": " 387/387 [00:00&lt;00:00, 46.5kB/s]"
          }
        },
        "7505028812f14fb68832912b48f811db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c14c37ffad664ad1a9ba8a45f47725ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a04346f7ef7446b08727ec13454aa22a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45d31cf4633c45bd8e28c39fd3ae4a9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bd1694650654dc6b25b6f65352c40eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3be869dcbc7442699267b0b32c11a9b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a247073dcf7f46df95ed951a6b24a2f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b09991e2a380497ab401b5c820b7fa6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ffef672d80f940feb5ba6314cdfa6f2d",
              "IPY_MODEL_100855218c154b2fa953fc2be4b77034",
              "IPY_MODEL_fb6a06485e4e4efd8760a6251c9e264b"
            ],
            "layout": "IPY_MODEL_d35bf76a4a6f40f4b39daae5ba4d6469"
          }
        },
        "ffef672d80f940feb5ba6314cdfa6f2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce1ea020073c4586bd9b75fc5f14943f",
            "placeholder": "​",
            "style": "IPY_MODEL_0869e45ac4784ee585a8e0800cd47b9c",
            "value": "README.md: "
          }
        },
        "100855218c154b2fa953fc2be4b77034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9a31c1ae9d84eb4b390517b1d2c75b8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35bb258ca0d9455791f5c6a6395fd094",
            "value": 1
          }
        },
        "fb6a06485e4e4efd8760a6251c9e264b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95ce0df4e77e4b67a9f03f0dc4f21262",
            "placeholder": "​",
            "style": "IPY_MODEL_67a831d434954c5bb81d804dd41bef93",
            "value": " 67.8k/? [00:00&lt;00:00, 5.78MB/s]"
          }
        },
        "d35bf76a4a6f40f4b39daae5ba4d6469": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce1ea020073c4586bd9b75fc5f14943f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0869e45ac4784ee585a8e0800cd47b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9a31c1ae9d84eb4b390517b1d2c75b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "35bb258ca0d9455791f5c6a6395fd094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95ce0df4e77e4b67a9f03f0dc4f21262": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67a831d434954c5bb81d804dd41bef93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "410a512142dc4a819ea2f2e284a5a860": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f648c32588e8490daba3bd90b3aeaf74",
              "IPY_MODEL_7eceead8f49840f9a126a5456738972a",
              "IPY_MODEL_bf500b55ffb0430c806012e567101305"
            ],
            "layout": "IPY_MODEL_eddf209692284c5cb5776125236b1b08"
          }
        },
        "f648c32588e8490daba3bd90b3aeaf74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ffeeeb2593c4f8eaad56973f7fedaf6",
            "placeholder": "​",
            "style": "IPY_MODEL_bcc3bf34e9a54c229ea0ec2d8c21e9c9",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "7eceead8f49840f9a126a5456738972a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_032bd5e527d444e29cc0f4e5759d4a1c",
            "max": 57,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06a521b937ea40cfa0696e107040fec9",
            "value": 57
          }
        },
        "bf500b55ffb0430c806012e567101305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d52796c05b134c389394886f6dbb9737",
            "placeholder": "​",
            "style": "IPY_MODEL_f21387dd45574c2893b924c1d618774c",
            "value": " 57.0/57.0 [00:00&lt;00:00, 3.98kB/s]"
          }
        },
        "eddf209692284c5cb5776125236b1b08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ffeeeb2593c4f8eaad56973f7fedaf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcc3bf34e9a54c229ea0ec2d8c21e9c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "032bd5e527d444e29cc0f4e5759d4a1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06a521b937ea40cfa0696e107040fec9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d52796c05b134c389394886f6dbb9737": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f21387dd45574c2893b924c1d618774c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e2e0eaed0aa408cb7fb9411a29fb881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_faeeb8b41a944e08a7d6aa6f391a15ee",
              "IPY_MODEL_cbe1c9aa48ba4bfb95b291370ffd7b53",
              "IPY_MODEL_779c530b2c6a4a719a5a1cba9dc710fc"
            ],
            "layout": "IPY_MODEL_5fb565d1603949aab7376c2d04e828d5"
          }
        },
        "faeeb8b41a944e08a7d6aa6f391a15ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_844a1b8d92764742a6c88750185a3ed0",
            "placeholder": "​",
            "style": "IPY_MODEL_b055dfe041ec4a8fa25aeb012878a501",
            "value": "config.json: 100%"
          }
        },
        "cbe1c9aa48ba4bfb95b291370ffd7b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a70c7570100d4d9d8f156945bc48807f",
            "max": 615,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c116a343988c47ae94a2b14fbd5f2700",
            "value": 615
          }
        },
        "779c530b2c6a4a719a5a1cba9dc710fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e73396e8b7454fcb9a81bfd5bfab5ffb",
            "placeholder": "​",
            "style": "IPY_MODEL_2d7a5e2904a240e7bf0cbbefd59f0fb0",
            "value": " 615/615 [00:00&lt;00:00, 36.6kB/s]"
          }
        },
        "5fb565d1603949aab7376c2d04e828d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "844a1b8d92764742a6c88750185a3ed0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b055dfe041ec4a8fa25aeb012878a501": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a70c7570100d4d9d8f156945bc48807f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c116a343988c47ae94a2b14fbd5f2700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e73396e8b7454fcb9a81bfd5bfab5ffb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d7a5e2904a240e7bf0cbbefd59f0fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95c69573bea34ce78381092144a4dbaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36cdf7a638174a259249094dfd49b9f6",
              "IPY_MODEL_48c959a66a934d759ffeed98ba647dfe",
              "IPY_MODEL_98e6d146b6724485b54aea15b2aaaac7"
            ],
            "layout": "IPY_MODEL_e069769875194ceeb61569e4fc847a6c"
          }
        },
        "36cdf7a638174a259249094dfd49b9f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bc120c66a71417f97b89e5cc3ca2370",
            "placeholder": "​",
            "style": "IPY_MODEL_c93ef644a58d403eab0ceed21b752d03",
            "value": "model.safetensors: 100%"
          }
        },
        "48c959a66a934d759ffeed98ba647dfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d985e97ce7743a99e5143b8adbe981a",
            "max": 133466304,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62d24eebf060474d817332e76661bca9",
            "value": 133466304
          }
        },
        "98e6d146b6724485b54aea15b2aaaac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9926ecc1fe4a43db89a9c64bf3d07000",
            "placeholder": "​",
            "style": "IPY_MODEL_5e5bfb70e16d4262ba206972c24472a8",
            "value": " 133M/133M [00:01&lt;00:00, 162MB/s]"
          }
        },
        "e069769875194ceeb61569e4fc847a6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bc120c66a71417f97b89e5cc3ca2370": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c93ef644a58d403eab0ceed21b752d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d985e97ce7743a99e5143b8adbe981a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62d24eebf060474d817332e76661bca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9926ecc1fe4a43db89a9c64bf3d07000": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e5bfb70e16d4262ba206972c24472a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e58c88a70c7e4556a7f9ae89315fc3e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_231388f5f4e946898b9bd44d435667e2",
              "IPY_MODEL_8b755d22787a48c0a56bcb2199a16a01",
              "IPY_MODEL_b6a71d3b744f414db640a205bc2046d3"
            ],
            "layout": "IPY_MODEL_81be6ae07f3c4a6188c473207fc6ef17"
          }
        },
        "231388f5f4e946898b9bd44d435667e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d555092bf374bd39e8f5cbb533b55ac",
            "placeholder": "​",
            "style": "IPY_MODEL_a990d167611d464b9cdab126d2542608",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "8b755d22787a48c0a56bcb2199a16a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b373c5e4a0b04bfa8db2a3e179b55289",
            "max": 314,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_baec7b704b3448238dab1c3168c5d555",
            "value": 314
          }
        },
        "b6a71d3b744f414db640a205bc2046d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e368c6db1d4c4963a22c6249e7588280",
            "placeholder": "​",
            "style": "IPY_MODEL_0c12c88452a441049bc968c506cfa9dc",
            "value": " 314/314 [00:00&lt;00:00, 35.0kB/s]"
          }
        },
        "81be6ae07f3c4a6188c473207fc6ef17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d555092bf374bd39e8f5cbb533b55ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a990d167611d464b9cdab126d2542608": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b373c5e4a0b04bfa8db2a3e179b55289": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baec7b704b3448238dab1c3168c5d555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e368c6db1d4c4963a22c6249e7588280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c12c88452a441049bc968c506cfa9dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ca343c625bf4979b8deef05ef2c4e45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3dd906c86f0f4dedb1992c566965ef27",
              "IPY_MODEL_ce42b3545fcb4336b3ebf3e1421fc512",
              "IPY_MODEL_378b6e6cf7b84351b6bea508f28007e6"
            ],
            "layout": "IPY_MODEL_9c2e2797d5bb4b368f55428b6283b491"
          }
        },
        "3dd906c86f0f4dedb1992c566965ef27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5306c45517624d57adba617eadce0b3e",
            "placeholder": "​",
            "style": "IPY_MODEL_1250b448bd494b15bcf3381db62e5291",
            "value": "vocab.txt: "
          }
        },
        "ce42b3545fcb4336b3ebf3e1421fc512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27d19f75c57d4800961024da0f708112",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_663bb6598b1646cca1f1c0bcb33e083b",
            "value": 1
          }
        },
        "378b6e6cf7b84351b6bea508f28007e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9c7b3c5101d411f8c5f91ac0acaaa7b",
            "placeholder": "​",
            "style": "IPY_MODEL_2888863a19254d8f80232c8044ae1374",
            "value": " 232k/? [00:00&lt;00:00, 7.83MB/s]"
          }
        },
        "9c2e2797d5bb4b368f55428b6283b491": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5306c45517624d57adba617eadce0b3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1250b448bd494b15bcf3381db62e5291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27d19f75c57d4800961024da0f708112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "663bb6598b1646cca1f1c0bcb33e083b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9c7b3c5101d411f8c5f91ac0acaaa7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2888863a19254d8f80232c8044ae1374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20f7dbea018a4866b74bc3da34fa53c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cfa57ac8b553438988b75008c8d18b01",
              "IPY_MODEL_e5abb1244a1a4e2f911677b7ab20cd34",
              "IPY_MODEL_dd5e67cf4fbb4da8b9f60577cf3cbde7"
            ],
            "layout": "IPY_MODEL_1678aab067cc43ac8c68bad1f422c758"
          }
        },
        "cfa57ac8b553438988b75008c8d18b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_552e308b81ec48348193b0231998769e",
            "placeholder": "​",
            "style": "IPY_MODEL_030d93aa9c0a45d19aa74016276aeac5",
            "value": "tokenizer.json: "
          }
        },
        "e5abb1244a1a4e2f911677b7ab20cd34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cbee76d20824d8c812149de4c93c3ff",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6568beabe11c428999cd9e52d172f7b9",
            "value": 1
          }
        },
        "dd5e67cf4fbb4da8b9f60577cf3cbde7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cafc57f9f2164fe99a9bb9ca5c9b1a7a",
            "placeholder": "​",
            "style": "IPY_MODEL_71638c8310d649eb819bf610af1a4872",
            "value": " 711k/? [00:00&lt;00:00, 43.2MB/s]"
          }
        },
        "1678aab067cc43ac8c68bad1f422c758": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "552e308b81ec48348193b0231998769e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "030d93aa9c0a45d19aa74016276aeac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8cbee76d20824d8c812149de4c93c3ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6568beabe11c428999cd9e52d172f7b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cafc57f9f2164fe99a9bb9ca5c9b1a7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71638c8310d649eb819bf610af1a4872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba370583d423493e96a17b348d46787e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_340383e8940c4840ad8a25140792a327",
              "IPY_MODEL_1e88f0958393489bbc5b043d61b06c93",
              "IPY_MODEL_4ca08dcc21f84d088b2def1ea5cd3d78"
            ],
            "layout": "IPY_MODEL_cc42ec88c7134c8db3dd7fd995a7c3db"
          }
        },
        "340383e8940c4840ad8a25140792a327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4af9f80f58da4e3fad6398da6d6c30b4",
            "placeholder": "​",
            "style": "IPY_MODEL_b73d5e3b429545188acea813ed4cbfe5",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "1e88f0958393489bbc5b043d61b06c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_783bcf99b0f3459993964065c62c3ba6",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2793d1af1fa44e9a1ee787bb2872c97",
            "value": 125
          }
        },
        "4ca08dcc21f84d088b2def1ea5cd3d78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_450d3379f17a43d98aef0fdc56ba7bdd",
            "placeholder": "​",
            "style": "IPY_MODEL_0a6a26f6ffde40d9aabc985ec96d40a1",
            "value": " 125/125 [00:00&lt;00:00, 14.8kB/s]"
          }
        },
        "cc42ec88c7134c8db3dd7fd995a7c3db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4af9f80f58da4e3fad6398da6d6c30b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b73d5e3b429545188acea813ed4cbfe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "783bcf99b0f3459993964065c62c3ba6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2793d1af1fa44e9a1ee787bb2872c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "450d3379f17a43d98aef0fdc56ba7bdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a6a26f6ffde40d9aabc985ec96d40a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2180c7c96d3f44cfbd423b9b432555b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c43cc94974545c5856fa367b6db6681",
              "IPY_MODEL_9853618bcbca44ff8f9e129835f380b0",
              "IPY_MODEL_2e690af96402456b9c86f3a8f3a19bac"
            ],
            "layout": "IPY_MODEL_0aef0c6acfb1455198927e573ad7504f"
          }
        },
        "2c43cc94974545c5856fa367b6db6681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37cf22ec15c445869f29bbaf8a88e033",
            "placeholder": "​",
            "style": "IPY_MODEL_7377daea0aee47efbfd5db7ecabd1203",
            "value": "config.json: 100%"
          }
        },
        "9853618bcbca44ff8f9e129835f380b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0fe87234da14c33bc0ba4973395e375",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31668d254d7c43bdb37d62d503974aa6",
            "value": 200
          }
        },
        "2e690af96402456b9c86f3a8f3a19bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a96270bb5962414e956897c78eaae81a",
            "placeholder": "​",
            "style": "IPY_MODEL_03acfa0ba1b64db496bd84fdc272fef4",
            "value": " 200/200 [00:00&lt;00:00, 22.9kB/s]"
          }
        },
        "0aef0c6acfb1455198927e573ad7504f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37cf22ec15c445869f29bbaf8a88e033": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7377daea0aee47efbfd5db7ecabd1203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0fe87234da14c33bc0ba4973395e375": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31668d254d7c43bdb37d62d503974aa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a96270bb5962414e956897c78eaae81a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03acfa0ba1b64db496bd84fdc272fef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}