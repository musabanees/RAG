{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e75806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "481f11bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chroma_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m chomra_client = chromadb.PersistentClient(path=\u001b[33m\"\u001b[39m\u001b[33m./mini-llama-articles\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# You are creating a specific bucket named \"mini-llama-articles\" to hold this specific set of data.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m chroma_collection = \u001b[43mchroma_client\u001b[49m.create_collection(\u001b[33m\"\u001b[39m\u001b[33mmini-llama-articles\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'chroma_client' is not defined"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# PersistentClient: This is the key part. It tells the system: \"Don't just keep this in RAM (memory). Save it to the hard drive.\n",
    "chomra_client = chromadb.PersistentClient(path=\"./mini-llama-articles\")\n",
    "# You are creating a specific bucket named \"mini-llama-articles\" to hold this specific set of data.\n",
    "chroma_collection = chroma_client.create_collection(\"mini-llama-articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6680ed8",
   "metadata": {},
   "source": [
    "`BAAI/bge-small-en-v1.5` has longer context size, if you see the `22max_position_embeddings`\n",
    "https://huggingface.co/BAAI/bge-small-en-v1.5/blob/main/config.json#:~:text=%22max_position_embeddings%22%3A%20512%2C\n",
    "\n",
    "For the simple RAG project notes, why `BAAI/bge-small-en-v1.5` is the ideal choice:\n",
    "\n",
    "### 1. High Performance with Minimal Resource Cost\n",
    "The `bge-small-en-v1.5` model strikes an incredible balance between accuracy and efficiency. Despite being a \"small\" model (only ~33 million parameters) that can run quickly on a standard CPU with just ~130MB of RAM, it consistently outperforms much larger models (like OpenAI’s older Ada-002) on industry-standard benchmarks like MTEB. This means you get production-quality retrieval—finding the right documents for your query—without needing expensive GPUs or paying for API calls, making it perfect for local development and testing.\n",
    "\n",
    "### 2. Optimized for RAG Workflows\n",
    "Unlike older sentence transformers (like `all-MiniLM-L6-v2`) that were designed for short sentences (max 256 tokens), this model is specifically optimized for Retrieval-Augmented Generation (RAG). It supports a **512-token context window**, allowing it to \"read\" and index half-page paragraphs without losing as much context. Additionally, the `v1.5` update fixed the \"similarity distribution\" issue, meaning the scores it assigns to documents are more meaningful, allowing your system to better distinguish between a \"perfect match\" and a \"kind of relevant\" document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7fc9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:3b\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08aace0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 1024\n",
      "Chunk overlap: 200\n",
      "LLM: callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002E9387826C0> system_prompt=None messages_to_prompt=<function messages_to_prompt at 0x000002E9165F9EE0> completion_to_prompt=<function default_completion_to_prompt at 0x000002E9179198A0> output_parser=None pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'> query_wrapper_prompt=None base_url='http://localhost:11434' model='llama3.2:3b' temperature=None context_window=131072 request_timeout=30.0 prompt_key='prompt' json_mode=False additional_kwargs={} is_function_calling_model=True keep_alive=None thinking=None\n",
      "Embedding model: model_name='BAAI/bge-small-en-v1.5' embed_batch_size=10 callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002E9387826C0> num_workers=None embeddings_cache=None max_length=512 normalize=True query_instruction=None text_instruction=None cache_folder=None show_progress_bar=False\n",
      "Tokenizer: functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all')\n",
      "Context Window: 131072\n",
      "Number of output tokens: 256\n"
     ]
    }
   ],
   "source": [
    "# Display the Setting Settings present in LlamaIndex\n",
    "print(\"Chunk size:\", Settings.chunk_size)\n",
    "print(\"Chunk overlap:\", Settings.chunk_overlap)\n",
    "\n",
    "print(\"LLM:\", Settings.llm)\n",
    "print(\"Embedding model:\", Settings.embed_model)\n",
    "print(\"Tokenizer:\", Settings.tokenizer )\n",
    "\n",
    "print(\"Context Window:\", Settings.context_window)\n",
    "print(\"Number of output tokens:\", Settings.num_output )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a456a2ea",
   "metadata": {},
   "source": [
    "# 2️⃣ Breaking it down\n",
    "``` LlamaIndex Workflows (an event-driven architecture). In that system, functions don't call each other directly; they communicate by throwing \"Events\" back and forth.```\n",
    "\n",
    "### **A. `Event`**\n",
    "\n",
    "* `Event` is a **base class in LlamaIndex workflows**.\n",
    "* Workflows in LlamaIndex are **step-based pipelines**, and **events** are the objects that pass between steps.\n",
    "* Examples of events:\n",
    "\n",
    "  * `StartEvent` → triggers the workflow\n",
    "  * `StopEvent` → indicates a step has finished\n",
    "* You can define **custom events** to carry specific data between steps.\n",
    "\n",
    "---\n",
    "\n",
    "### **B. `NodeWithScore`**\n",
    "\n",
    "* Each `NodeWithScore` is a **document chunk + similarity score**.\n",
    "* When you retrieve documents from your vector database, you get:\n",
    "\n",
    "  * The **chunk of text** (Node)\n",
    "  * Its **relevance score** (Score)\n",
    "\n",
    "So `NodeWithScore` represents **a retrieved document and how relevant it is to the query**.\n",
    "\n",
    "---\n",
    "\n",
    "### **C. `RetrieverEvent`**\n",
    "\n",
    "* This is a **custom event** that will hold the **results of the retrieval step** in your RAG workflow.\n",
    "* By defining:\n",
    "\n",
    "```python\n",
    "nodes: list[NodeWithScore]\n",
    "```\n",
    "\n",
    "You are saying:\n",
    "\n",
    "> “This event will carry a list of retrieved nodes (documents) with their similarity scores.”\n",
    "\n",
    "# 4️⃣ How it fits into a RAG workflow\n",
    "\n",
    "```\n",
    "StartEvent(query)\n",
    "       │\n",
    "       ▼\n",
    "Retrieve Step\n",
    "       │\n",
    "       ▼\n",
    "RetrieverEvent(nodes=[NodeWithScore, ...])\n",
    "       │\n",
    "       ▼\n",
    "Synthesize Step (uses nodes to generate answer)\n",
    "```\n",
    "\n",
    "* `RetrieverEvent` is **just a container**\n",
    "* Carries **retrieved text chunks + scores** from retrieval → synthesis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b760fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "class RetrieverEvent(Event):\n",
    "    \"\"\"Result of running retrieval\"\"\"\n",
    "\n",
    "    nodes: list[NodeWithScore]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f36ac",
   "metadata": {},
   "source": [
    "## The `ctx: Context` used here.\n",
    "Context exists for more advanced workflows, and you are already one small step away from using it meaningfully.\n",
    "Context is a shared, persistent state container for a single workflow run.\n",
    "\n",
    "### 1. What is ctx? (The Shared Brain)\n",
    "In a Workflow, every step (ingest, retrieve, synthesize) is an isolated island.\n",
    "* The retrieve function has no idea what variables exist inside ingest.\n",
    "* The synthesize function has no idea what variables exist inside retrieve\n",
    "\n",
    "ctx (Context) is the shared memory that connects these islands. It is a global storage box that lives for the entire duration of the workflow run. If you put something in the box during Step 1, you can take it out in Step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d85f93d",
   "metadata": {},
   "source": [
    "## This method is the Vector Search (Finds answers even if words don't match exactly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.response_synthesizers import CompactAndRefine\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step\n",
    ")\n",
    "\n",
    "class RAGWorkflow(Workflow):\n",
    "    @step\n",
    "    async def ingest(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n",
    "        dirname = ev.get(\"dirname\")\n",
    "        if not dirname:\n",
    "            return None\n",
    "        \n",
    "        documents = SimpleDirectoryReader(dirname).load_data()\n",
    "        # Vector Store Retriever (also known as Dense Retrieval or Semantic Search)\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents=documents\n",
    "        )\n",
    "        return StopEvent(result=index)\n",
    "    \n",
    "    @step\n",
    "    async def retrieve(self, ctx: Context, ev: StartEvent) -> RetrieverEvent | None:\n",
    "        \"\"\" Retrieve relevant documents from the index based on the query. \"\"\"\n",
    "        # Both values come from the StartEvent, not Context\n",
    "        query = ev.get(\"query\")\n",
    "        index = ev.get(\"index\")\n",
    "\n",
    "        if not query:\n",
    "            return None\n",
    "        \n",
    "        print(f\"Retrieving documents for query: {query}\")\n",
    "\n",
    "        await ctx.store.set(\"query\", query)\n",
    "\n",
    "        if index is None:\n",
    "            print(\"Index is empty, load some documents before querying!\")\n",
    "            return None\n",
    "\n",
    "        retriever = index.as_retriever( similarity_top_k=2 )\n",
    "        nodes = await retriever.aretrieve(query)\n",
    "        print(f\"Retrieved {len(nodes)} documents.\")\n",
    "        print((\"Document Retrieved:\"))\n",
    "        for node in nodes:\n",
    "            print(\"-----\"*10)\n",
    "            print(node.get_text())\n",
    "        return RetrieverEvent(nodes=nodes)\n",
    "\n",
    "\n",
    "# When the code gets to synthesize, it has the answers (nodes), but it has forgotten the question (query).\n",
    "# Without ctx, the LLM would receive 5 paragraphs of text but wouldn't know what question to answer about them.\n",
    "# ctx bridges this gap by teleporting the query variable from the first step to the last step.\n",
    "    @step\n",
    "    async def synthesize(self, ctx: Context, ev: RetrieverEvent) -> StopEvent:\n",
    "        \"\"\"Return a streaming response using reranked nodes.\"\"\"\n",
    "        summarizer = CompactAndRefine(streaming=True, verbose=True)\n",
    "        query = await ctx.store.get(\"query\", default=None)\n",
    "\n",
    "        response = await summarizer.asynthesize(query, nodes=ev.nodes)\n",
    "        return StopEvent(result=response)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22ffd1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = RAGWorkflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ba3bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest the documents and only ingest function runs\n",
    "index = await w.run(dirname=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e713aad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: How was DeepSeekR1 trained?\n",
      "Retrieved 2 documents.\n",
      "Document contents:\n",
      "--------------------------------------------------\n",
      "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n",
      "Reinforcement Learning\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\n",
      "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\n",
      "vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\n",
      "Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\n",
      "reasoning behaviors. However, it encounters challenges such as poor readability, and language\n",
      "mixing. To address these issues and further enhance reasoning performance, we introduce\n",
      "DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\n",
      "R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\n",
      "research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\n",
      "(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\n",
      "AIME 2024\n",
      "(Pass@1)\n",
      "Codeforces\n",
      "(Percentile)\n",
      "GPQA Diamond\n",
      "(Pass@1)\n",
      "MATH-500\n",
      "(Pass@1)\n",
      "MMLU\n",
      "(Pass@1)\n",
      "SWE-bench Verified\n",
      "(Resolved)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100Accuracy / Percentile (%)\n",
      "79.8\n",
      "96.3\n",
      "71.5\n",
      "97.3\n",
      "90.8\n",
      "49.2\n",
      "79.2\n",
      "96.6\n",
      "75.7\n",
      "96.4\n",
      "91.8\n",
      "48.9\n",
      "72.6\n",
      "90.6\n",
      "62.1\n",
      "94.3\n",
      "87.4\n",
      "36.8\n",
      "63.6\n",
      "93.4\n",
      "60.0\n",
      "90.0\n",
      "85.2\n",
      "41.6\n",
      "39.2\n",
      "58.7 59.1\n",
      "90.2\n",
      "88.5\n",
      "42.0\n",
      "DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\n",
      "Figure 1 |Benchmark performance of DeepSeek-R1.\n",
      "arXiv:2501.12948v1  [cs.CL]  22 Jan 2025\n",
      "--------------------------------------------------\n",
      "This im-\n",
      "provement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-\n",
      "icant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1\n",
      "excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis\n",
      "capabilities. This highlights the potential of reasoning models in AI-driven search and data\n",
      "analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\n",
      "demonstrating its capability in handling fact-based queries. A similar trend is observed where\n",
      "OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than\n",
      "DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\n",
      "answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an\n",
      "accuracy of over 70%.\n",
      "DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\n",
      "model’s ability to follow format instructions. These improvements can be linked to the inclusion\n",
      "of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\n",
      "training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,\n",
      "indicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering. Its\n",
      "significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale\n",
      "RL, which not only boosts reasoning capabilities but also improves performance across diverse\n",
      "domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\n",
      "average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\n",
      "13\n",
      "\n",
      "Final response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 15:50:53,879 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-R1 was trained using a multi-stage approach, which includes supervised fine-tuning (SFT) followed by large-scale reinforcement learning (RL). The model was initially trained with cold-start data before undergoing RL training. This combination of SFT and RL appears to have significantly enhanced the reasoning capabilities of DeepSeek-R1."
     ]
    }
   ],
   "source": [
    "# Run a query and only retrieve function runs\n",
    "result = await w.run(query=\"How was DeepSeekR1 trained?\", index=index)\n",
    "print(\"\\nFinal response:\")\n",
    "async for chunk in result.async_response_gen():\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfdf5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9930c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d343c71e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
