{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a932fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import nest_asyncio\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "FIRECRAWL_API_KEY = os.getenv(\"FIRECRAWL_API_KEY\")\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f78923",
   "metadata": {},
   "source": [
    "## Scrape vs. Crawl\n",
    "The mode parameter in the FireCrawlWebReader initialization specifies how the web content should be processed. There are two main modes: Crawling and scraping are related but distinct processes in web data extraction: \n",
    "\n",
    "* Crawling (crawl)involves systematically navigating through a website, and following links to discover and collect data from multiple pages. It’s ideal for gathering comprehensive datasets from an entire site or a section of it. For example, crawling can retrieve all blog posts from a website. \n",
    "* Scraping (scrape) focuses on extracting specific data from a single webpage or a limited set of pages. It’s best for targeted data collection, such as pulling the title and description from a single product page. \n",
    "\n",
    "In short, crawling is about exploring multiple pages, while scraping is about extracting data from specific pages. Choosing the appropriate mode depends on your specific use case and the amount of data you need to extract from the target website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b642300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.web import FireCrawlWebReader\n",
    "\n",
    "# Initialize the FireCrawl web reader\n",
    "reader = FireCrawlWebReader(\n",
    "    api_key=FIRECRAWL_API_KEY,\n",
    "    mode=\"scrape\",\n",
    "    params={\"formats\": [\"markdown\"]} # You can also use \"html\" here, I had issue related to max chunk, this could be added if embedding model supports larger chunks\n",
    ")\n",
    "# Markdown format - The content of the webpage converted into Markdown syntax (e.g., # Title, **bold**, [link](url)).\n",
    "# HTML format - The raw HTML content of the webpage.\n",
    "# https://docs.firecrawl.dev/features/scrape#scrape-formats\n",
    "\n",
    "documents = reader.load_data(url=\"https://towardsai.net/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d71efbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='a709554a-0c3e-471b-8b92-f20909f1d800', embedding=None, metadata={'title': 'Towards AI', 'description': 'Master AI with Towards AI. We offer practical AI Engineering courses, corporate AI bootcamps, and LLM development consultancy for individuals and companies.', 'url': 'https://towardsai.net/', 'language': 'en-US', 'keywords': None, 'robots': 'max-image-preview:large', 'og_title': 'Towards AI', 'og_description': 'Master AI with Towards AI. We offer practical AI Engineering courses, corporate AI bootcamps, and LLM development consultancy for individuals and companies.', 'og_url': 'https://towardsai.net/', 'og_image': 'https://towardsai.net/wp-content/uploads/2024/09/towards-ai-og-graph.jpg', 'og_audio': None, 'og_determiner': None, 'og_locale': None, 'og_locale_alternate': None, 'og_site_name': None, 'og_video': None, 'favicon': 'https://b3688296.smushcdn.com/3688296/wp-content/uploads/2019/05/cropped-towards-ai-square-circle-png-32x32.png?lossy=0&strip=1&webp=1', 'dc_terms_created': None, 'dc_date_created': None, 'dc_date': None, 'dc_terms_type': None, 'dc_type': None, 'dc_terms_audience': None, 'dc_terms_subject': None, 'dc_subject': None, 'dc_description': None, 'dc_terms_keywords': None, 'modified_time': None, 'published_time': None, 'article_tag': None, 'article_section': None, 'source_url': 'https://towardsai.net/', 'status_code': 200, 'scrape_id': '019b66e9-aefb-70d1-ae04-9913ac5cb01e', 'num_pages': None, 'content_type': 'text/html; charset=UTF-8', 'proxy_used': 'basic', 'timezone': None, 'cache_state': 'hit', 'cached_at': '2025-12-28T21:05:57.533Z', 'credits_used': 1, 'concurrency_limited': False, 'concurrency_queue_duration_ms': None, 'error': None, 'generator': ['WordPress 6.9', 'Powered by WPBakery Page Builder - drag and drop page builder for WordPress.'], 'fb:app_id': '1166924400135297', 'og:url': 'https://towardsai.net/', 'foundingDate': '2019-05-01', 'twitter:site': 'towards_ai', 'bestRating': '5', 'viewport': 'width=device-width, initial-scale=1', 'og:image': 'https://towardsai.net/wp-content/uploads/2024/09/towards-ai-og-graph.jpg', 'msapplication-TileColor': '#c1ebff', 'og:title': 'Towards AI', 'worstRating': '1', 'twitter:description': 'Master AI with Towards AI. We offer practical AI Engineering courses, corporate AI bootcamps, and LLM development consultancy for individuals and companies.', 'twitter:image': 'https://towardsai.net/wp-content/uploads/2024/09/towards-ai-og-graph.jpg', 'twitter:title': 'Towards AI', 'twitter:card': 'summary_large_image', 'msapplication-TileImage': 'https://towardsai.net/wp-content/uploads/2019/05/cropped-towards-ai-square-circle-png-270x270.png', 'theme-color': '#c1ebff', 'target': 'https://towardsai.net/?s={search_term_string}', 'og:description': 'Master AI with Towards AI. We offer practical AI Engineering courses, corporate AI bootcamps, and LLM development consultancy for individuals and companies.', 'og:image:width': '1440', 'og:image:height': '810', 'og:type': 'website'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='- [Latest](https://towardsai.net/p/)\\n- [Trends](https://medium.com/towards-artificial-intelligence/trending)\\n- [Shop](https://gumroad.com/towardsai)\\n\\n[Google News](https://news.google.com/publications/CAAqBwgKMNiLmgswgpayAw)[Medium](https://pub.towardsai.net/)[Linkedin](https://linkedin.com/company/towards-artificial-intelligence)[Twitter](https://twitter.com/towards_ai?lang=en)[Facebook](https://www.facebook.com/towardsAl/)[Instagram](https://instagram.com/towards_ai/)[Github](https://github.com/towardsai)[RSS Feed](http://feeds.feedburner.com/towards-ai)[Email us](mailto:pub@towardsai.net)\\n\\nName: Towards AI\\nLegal Name: Towards AI, Inc.\\nDescription: Towards AI is the world\\'s leading artificial intelligence (AI) and technology publication. Read by thought-leaders and decision-makers around the world.\\nPhone Number: +1-650-246-9381\\nEmail: pub@towardsai.net\\n\\n228 Park Avenue SouthNew York,\\nNY10003United States\\n\\nWebsite: [https://towardsai.net/](https://towardsai.net/)\\nPublisher: [https://towardsai.net/#publisher](https://towardsai.net/#publisher)\\nDiversity Policy: [https://towardsai.net/about](https://towardsai.net/about)\\nEthics Policy: [https://towardsai.net/about](https://towardsai.net/about)\\nMasthead: [https://towardsai.net/about](https://towardsai.net/about)\\n\\nName: Towards AI\\nLegal Name: Towards AI, Inc.\\nDescription: Towards AI is the world\\'s leading artificial intelligence (AI) and technology publication.\\nFounders:\\nRoberto Iriondo,\\n[Website](https://www.robertoiriondo.com/),\\nJob Title: Co-founder and Advisor\\nWorks for: Towards AI, Inc.\\nFollow Roberto:\\n[X](https://x.com/@robiriondo),\\n[LinkedIn](https://www.linkedin.com/in/robiriondo),\\n[GitHub](https://github.com/robiriondo),\\n[Google Scholar](https://scholar.google.com/citations?user=dTn7NEcAAAAJ),\\n[Towards AI Profile](https://towardsai.net/p/author/robiriondo),\\n[Medium](https://medium.com/@robiriondo),\\n[ML@CMU](https://www.ml.cmu.edu/robiriondo),\\n[FreeCodeCamp](https://www.freecodecamp.org/news/author/robiriondo/),\\n[Crunchbase](https://www.crunchbase.com/person/roberto-iriondo),\\n[Bloomberg](https://www.bloomberg.com/profile/person/22994840),\\n[Roberto Iriondo, Generative AI Lab](https://roberto.generativeailab.org/),\\n[Generative AI Lab](https://generativeailab.org/) [VeloxTrend](https://veloxtrend.com/) [Ultrarix Capital Partners](https://ultrarix.com/)Denis Piffaretti,\\nJob Title: Co-founder\\nWorks for: Towards AI, Inc.Louie Peters,\\nJob Title: Co-founder\\nWorks for: Towards AI, Inc.Louis-François Bouchard,\\nJob Title: Co-founder\\nWorks for: Towards AI, Inc.\\n\\nCover:\\n\\n\\n![Towards AI Cover](https://b3688296.smushcdn.com/3688296/wp-content/uploads/2024/09/towards-ai-og-graph.jpg?lossy=0&strip=1&webp=1)\\n\\nLogo:\\n\\n\\n![Towards AI Logo](https://b3688296.smushcdn.com/3688296/wp-content/uploads/2019/05/towards-ai-square-circle-png.png?lossy=0&strip=1&webp=1)\\n\\nAreas Served: Worldwide\\nAlternate Name: Towards AI, Inc.\\nAlternate Name: Towards AI Co.\\nAlternate Name: towards ai\\nAlternate Name: towardsai\\nAlternate Name: towards.ai\\nAlternate Name: tai\\nAlternate Name: toward ai\\nAlternate Name: toward.ai\\nAlternate Name: Towards AI, Inc.\\nAlternate Name: towardsai.net\\nAlternate Name: pub.towardsai.net\\n\\nFollow us on:\\n[Facebook](https://www.facebook.com/towardsAl/ \"Facebook\") [X](https://x.com/towards_ai \"X\") [LinkedIn](https://www.linkedin.com/company/towards-artificial-intelligence \"LinkedIn\") [Instagram](https://www.instagram.com/towards_ai/ \"Instagram\") [Youtube](https://www.youtube.com/channel/UCQNjFuhOJM1YqFTPY1Q_kYQ \"Youtube\") [Github](https://github.com/towardsai \"Github\") [Google My Business](https://local.google.com/place?id=4955254490173856159&use=srp&ved=1t%3A65428&_ga=2.191706990.559569912.1635283323-985655235.1633987376#fpstate=lie \"Google my Business\") [Google Search](https://www.google.com/search?ved=1t:65428&_ga=2.191706990.559569912.1635283323-985655235.1633987376&q=Towards+AI&ludocid=4955254490173856159&lsig=AB86z5Ur3DZsSmdOFFUwd-bMHTIe#fpstate=lie \"Google Search\") [Google News](https://news.google.com/publications/CAAqBwgKMNiLmgswgpayAw?oc=3&ceid=US:en \"Google News\") [Google Maps](https://g.page/TowardsAI?gm \"Google Maps\") [Discord](https://discord.com/invite/V7RGX9XKAW \"Discord\") [Shop](https://gumroad.com/towardsai \"Shop\") [Towards AI, Medium Editorial](https://towardsai.medium.com/ \"Towards AI, Medium Editorial\") [Medium](https://medium.com/towards-artificial-intelligence \"Medium\") [Flipboard](https://flipboard.com/@Towards_AI \"Flipboard\") [Publication](https://pub.towardsai.net/ \"Publication\") [Feed](https://feed.towardsai.net/ \"AI Feed\") [Sponsors](https://sponsors.towardsai.net/ \"Sponsors\") [Sponsors](https://members.towardsai.net/ \"Members\") [Contribute](https://contribute.towardsai.net/ \"Contribute\")\\n\\n5 stars – based on\\n497 reviews\\n\\n\\n\\n\\n#### Frequently Used, Contextual References\\n\\nTODO: Remember to copy unique IDs whenever it needs used. i.e., URL: 304b2e42315e\\n\\n## Resources\\n\\nOur 15 AI experts built the most comprehensive, practical, 90+ lesson courses to master AI Engineering - we have pathways for any experience at [Towards AI Academy](https://academy.towardsai.net/?utm_source=taiwordpress&utm_medium=banner). Cohorts still open - use COHORT10 for 10% off.\\n\\n# Towards AI is the the leading AI community and content platform making AI accessible to all.\\n\\n### Welcome to Towards AI\\n\\n### Making AI & ML accessible to all\\n\\nWe have taught over 500,000 AI learners over the past 5 years.\\nNow we also offer full practical Generative AI courses!\\nTake our one-stop LLM Developer conversion course and help lead this new field.\\n\\n\\n[Sign Up for the Course](https://academy.towardsai.net/courses/beginner-to-advanced-llm-dev)\\n\\n[Watch the overview](https://www.youtube.com/watch?v=9No-FiEInLA)\\n\\n![startup-03-image-01](<Base64-Image-Removed>)\\n\\n### Updates on Towards AI\\n\\n### Latest Stories\\n\\nThe header\\n\\n[Making Gems with Google Docs](https://towardsai.net/p/machine-learning/making-gems-with-google-docs)\\n\\ntowardsai.net\\\\|Dec 9th 2025\\n\\n[Understanding XGBoost: A Deep Dive into the Algorithm](https://towardsai.net/p/machine-learning/understanding-xgboost-a-deep-dive-into-the-algorithm)\\n\\ntowardsai.net\\\\|Dec 9th 2025\\n\\n[How to Build Travel AI Agents Using Phidata and Qdrant](https://towardsai.net/p/machine-learning/how-to-build-travel-ai-agents-using-phidata-and-qdrant)\\n\\ntowardsai.net\\\\|Dec 9th 2025\\n\\n[Prompt Engineering Quick Guide](https://towardsai.net/p/machine-learning/prompt-engineering-quick-guide)\\n\\ntowardsai.net\\\\|Dec 9th 2025\\n\\n[FeedGrabbr](https://feedgrabbr.com/ \"FeedGrabbr\")\\n\\n[All Latest Stories](https://towardsai.net/p \"Follow our publication\")\\n\\n#### [Ui / ux Design](https://towardsai.net/\\\\#)\\n\\nGiving you the best possible experience when using websites and all kinds of services.\\n\\n#### [Branding Strategy](https://towardsai.net/\\\\#)\\n\\nA well-defined and executed brand strategy affects all aspects of a business.\\n\\n#### [Video Production](https://towardsai.net/\\\\#)\\n\\nWe help businesses & brands achieve their goals through video content that engages and compels.\\n\\n#### [Marketing Campaign & PR](https://towardsai.net/\\\\#)\\n\\nA great way to slip past any defences and connect with your audience via channels.\\n\\n\\\\[rev\\\\_slider alias=”startup-03″\\\\]\\n\\n## Towards AI Stats\\n\\n### Past 30 Days\\n\\n500,000\\n\\n###### Social Media Followers\\n\\n400,000\\n\\n###### Article Reads\\n\\n130,000\\n\\n###### Newsletter Reads\\n\\n100,000\\n\\n###### Students\\n\\n4,000\\n\\n###### Writers\\n\\n![startup-03-image-03](<Base64-Image-Removed>)\\n\\n### What makes Towards AI  different?\\n\\n#### [Free Resources](https://towardsai.net/\\\\#)\\n\\nPurchase this AI so as to gain access to abundant resources that are totally free of charge.\\n\\n#### [Exquisite Design](https://towardsai.net/\\\\#)\\n\\nUsers will have a chance to enjoy this one-of-a-kind WordPress AI with its exquisite design.\\n\\n[Try it out](https://towardsai.net/#)\\n\\n[View projects](https://towardsai.net/#)\\n\\n### Everything You Need In One Place\\n\\n01\\n\\n###### Fully Responsive\\n\\nTowards AI is intended to be highly responsive and customizable for site building process. Thanks to its devoted, fastidious, and compact design, Towards AI can be considered among plenty of unique AIs that serve to create highly responsive websites.\\n\\n02\\n\\n###### Free Trial\\n\\nIf you are excited to know more about this AI, you can try it out for free before deciding to purchase. Even after making your purchase, if you find it unsatisfactory, you can always make a refund request to the AI marketplace administrator to get your money back.\\n\\n03\\n\\n###### Excellent Support\\n\\nIf you have any problem or difficulty during the process of working with Towards AI, you should contact our customer support team. Our team of professional and skilled staff will reply as soon as possible and propose you with the best solution to tackle the issues.\\n\\n![startup-03-image-02](<Base64-Image-Removed>)\\n\\n### stay up-to-date with towards ai\\n\\n### Join our AI Community\\n\\n[Unleash your AI journey](https://community.towardsai.net/)\\n\\n### GDPR CCPA Statement\\n\\nIn order for Towards AI to work properly, we log user data. By using Towards AI, you agree to our [Privacy Policy](https://towardsai.net/privacy), including our cookie policy.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16605aa",
   "metadata": {},
   "source": [
    "> I was getting issue related to the metadata becoming greater than the max context window of the embedding model. \n",
    "I resolved this by removing irrelevant fields, but in real production, we can use different embedding having a larger context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a58a7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in Metadata: dict_keys(['title', 'description', 'url', 'language', 'keywords', 'robots', 'og_title', 'og_description', 'og_url', 'og_image', 'og_audio', 'og_determiner', 'og_locale', 'og_locale_alternate', 'og_site_name', 'og_video', 'favicon', 'dc_terms_created', 'dc_date_created', 'dc_date', 'dc_terms_type', 'dc_type', 'dc_terms_audience', 'dc_terms_subject', 'dc_subject', 'dc_description', 'dc_terms_keywords', 'modified_time', 'published_time', 'article_tag', 'article_section', 'source_url', 'status_code', 'scrape_id', 'num_pages', 'content_type', 'proxy_used', 'timezone', 'cache_state', 'cached_at', 'credits_used', 'concurrency_limited', 'concurrency_queue_duration_ms', 'error', 'generator', 'fb:app_id', 'og:url', 'foundingDate', 'twitter:site', 'bestRating', 'viewport', 'og:image', 'msapplication-TileColor', 'og:title', 'worstRating', 'twitter:description', 'twitter:image', 'twitter:title', 'twitter:card', 'msapplication-TileImage', 'theme-color', 'target', 'og:description', 'og:image:width', 'og:image:height', 'og:type'])\n",
      "Key: title | Length: 10 characters\n",
      "Key: description | Length: 156 characters\n",
      "Key: url | Length: 22 characters\n",
      "Key: language | Length: 5 characters\n",
      "Key: keywords | Length: 4 characters\n",
      "Key: robots | Length: 23 characters\n",
      "Key: og_title | Length: 10 characters\n",
      "Key: og_description | Length: 156 characters\n",
      "Key: og_url | Length: 22 characters\n",
      "Key: og_image | Length: 72 characters\n",
      "Key: og_audio | Length: 4 characters\n",
      "Key: og_determiner | Length: 4 characters\n",
      "Key: og_locale | Length: 4 characters\n",
      "Key: og_locale_alternate | Length: 4 characters\n",
      "Key: og_site_name | Length: 4 characters\n",
      "Key: og_video | Length: 4 characters\n",
      "Key: favicon | Length: 134 characters\n",
      "Key: dc_terms_created | Length: 4 characters\n",
      "Key: dc_date_created | Length: 4 characters\n",
      "Key: dc_date | Length: 4 characters\n",
      "Key: dc_terms_type | Length: 4 characters\n",
      "Key: dc_type | Length: 4 characters\n",
      "Key: dc_terms_audience | Length: 4 characters\n",
      "Key: dc_terms_subject | Length: 4 characters\n",
      "Key: dc_subject | Length: 4 characters\n",
      "Key: dc_description | Length: 4 characters\n",
      "Key: dc_terms_keywords | Length: 4 characters\n",
      "Key: modified_time | Length: 4 characters\n",
      "Key: published_time | Length: 4 characters\n",
      "Key: article_tag | Length: 4 characters\n",
      "Key: article_section | Length: 4 characters\n",
      "Key: source_url | Length: 22 characters\n",
      "Key: status_code | Length: 3 characters\n",
      "Key: scrape_id | Length: 36 characters\n",
      "Key: num_pages | Length: 4 characters\n",
      "Key: content_type | Length: 24 characters\n",
      "Key: proxy_used | Length: 5 characters\n",
      "Key: timezone | Length: 4 characters\n",
      "Key: cache_state | Length: 3 characters\n",
      "Key: cached_at | Length: 24 characters\n",
      "Key: credits_used | Length: 1 characters\n",
      "Key: concurrency_limited | Length: 5 characters\n",
      "Key: concurrency_queue_duration_ms | Length: 4 characters\n",
      "Key: error | Length: 4 characters\n",
      "Key: generator | Length: 97 characters\n",
      "Key: fb:app_id | Length: 16 characters\n",
      "Key: og:url | Length: 22 characters\n",
      "Key: foundingDate | Length: 10 characters\n",
      "Key: twitter:site | Length: 10 characters\n",
      "Key: bestRating | Length: 1 characters\n",
      "Key: viewport | Length: 35 characters\n",
      "Key: og:image | Length: 72 characters\n",
      "Key: msapplication-TileColor | Length: 7 characters\n",
      "Key: og:title | Length: 10 characters\n",
      "Key: worstRating | Length: 1 characters\n",
      "Key: twitter:description | Length: 156 characters\n",
      "Key: twitter:image | Length: 72 characters\n",
      "Key: twitter:title | Length: 10 characters\n",
      "Key: twitter:card | Length: 19 characters\n",
      "Key: msapplication-TileImage | Length: 97 characters\n",
      "Key: theme-color | Length: 7 characters\n",
      "Key: target | Length: 45 characters\n",
      "Key: og:description | Length: 156 characters\n",
      "Key: og:image:width | Length: 4 characters\n",
      "Key: og:image:height | Length: 3 characters\n",
      "Key: og:type | Length: 7 characters\n"
     ]
    }
   ],
   "source": [
    "# Print the keys and their sizes for the first document\n",
    "doc = documents[0]\n",
    "print(\"Keys in Metadata:\", doc.metadata.keys())\n",
    "\n",
    "for key, value in doc.metadata.items():\n",
    "    print(f\"Key: {key} | Length: {len(str(value))} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a28009f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Towards AI', 'description': 'Master AI with Towards AI. We offer practical AI Engineering courses, corporate AI bootcamps, and LLM development consultancy for individuals and companies.', 'url': 'https://towardsai.net/'}\n"
     ]
    }
   ],
   "source": [
    "# Loop through all documents and remove heavy fields\n",
    "for docs in documents:\n",
    "    # Keep only the URL and Title, delete everything else\n",
    "    to_keep = {'url', 'title', 'description', 'sourceURL'}\n",
    "    \n",
    "    # Create a clean dictionary\n",
    "    clean_metadata = {k: v for k, v in docs.metadata.items() if k in to_keep}\n",
    "    \n",
    "    # Overwrite the metadata\n",
    "    docs.metadata = clean_metadata\n",
    "\n",
    "# Check if it worked \n",
    "print(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f1e88f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='a709554a-0c3e-471b-8b92-f20909f1d800', embedding=None, metadata={'title': 'Towards AI', 'description': 'Master AI with Towards AI. We offer practical AI Engineering courses, corporate AI bootcamps, and LLM development consultancy for individuals and companies.', 'url': 'https://towardsai.net/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='- [Latest](https://towardsai.net/p/)\\n- [Trends](https://medium.com/towards-artificial-intelligence/trending)\\n- [Shop](https://gumroad.com/towardsai)\\n\\n[Google News](https://news.google.com/publications/CAAqBwgKMNiLmgswgpayAw)[Medium](https://pub.towardsai.net/)[Linkedin](https://linkedin.com/company/towards-artificial-intelligence)[Twitter](https://twitter.com/towards_ai?lang=en)[Facebook](https://www.facebook.com/towardsAl/)[Instagram](https://instagram.com/towards_ai/)[Github](https://github.com/towardsai)[RSS Feed](http://feeds.feedburner.com/towards-ai)[Email us](mailto:pub@towardsai.net)\\n\\nName: Towards AI\\nLegal Name: Towards AI, Inc.\\nDescription: Towards AI is the world\\'s leading artificial intelligence (AI) and technology publication. Read by thought-leaders and decision-makers around the world.\\nPhone Number: +1-650-246-9381\\nEmail: pub@towardsai.net\\n\\n228 Park Avenue SouthNew York,\\nNY10003United States\\n\\nWebsite: [https://towardsai.net/](https://towardsai.net/)\\nPublisher: [https://towardsai.net/#publisher](https://towardsai.net/#publisher)\\nDiversity Policy: [https://towardsai.net/about](https://towardsai.net/about)\\nEthics Policy: [https://towardsai.net/about](https://towardsai.net/about)\\nMasthead: [https://towardsai.net/about](https://towardsai.net/about)\\n\\nName: Towards AI\\nLegal Name: Towards AI, Inc.\\nDescription: Towards AI is the world\\'s leading artificial intelligence (AI) and technology publication.\\nFounders:\\nRoberto Iriondo,\\n[Website](https://www.robertoiriondo.com/),\\nJob Title: Co-founder and Advisor\\nWorks for: Towards AI, Inc.\\nFollow Roberto:\\n[X](https://x.com/@robiriondo),\\n[LinkedIn](https://www.linkedin.com/in/robiriondo),\\n[GitHub](https://github.com/robiriondo),\\n[Google Scholar](https://scholar.google.com/citations?user=dTn7NEcAAAAJ),\\n[Towards AI Profile](https://towardsai.net/p/author/robiriondo),\\n[Medium](https://medium.com/@robiriondo),\\n[ML@CMU](https://www.ml.cmu.edu/robiriondo),\\n[FreeCodeCamp](https://www.freecodecamp.org/news/author/robiriondo/),\\n[Crunchbase](https://www.crunchbase.com/person/roberto-iriondo),\\n[Bloomberg](https://www.bloomberg.com/profile/person/22994840),\\n[Roberto Iriondo, Generative AI Lab](https://roberto.generativeailab.org/),\\n[Generative AI Lab](https://generativeailab.org/) [VeloxTrend](https://veloxtrend.com/) [Ultrarix Capital Partners](https://ultrarix.com/)Denis Piffaretti,\\nJob Title: Co-founder\\nWorks for: Towards AI, Inc.Louie Peters,\\nJob Title: Co-founder\\nWorks for: Towards AI, Inc.Louis-François Bouchard,\\nJob Title: Co-founder\\nWorks for: Towards AI, Inc.\\n\\nCover:\\n\\n\\n![Towards AI Cover](https://b3688296.smushcdn.com/3688296/wp-content/uploads/2024/09/towards-ai-og-graph.jpg?lossy=0&strip=1&webp=1)\\n\\nLogo:\\n\\n\\n![Towards AI Logo](https://b3688296.smushcdn.com/3688296/wp-content/uploads/2019/05/towards-ai-square-circle-png.png?lossy=0&strip=1&webp=1)\\n\\nAreas Served: Worldwide\\nAlternate Name: Towards AI, Inc.\\nAlternate Name: Towards AI Co.\\nAlternate Name: towards ai\\nAlternate Name: towardsai\\nAlternate Name: towards.ai\\nAlternate Name: tai\\nAlternate Name: toward ai\\nAlternate Name: toward.ai\\nAlternate Name: Towards AI, Inc.\\nAlternate Name: towardsai.net\\nAlternate Name: pub.towardsai.net\\n\\nFollow us on:\\n[Facebook](https://www.facebook.com/towardsAl/ \"Facebook\") [X](https://x.com/towards_ai \"X\") [LinkedIn](https://www.linkedin.com/company/towards-artificial-intelligence \"LinkedIn\") [Instagram](https://www.instagram.com/towards_ai/ \"Instagram\") [Youtube](https://www.youtube.com/channel/UCQNjFuhOJM1YqFTPY1Q_kYQ \"Youtube\") [Github](https://github.com/towardsai \"Github\") [Google My Business](https://local.google.com/place?id=4955254490173856159&use=srp&ved=1t%3A65428&_ga=2.191706990.559569912.1635283323-985655235.1633987376#fpstate=lie \"Google my Business\") [Google Search](https://www.google.com/search?ved=1t:65428&_ga=2.191706990.559569912.1635283323-985655235.1633987376&q=Towards+AI&ludocid=4955254490173856159&lsig=AB86z5Ur3DZsSmdOFFUwd-bMHTIe#fpstate=lie \"Google Search\") [Google News](https://news.google.com/publications/CAAqBwgKMNiLmgswgpayAw?oc=3&ceid=US:en \"Google News\") [Google Maps](https://g.page/TowardsAI?gm \"Google Maps\") [Discord](https://discord.com/invite/V7RGX9XKAW \"Discord\") [Shop](https://gumroad.com/towardsai \"Shop\") [Towards AI, Medium Editorial](https://towardsai.medium.com/ \"Towards AI, Medium Editorial\") [Medium](https://medium.com/towards-artificial-intelligence \"Medium\") [Flipboard](https://flipboard.com/@Towards_AI \"Flipboard\") [Publication](https://pub.towardsai.net/ \"Publication\") [Feed](https://feed.towardsai.net/ \"AI Feed\") [Sponsors](https://sponsors.towardsai.net/ \"Sponsors\") [Sponsors](https://members.towardsai.net/ \"Members\") [Contribute](https://contribute.towardsai.net/ \"Contribute\")\\n\\n5 stars – based on\\n497 reviews\\n\\n\\n\\n\\n#### Frequently Used, Contextual References\\n\\nTODO: Remember to copy unique IDs whenever it needs used. i.e., URL: 304b2e42315e\\n\\n## Resources\\n\\nOur 15 AI experts built the most comprehensive, practical, 90+ lesson courses to master AI Engineering - we have pathways for any experience at [Towards AI Academy](https://academy.towardsai.net/?utm_source=taiwordpress&utm_medium=banner). Cohorts still open - use COHORT10 for 10% off.\\n\\n# Towards AI is the the leading AI community and content platform making AI accessible to all.\\n\\n### Welcome to Towards AI\\n\\n### Making AI & ML accessible to all\\n\\nWe have taught over 500,000 AI learners over the past 5 years.\\nNow we also offer full practical Generative AI courses!\\nTake our one-stop LLM Developer conversion course and help lead this new field.\\n\\n\\n[Sign Up for the Course](https://academy.towardsai.net/courses/beginner-to-advanced-llm-dev)\\n\\n[Watch the overview](https://www.youtube.com/watch?v=9No-FiEInLA)\\n\\n![startup-03-image-01](<Base64-Image-Removed>)\\n\\n### Updates on Towards AI\\n\\n### Latest Stories\\n\\nThe header\\n\\n[Making Gems with Google Docs](https://towardsai.net/p/machine-learning/making-gems-with-google-docs)\\n\\ntowardsai.net\\\\|Dec 9th 2025\\n\\n[Understanding XGBoost: A Deep Dive into the Algorithm](https://towardsai.net/p/machine-learning/understanding-xgboost-a-deep-dive-into-the-algorithm)\\n\\ntowardsai.net\\\\|Dec 9th 2025\\n\\n[How to Build Travel AI Agents Using Phidata and Qdrant](https://towardsai.net/p/machine-learning/how-to-build-travel-ai-agents-using-phidata-and-qdrant)\\n\\ntowardsai.net\\\\|Dec 9th 2025\\n\\n[Prompt Engineering Quick Guide](https://towardsai.net/p/machine-learning/prompt-engineering-quick-guide)\\n\\ntowardsai.net\\\\|Dec 9th 2025\\n\\n[FeedGrabbr](https://feedgrabbr.com/ \"FeedGrabbr\")\\n\\n[All Latest Stories](https://towardsai.net/p \"Follow our publication\")\\n\\n#### [Ui / ux Design](https://towardsai.net/\\\\#)\\n\\nGiving you the best possible experience when using websites and all kinds of services.\\n\\n#### [Branding Strategy](https://towardsai.net/\\\\#)\\n\\nA well-defined and executed brand strategy affects all aspects of a business.\\n\\n#### [Video Production](https://towardsai.net/\\\\#)\\n\\nWe help businesses & brands achieve their goals through video content that engages and compels.\\n\\n#### [Marketing Campaign & PR](https://towardsai.net/\\\\#)\\n\\nA great way to slip past any defences and connect with your audience via channels.\\n\\n\\\\[rev\\\\_slider alias=”startup-03″\\\\]\\n\\n## Towards AI Stats\\n\\n### Past 30 Days\\n\\n500,000\\n\\n###### Social Media Followers\\n\\n400,000\\n\\n###### Article Reads\\n\\n130,000\\n\\n###### Newsletter Reads\\n\\n100,000\\n\\n###### Students\\n\\n4,000\\n\\n###### Writers\\n\\n![startup-03-image-03](<Base64-Image-Removed>)\\n\\n### What makes Towards AI  different?\\n\\n#### [Free Resources](https://towardsai.net/\\\\#)\\n\\nPurchase this AI so as to gain access to abundant resources that are totally free of charge.\\n\\n#### [Exquisite Design](https://towardsai.net/\\\\#)\\n\\nUsers will have a chance to enjoy this one-of-a-kind WordPress AI with its exquisite design.\\n\\n[Try it out](https://towardsai.net/#)\\n\\n[View projects](https://towardsai.net/#)\\n\\n### Everything You Need In One Place\\n\\n01\\n\\n###### Fully Responsive\\n\\nTowards AI is intended to be highly responsive and customizable for site building process. Thanks to its devoted, fastidious, and compact design, Towards AI can be considered among plenty of unique AIs that serve to create highly responsive websites.\\n\\n02\\n\\n###### Free Trial\\n\\nIf you are excited to know more about this AI, you can try it out for free before deciding to purchase. Even after making your purchase, if you find it unsatisfactory, you can always make a refund request to the AI marketplace administrator to get your money back.\\n\\n03\\n\\n###### Excellent Support\\n\\nIf you have any problem or difficulty during the process of working with Towards AI, you should contact our customer support team. Our team of professional and skilled staff will reply as soon as possible and propose you with the best solution to tackle the issues.\\n\\n![startup-03-image-02](<Base64-Image-Removed>)\\n\\n### stay up-to-date with towards ai\\n\\n### Join our AI Community\\n\\n[Unleash your AI journey](https://community.towardsai.net/)\\n\\n### GDPR CCPA Statement\\n\\nIn order for Towards AI to work properly, we log user data. By using Towards AI, you agree to our [Privacy Policy](https://towardsai.net/privacy), including our cookie policy.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fcabb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.core import VectorStoreIndex\n",
    "import google.genai.types as types\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
    "    max_output_tokens=1024,\n",
    "    temperature=0.6,\n",
    ")\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    generation_config=config,\n",
    "    )\n",
    "\n",
    "Settings.embed_model = CohereEmbedding(\n",
    "    model_name=\"embed-english-v3.0\",\n",
    "    input_type=\"search_document\",\n",
    "    api_key=COHERE_API_KEY\n",
    ")\n",
    "Settings.llm = llm\n",
    "Settings.text_splitter = SentenceSplitter(chunk_size=384, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3b05c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:12<00:00, 12.59s/it]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:00<00:00, 17.27it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddc702fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Towards AI aims to help individuals and companies master AI through practical AI Engineering courses, corporate AI bootcamps, and LLM development consultancy. It also serves as a leading artificial intelligence and technology publication, read by thought-leaders and decision-makers globally.\n",
      "-----------------\n",
      "Node ID\t f6254025-da11-408c-ae9e-cb4d443d4637\n",
      "Title\t Towards AI\n",
      "URL\t https://towardsai.net/\n",
      "Score\t 0.5347926030189166\n",
      "Description\t Master AI with Towards AI. We offer practical AI Engineering courses, corporate AI bootcamps, and LLM development consultancy for individuals and companies.\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t 6097ac95-92dd-436f-963a-89a445fcb8ba\n",
      "Title\t Towards AI\n",
      "URL\t https://towardsai.net/\n",
      "Score\t 0.5275377338361159\n",
      "Description\t Master AI with Towards AI. We offer practical AI Engineering courses, corporate AI bootcamps, and LLM development consultancy for individuals and companies.\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "res = query_engine.query(\"What is towards AI aim?\")\n",
    "\n",
    "print(res.response)\n",
    "\n",
    "print(\"-----------------\")\n",
    "# Show the retrieved nodes\n",
    "for src in res.source_nodes:\n",
    "  print(\"Node ID\\t\", src.node_id)\n",
    "  print(\"Title\\t\", src.metadata['title'])\n",
    "  print(\"URL\\t\", src.metadata['url'])\n",
    "  print(\"Score\\t\", src.score)\n",
    "  print(\"Description\\t\", src.metadata.get(\"description\"))\n",
    "  print(\"-_\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c793eb3",
   "metadata": {},
   "source": [
    "# CRAWL A WEBSITE\n",
    "\n",
    "## Load The CSV\n",
    "\n",
    "CSV contains the list of tools and url of the page which we use to get information about the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d7580af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# Google Sheets file URL (CSV export link)\n",
    "url = 'https://docs.google.com/spreadsheets/d/1gHB-aQJGt9Nl3cyOP2GorAkBI_Us2AqkYnfqrmejStc/export?format=csv'\n",
    "\n",
    "# Send a GET request to fetch the CSV file\n",
    "response = requests.get(url)\n",
    "\n",
    "response_list = []\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Decode the content to a string\n",
    "    content = response.content.decode('utf-8')\n",
    "\n",
    "    # Use the csv.DictReader to read the content as a dictionary\n",
    "    csv_reader = csv.DictReader(content.splitlines(), delimiter=',')\n",
    "    response_list = [row for row in csv_reader]\n",
    "else:\n",
    "    print(f\"Failed to retrieve the file: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df9b6bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'Jax',\n",
       "  'Tool Type': 'Library',\n",
       "  'Parent': '',\n",
       "  'Company': '',\n",
       "  'Description': 'Autograd and XLA for high-performance machine learning research',\n",
       "  'Category': 'Scientific Computing',\n",
       "  'URL': 'https://jax.readthedocs.io/en/latest/',\n",
       "  'Is a direct URL company /tool website?': 'Yes',\n",
       "  '': ''},\n",
       " {'Name': 'Keras',\n",
       "  'Tool Type': 'Library',\n",
       "  'Parent': '',\n",
       "  'Company': '',\n",
       "  'Description': 'Open-source library that provides a Python interface for artificial neural networks',\n",
       "  'Category': 'Model Development',\n",
       "  'URL': 'https://keras.io/',\n",
       "  'Is a direct URL company /tool website?': 'Yes',\n",
       "  '': ''},\n",
       " {'Name': 'Koalas',\n",
       "  'Tool Type': 'Library',\n",
       "  'Parent': '',\n",
       "  'Company': '',\n",
       "  'Description': 'pandas API on Apache Spark',\n",
       "  'Category': 'Big Data Processing',\n",
       "  'URL': 'https://koalas.readthedocs.io/en/latest/',\n",
       "  'Is a direct URL company /tool website?': 'Yes',\n",
       "  '': ''},\n",
       " {'Name': 'Librosa',\n",
       "  'Tool Type': 'Library',\n",
       "  'Parent': '',\n",
       "  'Company': '',\n",
       "  'Description': 'Python package for music and audio analysis',\n",
       "  'Category': 'Audio Processing',\n",
       "  'URL': 'https://librosa.org/doc/latest/tutorial.html',\n",
       "  'Is a direct URL company /tool website?': 'Yes',\n",
       "  '': ''},\n",
       " {'Name': 'LIME (Local Interpretable Model-agnostic Explanations)',\n",
       "  'Tool Type': 'Library',\n",
       "  'Parent': '',\n",
       "  'Company': '',\n",
       "  'Description': 'Explaining the predictions of any machine learning classifier',\n",
       "  'Category': 'Model Interpretability',\n",
       "  'URL': 'https://lime-ml.readthedocs.io/en/latest/#',\n",
       "  'Is a direct URL company /tool website?': 'No',\n",
       "  '': ''},\n",
       " {'Name': 'Luigi',\n",
       "  'Tool Type': 'Library',\n",
       "  'Parent': '',\n",
       "  'Company': '',\n",
       "  'Description': 'Python package for building complex pipelines of batch jobs',\n",
       "  'Category': 'Workflow Management',\n",
       "  'URL': 'https://luigi.readthedocs.io/en/stable/workflows.html',\n",
       "  'Is a direct URL company /tool website?': 'No',\n",
       "  '': ''},\n",
       " {'Name': 'Marshmallow',\n",
       "  'Tool Type': 'Library',\n",
       "  'Parent': '',\n",
       "  'Company': '',\n",
       "  'Description': 'ORM/ODM/framework-agnostic library for converting complex datatypes to and from native Python datatypes',\n",
       "  'Category': 'Data Serialization',\n",
       "  'URL': 'https://marshmallow.readthedocs.io/en/stable/',\n",
       "  'Is a direct URL company /tool website?': 'Yes',\n",
       "  '': ''},\n",
       " {'Name': 'Matplotlib',\n",
       "  'Tool Type': 'Library',\n",
       "  'Parent': '',\n",
       "  'Company': '',\n",
       "  'Description': 'Low level graph plotting library in python that serves as a visualization utility',\n",
       "  'Category': 'Data Visualization',\n",
       "  'URL': 'https://matplotlib.org/',\n",
       "  'Is a direct URL company /tool website?': 'Yes',\n",
       "  '': ''},\n",
       " {'Name': 'Mimesis',\n",
       "  'Tool Type': 'Library',\n",
       "  'Parent': '',\n",
       "  'Company': '',\n",
       "  'Description': 'High-performance fake data generator',\n",
       "  'Category': 'Data Generation',\n",
       "  'URL': 'https://mimesis.name/en/v13.1.0/',\n",
       "  'Is a direct URL company /tool website?': 'Yes',\n",
       "  '': ''},\n",
       " {'Name': 'MLxtend',\n",
       "  'Tool Type': 'Library',\n",
       "  'Parent': '',\n",
       "  'Company': '',\n",
       "  'Description': \"Library of extension and helper modules for Python's data science stack\",\n",
       "  'Category': 'Machine Learning',\n",
       "  'URL': 'https://rasbt.github.io/mlxtend/',\n",
       "  'Is a direct URL company /tool website?': 'No',\n",
       "  '': ''}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "start_index = random.randint(0, len(response_list) - 3)\n",
    "website_list = response_list[start_index:start_index+10] # Crawling 10 websites only.\n",
    "website_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f7f97",
   "metadata": {},
   "source": [
    "## Initialize the Firecrawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5aff7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from firecrawl import FirecrawlApp\n",
    "app = FirecrawlApp(api_key=FIRECRAWL_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2e6b25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://jax.readthedocs.io/en/latest/\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://keras.io/\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://koalas.readthedocs.io/en/latest/\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://librosa.org/doc/latest/tutorial.html\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://lime-ml.readthedocs.io/en/latest/#\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://luigi.readthedocs.io/en/stable/workflows.html\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://marshmallow.readthedocs.io/en/stable/\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://matplotlib.org/\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://mimesis.name/en/v13.1.0/\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://rasbt.github.io/mlxtend/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCrawling: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     response = \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscrape_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mformats\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmarkdown\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhtml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     crawled_websites += \u001b[32m1\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\firecrawl\\v2\\client.py:299\u001b[39m, in \u001b[36mFirecrawlClient.crawl\u001b[39m\u001b[34m(self, url, prompt, exclude_paths, include_paths, max_discovery_depth, ignore_sitemap, ignore_query_parameters, limit, crawl_entire_domain, allow_external_links, allow_subdomains, delay, max_concurrency, webhook, scrape_options, zero_data_retention, poll_interval, timeout, request_timeout, integration)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mStart a crawl job and wait for it to complete.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    277\u001b[39m \u001b[33;03m    TimeoutError: If timeout is reached\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    279\u001b[39m request = CrawlRequest(\n\u001b[32m    280\u001b[39m     url=url,\n\u001b[32m    281\u001b[39m     prompt=prompt,\n\u001b[32m   (...)\u001b[39m\u001b[32m    296\u001b[39m     integration=integration,\n\u001b[32m    297\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcrawl_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpoll_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpoll_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\firecrawl\\v2\\methods\\crawl.py:402\u001b[39m, in \u001b[36mcrawl\u001b[39m\u001b[34m(client, request, poll_interval, timeout, request_timeout)\u001b[39m\n\u001b[32m    399\u001b[39m effective_request_timeout = request_timeout \u001b[38;5;28;01mif\u001b[39;00m request_timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m timeout\n\u001b[32m    401\u001b[39m \u001b[38;5;66;03m# Wait for completion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwait_for_crawl_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpoll_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43meffective_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\firecrawl\\v2\\methods\\crawl.py:349\u001b[39m, in \u001b[36mwait_for_crawl_completion\u001b[39m\u001b[34m(client, job_id, poll_interval, timeout, request_timeout)\u001b[39m\n\u001b[32m    346\u001b[39m start_time = time.monotonic()\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     crawl_job = \u001b[43mget_crawl_status\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    355\u001b[39m     \u001b[38;5;66;03m# Check if job is complete\u001b[39;00m\n\u001b[32m    356\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m crawl_job.status \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mcompleted\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfailed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcancelled\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\firecrawl\\v2\\methods\\crawl.py:199\u001b[39m, in \u001b[36mget_crawl_status\u001b[39m\u001b[34m(client, job_id, pagination_config, request_timeout)\u001b[39m\n\u001b[32m    193\u001b[39m auto_paginate = pagination_config.auto_paginate \u001b[38;5;28;01mif\u001b[39;00m pagination_config \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auto_paginate \u001b[38;5;129;01mand\u001b[39;00m response_data.get(\u001b[33m\"\u001b[39m\u001b[33mnext\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[32m    195\u001b[39m     pagination_config\n\u001b[32m    196\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m pagination_config.max_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    197\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(documents) >= pagination_config.max_results\n\u001b[32m    198\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     documents = \u001b[43m_fetch_all_pages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpagination_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Create CrawlJob with current status and data\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CrawlJob(\n\u001b[32m    209\u001b[39m     status=response_data.get(\u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    210\u001b[39m     completed=response_data.get(\u001b[33m\"\u001b[39m\u001b[33mcompleted\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    215\u001b[39m     data=documents\n\u001b[32m    216\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\firecrawl\\v2\\methods\\crawl.py:262\u001b[39m, in \u001b[36m_fetch_all_pages\u001b[39m\u001b[34m(client, next_url, initial_documents, pagination_config, request_timeout)\u001b[39m\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[38;5;66;03m# Fetch next page\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.ok:\n\u001b[32m    265\u001b[39m     \u001b[38;5;66;03m# Log error but continue with what we have\u001b[39;00m\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\firecrawl\\v2\\utils\\http_client.py:118\u001b[39m, in \u001b[36mHttpClient.get\u001b[39m\u001b[34m(self, endpoint, headers, timeout, retries, backoff_factor)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m         response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m502\u001b[39m:\n\u001b[32m    125\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m attempt < retries - \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\urllib3\\connection.py:759\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    758\u001b[39m     sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n\u001b[32m    761\u001b[39m     tls_in_tls = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\urllib3\\connection.py:204\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[32m    200\u001b[39m \n\u001b[32m    201\u001b[39m \u001b[33;03m:return: New socket connection.\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Extra Learning Resources\\Extra Learning Resources\\NLP\\RAG\\Practice Learning\\venv_rag\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[32m     75\u001b[39m err = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Crawl websites and handle responses\n",
    "url_response = {}\n",
    "crawl_per_min = 1  # Max crawl per minute\n",
    "\n",
    "# Track crawls\n",
    "crawled_websites = 0\n",
    "scraped_pages = 0\n",
    "\n",
    "for i, website_dict in enumerate(website_list):\n",
    "    url = website_dict.get('URL')\n",
    "    print(f\"Crawling: {url}\")\n",
    "\n",
    "    try:\n",
    "        response = app.crawl(\n",
    "            url,\n",
    "            limit=2,\n",
    "            scrape_options={\n",
    "                'formats': ['markdown', 'html']\n",
    "            }\n",
    "        )\n",
    "        crawled_websites += 1\n",
    "\n",
    "    except Exception as exc:\n",
    "        print(f\"Failed to fetch {url} -> {exc}\")\n",
    "        continue\n",
    "\n",
    "    # Store the scraped data and associated info in the response dict\n",
    "    url_response[url] = {\n",
    "        \"scraped_data\": response.data,\n",
    "        \"csv_data\": website_dict\n",
    "    }\n",
    "\n",
    "    # Pause to comply with crawl per minute limit for free version its 1 crawl per minute\n",
    "    if i!=len(website_list) and (i + 1) % crawl_per_min == 0:\n",
    "        print(\"Pausing for 1 minute to comply with crawl limit...\")\n",
    "        time.sleep(60)  # Pause for 1 minute after every crawl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "documents = []\n",
    "\n",
    "for _, scraped_content in url_response.items():\n",
    "    csv_data = scraped_content.get(\"csv_data\")\n",
    "    scraped_results = scraped_content.get(\"scraped_data\")\n",
    "\n",
    "    for scraped_site_dict in scraped_results:\n",
    "        for result in scraped_results:\n",
    "            markdown_content = result.get(\"markdown\")\n",
    "            title = result.get(\"metadata\").get(\"title\")\n",
    "            url = result.get(\"metadata\").get(\"sourceURL\")\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    text=markdown_content,\n",
    "                    metadata={\n",
    "                        \"title\": title,\n",
    "                        \"url\": url,\n",
    "                        \"description\": csv_data.get(\"Description\"),\n",
    "                        \"category\": csv_data.get(\"Category\")\n",
    "                    }\n",
    "                )\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b5dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.core import VectorStoreIndex\n",
    "import google.genai.types as types\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
    "    max_output_tokens=1024,\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    generation_config=config,\n",
    "    )\n",
    "\n",
    "Settings.embed_model = CohereEmbedding(\n",
    "    model_name=\"embed-english-v3.0\",\n",
    "    input_type=\"search_document\",\n",
    "    api_key=COHERE_API_KEY\n",
    ")\n",
    "Settings.llm = llm\n",
    "Settings.text_splitter = SentenceSplitter(chunk_size=300, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead53049",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = query_engine.query(\"Explain GraphRAG\")\n",
    "print(res.response)\n",
    "\n",
    "print(\"-----------------\")\n",
    "# Show the retrieved nodes\n",
    "for src in res.source_nodes:\n",
    "  print(\"Node ID\\t\", src.node_id)\n",
    "  print(\"Title\\t\", src.metadata['title'])\n",
    "  print(\"URL\\t\", src.metadata['url'])\n",
    "  print(\"Score\\t\", src.score)\n",
    "  print(\"Description\\t\", src.metadata.get(\"description\"))\n",
    "  print(\"Category\\t\", src.metadata.get(\"category\"))\n",
    "  print(\"-_\"*20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
