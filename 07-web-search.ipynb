{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92611097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import nest_asyncio\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "GOOGLE_SEARCH_KEY = os.getenv('GOOGLE_SEARCH_KEY')\n",
    "GOOGLE_SEARCH_ENGINE = os.getenv('GOOGLE_SEARCH_ENGINE')\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2174e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.core import VectorStoreIndex\n",
    "import google.genai.types as types\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
    "    max_output_tokens=1024,\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    generation_config=config,\n",
    "    )\n",
    "\n",
    "Settings.embed_model = CohereEmbedding(\n",
    "    model_name=\"embed-english-v3.0\",\n",
    "    input_type=\"search_document\",\n",
    "    api_key=COHERE_API_KEY\n",
    ")\n",
    "Settings.llm = llm\n",
    "Settings.text_splitter = SentenceSplitter(chunk_size=300, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e7edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "# define sample Tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# initialize ReAct agent\n",
    "agent = ReActAgent(tools=[multiply], verbose=True)\n",
    "\n",
    "# Create a context to store the conversation history/session state\n",
    "ctx = Context(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b340c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReActAgent(name='Agent', description='An agent that can perform a task', system_prompt=None, tools=[<llama_index.core.tools.function_tool.FunctionTool object at 0x000001FEC59F09E0>], tool_retriever=None, can_handoff_to=None, llm=GoogleGenAI(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001FEDAC07380>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x000001FED492ED40>, completion_to_prompt=<function default_completion_to_prompt at 0x000001FED5C8A700>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='gemini-2.5-flash', temperature=0.1, context_window=None, max_retries=3, is_function_calling_model=True, cached_content=None, built_in_tool=None), initial_state={}, state_prompt='Current state:\\n{state}\\n\\nCurrent message:\\n{msg}\\n', output_cls=None, structured_output_fn=None, streaming=True, reasoning_key='current_reasoning', output_parser=<llama_index.core.agent.react.output_parser.ReActOutputParser object at 0x000001FED9622780>, formatter=ReActChatFormatter(system_header='You are designed to help with a variety of tasks, from answering questions to providing summaries to other types of analyses.\\n\\n## Tools\\n\\nYou have access to a wide variety of tools. You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand.\\nThis may require breaking the task into subtasks and using different tools to complete each subtask.\\n\\nYou have access to the following tools:\\n{tool_desc}\\n\\n\\n## Output Format\\n\\nPlease answer in the same language as the question and use the following format:\\n\\n```\\nThought: The current language of the user is: (user\\'s language). I need to use a tool to help me answer the question.\\nAction: tool name (one of {tool_names}) if using a tool.\\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\\n```\\n\\nPlease ALWAYS start with a Thought.\\n\\nNEVER surround your response with markdown code markers. You may use code markers within your response if you need to.\\n\\nPlease use a valid JSON format for the Action Input. Do NOT do this {{\\'input\\': \\'hello world\\', \\'num_beams\\': 5}}. If you include the \"Action:\" line, then you MUST include the \"Action Input:\" line too, even if the tool does not need kwargs, in that case you MUST use \"Action Input: {{}}\".\\n\\nIf this format is used, the tool will respond in the following format:\\n\\n```\\nObservation: tool response\\n```\\n\\nYou should keep repeating the above format till you have enough information to answer the question without using any more tools. At that point, you MUST respond in one of the following two formats:\\n\\n```\\nThought: I can answer without using any more tools. I\\'ll use the user\\'s language to answer\\nAnswer: [your answer here (In the same language as the user\\'s question)]\\n```\\n\\n```\\nThought: I cannot answer the question with the provided tools.\\nAnswer: [your answer here (In the same language as the user\\'s question)]\\n```\\n\\n## Current Conversation\\n\\nBelow is the current conversation consisting of interleaving human and assistant messages.\\n', context='', observation_role=<MessageRole.USER: 'user'>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccbf39de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: multiply\n",
      "Action Input: {\"a\": 43, \"b\": 45}Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The multiplication of 43 and 45 is 1935."
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import AgentStream, ToolCallResult\n",
    "\n",
    "handler = agent.run(\"What is the multiplication of 43 and 45?\", ctx=ctx)\n",
    "\n",
    "async for ev in handler.stream_events():\n",
    "    # if isinstance(ev, ToolCallResult):\n",
    "    #     print(f\"\\nCall {ev.tool_name} with {ev.tool_kwargs}\\nReturned: {ev.tool_output}\")\n",
    "    if isinstance(ev, AgentStream):\n",
    "        print(f\"{ev.delta}\", end=\"\", flush=True)\n",
    "\n",
    "response = await handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bc141ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The multiplication of 43 and 45 is 1935.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd9dd9",
   "metadata": {},
   "source": [
    "## Define Google Search Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "972afb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.google import GoogleSearchToolSpec # Get this from llamaHUb\n",
    "\n",
    "tool_spec = GoogleSearchToolSpec(key=GOOGLE_SEARCH_KEY, engine=GOOGLE_SEARCH_ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4522e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Google Search Tool...\n",
      "Sending request to Google...\n",
      "\n",
      "✅ SUCCESS! Google returned data.\n",
      "------------------------------\n",
      "First Result: N\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.tools.google import GoogleSearchToolSpec\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"Initializing Google Search Tool...\")\n",
    "    tool_spec = GoogleSearchToolSpec(key=GOOGLE_SEARCH_KEY, engine=GOOGLE_SEARCH_ENGINE)\n",
    "    \n",
    "    print(\"Sending request to Google...\")\n",
    "    results = tool_spec.google_search(query=\"Llama 4 model parameters\")\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\n✅ SUCCESS! Google returned data.\")\n",
    "        print(\"-\" * 30)\n",
    "        # Since 'results' is a list of strings, we just print the first one directly\n",
    "        print(f\"First Result: {results[0]}\") \n",
    "        print(\"-\" * 30)\n",
    "    else:\n",
    "        print(\"\\n⚠️ Connection worked, but 0 results found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ FAILURE: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3afe908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize our tool spec\n",
    "from llama_index.core.tools.tool_spec.load_and_search import LoadAndSearchToolSpec\n",
    "\n",
    "# Wrap the google search tool to create an index on top of the returned Google search\n",
    "wrapped_search_tool = LoadAndSearchToolSpec.from_defaults(\n",
    "    tool_spec.to_tool_list()[0],\n",
    ").to_tool_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e538b807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call google_search with {'query': 'LLaMA 4 model parameters'}\n",
      "Returned: Content loaded! You can now search the information using read_google_search\n",
      "\n",
      "Call read_google_search with {'query': 'LLaMA 4 model parameters'}\n",
      "Returned: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-encoding': 'gzip', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'pragma': 'no-cache', 'vary': 'Origin,Accept-Encoding', 'x-accel-expires': '0', 'x-debug-trace-id': '6198dc641d686b394bbb7a65b5323e6f', 'date': 'Fri, 02 Jan 2026 12:28:49 GMT', 'x-envoy-upstream-service-time': '18', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000', 'transfer-encoding': 'chunked'}, status_code: 429, body: {'id': '56b24b7e-324b-41f4-8539-816b1f6b4e63', 'message': 'Please wait and try again later'}\n",
      "I am sorry, but I cannot provide you with the exact number of parameters for the LLaMA 4 model. My search query to find this information was not successful.\n",
      "\n",
      "However, I can tell you about some earlier LLaMA models and their parameters:\n",
      "\n",
      "*   **LLaMA 1:**\n",
      "    *   6.7B parameters\n",
      "    *   13B parameters\n",
      "    *   33B parameters\n",
      "    *   65B parameters\n",
      "*   **LLaMA 2:**\n",
      "    *   7B parameters\n",
      "    *   13B parameters\n",
      "    *   70B parameters\n",
      "\n",
      "It's important to note that as new models are released, they often come with different parameter counts and architectural improvements."
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "\n",
    "# System prompt encouraging tool usage\n",
    "system_prompt = \"\"\"You are a helpful assistant that can search the web for current information.\n",
    "When you don't have information about recent events or models released after your knowledge cutoff,\n",
    "use the available search tools to find accurate, up-to-date information.\"\"\"\n",
    "\n",
    "# Create agent with proper configuration\n",
    "search_agent = FunctionAgent(\n",
    "    tools=wrapped_search_tool,\n",
    "    llm=Settings.llm,\n",
    "    system_prompt=system_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "ctx = Context(search_agent)\n",
    "\n",
    "handler = search_agent.run(\"How many parameters LLaMA 4 model has? List the models with parameters\", ctx=ctx)\n",
    "\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ToolCallResult):\n",
    "        print(f\"\\nCall {ev.tool_name} with {ev.tool_kwargs}\\nReturned: {ev.tool_output}\")\n",
    "    if isinstance(ev, AgentStream):\n",
    "        print(f\"{ev.delta}\", end=\"\", flush=True)\n",
    "\n",
    "response = await handler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859432c3",
   "metadata": {},
   "source": [
    "# Need To Find The Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e49cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.google import GoogleSearchToolSpec\n",
    "\n",
    "tool_spec = GoogleSearchToolSpec(key=GOOGLE_SEARCH_KEY, engine=GOOGLE_SEARCH_ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64454ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No search results available'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results = tool_spec.google_search(\"LLaMA 4 model details\")\n",
    "\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9221e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No search results available'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
